# Transformers for NLP


## Table of Content

- :black_square_button: = TODO
- :white_check_mark: = Satisfying
- :sparkles: = Finished

| Notebook | Status | Description |
|-----|-----|-----|
| Datasets | :black_square_button: |Practical description of Datasets & Dataloaders for memory efficiency |
| Tokenization - Benchmark - Pretrained tokenizers | :black_square_button: | Presentation of different tokenization approaches, along with example tokenizers provided by well-renouned pretrained models |
| Tokenization - Unigram tokenizer - Clinical Trials ICTRP | :white_check_mark: | Fully documented construction and fitting of a Unigram tokenizer |
| Token Embedding - Benchmark - Word2Vec, FastText, Doc2Vec | :white_check_mark: | Presentation of context-free, SGD-based token embedding methods |
| Token Embedding - Benchmark - Matrix Factorization methods | :black_square_button: | Presentation of context-free, Matrix factorization token embedding methods |
| Token Embedding - Clinical Trials ICTRP | :white_check_mark: | Fitting of W2V embedding table on a corpus of I/E criteria |
