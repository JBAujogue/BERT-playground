# Transformers for NLP


## Table of Content


<table>
    <thead>
        <tr>
            <th>Task</th>
            <th>Notebook</th>
            <th>Description</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td rowspan=1>Dataset</td>
            <td rowspan=1>Datasets - Benchmark</td>
            <td>Practical description of Datasets & Dataloaders for memory efficiency</td>
        </tr>
        <tr>
            <td rowspan=2>Tokenization</td>
            <td rowspan=1>Benchmark - Pretrained tokenizers</td>
            <td>Presentation of different tokenization approaches, along with example tokenizers provided by well-renouned pretrained models</td>
        </tr>
        <tr>
            <td rowspan=1>Tokenization - Unigram tokenizer</td>
            <td>Fully documented construction and fitting of a Unigram tokenizer</td>
        </tr>
        <tr>
            <td rowspan=2>L2 Name B</td>
            <td>L3 Name C</td>
        </tr>
        <tr>
            <td>L3 Name D</td>
        </tr>
    </tbody>
</table>

| Notebook | Description |
|:-----:|-----|
| **Dataset** |
| Datasets - Benchmark | Practical description of Datasets & Dataloaders for memory efficiency |
| **Tokenization** |
| Tokenization - Benchmark - Pretrained tokenizers | Presentation of different tokenization approaches, along with example tokenizers provided by well-renouned pretrained models |
| Tokenization - Unigram tokenizer - Clinical Trials ICTRP | Fully documented construction and fitting of a Unigram tokenizer |
| **Token Embedding** |
| Token Embedding - Benchmark - Word2Vec, FastText, Doc2Vec | Presentation of context-free, SGD-based token embedding methods |
| Token Embedding - Benchmark - Matrix Factorization methods | Presentation of context-free, Matrix factorization token embedding methods |
| Token Embedding - Clinical Trials ICTRP | Fitting of W2V embedding table on a corpus of I/E criteria |
