{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4cRE8IbIrIV"
   },
   "source": [
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 35px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "      Named Entity Recognition\n",
    "  </div> \n",
    "  \n",
    "<div style=\"\n",
    "      font-weight: normal; \n",
    "      font-size: 25px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "     Finetuning on CHIA (PubMedBert)\n",
    "  </div> \n",
    "\n",
    "\n",
    "  <div style=\"\n",
    "      font-size: 15px; \n",
    "      line-height: 12px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Jean-baptiste AUJOGUE\n",
    "  </div> \n",
    "\n",
    "\n",
    "  <div style=\" float:right; \n",
    "      font-size: 12px; \n",
    "      line-height: 12px; \n",
    "  padding: 10px 15px 8px;\">\n",
    "  December 2022\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"TOC\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table Of Content\n",
    "\n",
    "1. [Dataset](#data) <br>\n",
    "2. [Albert finetuning](#albert) <br>\n",
    "3. [Inference](#inference) <br>\n",
    "\n",
    "\n",
    "\n",
    "#### Reference\n",
    "\n",
    "- Hugginface full list of [tutorial notebooks](https://github.com/huggingface/transformers/tree/main/notebooks) (see also [here](https://huggingface.co/docs/transformers/main/notebooks#pytorch-examples))\n",
    "- Huggingface full list of [training scripts](https://github.com/huggingface/transformers/tree/main/examples/pytorch)\n",
    "- Huggingface [tutorial notebook](https://github.com/huggingface/notebooks/blob/main/examples/token_classification.ipynb) on token classification\n",
    "- Huggingface [course](https://huggingface.co/course/chapter7/2?fw=tf) on token classification\n",
    "- Huggingface [training script](https://github.com/huggingface/transformers/blob/main/examples/pytorch/token-classification/run_ner.py) on token classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jb\\miniconda3\\envs\\transformers_nlp\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "import copy\n",
    "import string\n",
    "\n",
    "# data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import (\n",
    "    Dataset, \n",
    "    DatasetDict,\n",
    "    ClassLabel, \n",
    "    Features, \n",
    "    Sequence, \n",
    "    Value,\n",
    ")\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "# DL\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoModelForTokenClassification, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    pipeline,\n",
    "    set_seed,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "# viz\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformers settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.22.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom paths & imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_repo = os.path.dirname(os.getcwd())\n",
    "path_to_data = os.path.join(path_to_repo, 'datasets', 'chia', 'chia-ner')\n",
    "path_to_logs = os.path.join(path_to_repo, 'logs', 'NER')\n",
    "path_to_save_mlm = os.path.join(path_to_repo, 'saves', 'MLM')\n",
    "path_to_save_ner = os.path.join(path_to_repo, 'saves', 'NER')\n",
    "path_to_src  = os.path.join(path_to_repo, 'src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, path_to_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptools.ner.preprocessing import tokenize_and_align_categories, create_labels\n",
    "from nlptools.ner.metrics import compute_metrics, compute_metrics_finegrained\n",
    "from nlptools.ner.postprocessing import parse_trf_ner_output, remove_entity_overlaps, correct_entity_boundaries\n",
    "from nlptools.ner.visualization import render_ner_as_html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = 'notebook1'\n",
    "dataset_name = 'chia'\n",
    "hub_model_name = 'michiyasunaga/BioLinkBERT-base'\n",
    "base_model_name = 'biolinkbert'\n",
    "final_model_name = '{}-{}'.format(base_model_name, dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "# 1. Dataset\n",
    "\n",
    "[Table of content](#TOC)\n",
    "\n",
    "We generate a collection of instances of the `datasets.Dataset` class. \n",
    "\n",
    "Note that these are different from the fairly generic `torch.utils.data.Dataset` class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load BIO corpus\n",
    "\n",
    "[Table of content](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Sequence_id</th>\n",
       "      <th>Mention</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NCT00050349_exc</td>\n",
       "      <td>NCT00050349_exc_0</td>\n",
       "      <td>Patients</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NCT00050349_exc</td>\n",
       "      <td>NCT00050349_exc_0</td>\n",
       "      <td>with</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NCT00050349_exc</td>\n",
       "      <td>NCT00050349_exc_0</td>\n",
       "      <td>symptomatic</td>\n",
       "      <td>B-Qualifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NCT00050349_exc</td>\n",
       "      <td>NCT00050349_exc_0</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NCT00050349_exc</td>\n",
       "      <td>NCT00050349_exc_0</td>\n",
       "      <td>CNS</td>\n",
       "      <td>B-Condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NCT00050349_exc</td>\n",
       "      <td>NCT00050349_exc_0</td>\n",
       "      <td>metastases</td>\n",
       "      <td>I-Condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NCT00050349_exc</td>\n",
       "      <td>NCT00050349_exc_0</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NCT00050349_exc</td>\n",
       "      <td>NCT00050349_exc_0</td>\n",
       "      <td>or</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NCT00050349_exc</td>\n",
       "      <td>NCT00050349_exc_0</td>\n",
       "      <td>leptomeningeal</td>\n",
       "      <td>B-Condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NCT00050349_exc</td>\n",
       "      <td>NCT00050349_exc_0</td>\n",
       "      <td>involvement</td>\n",
       "      <td>I-Condition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Id        Sequence_id         Mention     Category\n",
       "0  NCT00050349_exc  NCT00050349_exc_0        Patients            O\n",
       "1  NCT00050349_exc  NCT00050349_exc_0            with            O\n",
       "2  NCT00050349_exc  NCT00050349_exc_0     symptomatic  B-Qualifier\n",
       "3  NCT00050349_exc  NCT00050349_exc_0                            O\n",
       "4  NCT00050349_exc  NCT00050349_exc_0             CNS  B-Condition\n",
       "5  NCT00050349_exc  NCT00050349_exc_0      metastases  I-Condition\n",
       "6  NCT00050349_exc  NCT00050349_exc_0                            O\n",
       "7  NCT00050349_exc  NCT00050349_exc_0              or            O\n",
       "8  NCT00050349_exc  NCT00050349_exc_0  leptomeningeal  B-Condition\n",
       "9  NCT00050349_exc  NCT00050349_exc_0     involvement  I-Condition"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bio = pd.read_csv(os.path.join(path_to_data, 'chia_bio.tsv'), sep = \"\\t\")\n",
    "\n",
    "df_bio.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For conveniance we create an instance of `ClassLabel` that keeps the class name - class index alignment in a single object :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_labels = sorted(list(set(df_bio.Category.unique())))\n",
    "class_labels = ClassLabel(names = class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = class_labels._str2int\n",
    "id2label = {i: l for l, i in label2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Generate train / valid / test dataset\n",
    "\n",
    "[Table of content](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_list(df, grp_col, item_col):\n",
    "    return df.groupby(grp_col).apply(lambda g: g[item_col].tolist()).tolist()\n",
    "\n",
    "\n",
    "\n",
    "def convert_dataframe_to_dataset(df_bio):\n",
    "    data = {\n",
    "        'ids': df_bio.Sequence_id.unique().tolist(),\n",
    "        'mentions': get_item_list(df_bio, 'Sequence_id', 'Mention'),\n",
    "        'categories': get_item_list(df_bio, 'Sequence_id', 'Category'),\n",
    "    }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 800, 100, 100)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset separation: 800 trials for training, 100 trials for validation and 100 trials for testing\n",
    "ids_bio = sorted(list(set(df_bio.Id.apply(lambda i: i.split('_')[0]))))\n",
    "\n",
    "ids_trn, ids_dev = train_test_split(ids_bio, train_size = 0.8, random_state = 13, shuffle = True)\n",
    "ids_dev, ids_tst = train_test_split(ids_dev, train_size = 0.5, random_state = 13, shuffle = True)\n",
    "\n",
    "len(ids_bio), len(ids_trn), len(ids_dev), len(ids_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = df_bio[df_bio.Id.apply(lambda i: i.split('_')[0]).isin(ids_trn)]\n",
    "df_dev = df_bio[df_bio.Id.apply(lambda i: i.split('_')[0]).isin(ids_dev)]\n",
    "df_tst = df_bio[df_bio.Id.apply(lambda i: i.split('_')[0]).isin(ids_tst)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_bio = convert_dataframe_to_dataset(df_bio)\n",
    "dict_trn = convert_dataframe_to_dataset(df_trn)\n",
    "dict_dev = convert_dataframe_to_dataset(df_dev)\n",
    "dict_tst = convert_dataframe_to_dataset(df_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'ner_tags': Sequence(class_labels),\n",
    "features = Features({\n",
    "    'ids': Value(dtype = 'string'), \n",
    "    'mentions': Sequence(Value(dtype = 'string')), \n",
    "    'categories': Sequence(Value(dtype = 'string')),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = DatasetDict({\n",
    "    'trn': Dataset.from_dict(dict_trn, features = features),\n",
    "    'dev': Dataset.from_dict(dict_dev, features = features),\n",
    "    'tst': Dataset.from_dict(dict_tst, features = features),\n",
    "    'all': Dataset.from_dict(dict_bio, features = features),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVx71GdAIrJH"
   },
   "source": [
    "## 1.3 Apply model-specific tokenization\n",
    "\n",
    "[Table of content](#TOC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXNLu_-nIrJI"
   },
   "outputs": [],
   "source": [
    "tokenizer_path = os.path.join(path_to_save_mlm, base_model_name, 'tokenizer')\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    print('Tokenizer loaded from local checkpoint.')\n",
    "except:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(hub_model_name)\n",
    "    tokenizer.save_pretrained(tokenizer_path)\n",
    "    print('Tokenizer downloaded from Huggingface model hub.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then perform the following operations:\n",
    "- transform the `tokens` column of the datasets by tokenizing further each word into sub-word units\n",
    "- transform the `labels` and `ner_tags` columns of the dataset to align with newly created tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_I_mapping = {l: 'I'+l[1:] for l in class_labels.names if l.startswith('B-')}\n",
    "B_I_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDtsaJeVIrJT",
    "outputId": "aa4734bf-4ef5-4437-9948-2c16363da719"
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    function = lambda examples: tokenize_and_align_categories(tokenizer, examples, B_I_mapping), \n",
    "    batched  = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By contrast to the generic BIO annotated data, this new data depends on the tokenizer, and is therefore _model-specific_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of lengths\n",
    "lengths = [len(ts) for ts in tokenized_datasets['all']['input_ids']]\n",
    "lengths = [l for l in lengths if l<= 100]\n",
    "\n",
    "pd.DataFrame(lengths, columns = ['Length']).plot.hist(bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in ['ids', 'mentions', 'categories', 'tokens', 'token_categories', 'input_ids']:\n",
    "    print(c, ':', tokenized_datasets['tst'][c][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVx71GdAIrJH"
   },
   "source": [
    "## 1.4 Map categories to integers\n",
    "\n",
    "[Table of content](#TOC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.map(\n",
    "    function = lambda examples: create_labels(examples, class_labels), \n",
    "    batched  = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new `labels` column is available in the dataset, and corresponds to the integer index, in the `class_labels` object, of the category of each token (or -100 if the category is None) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in ['ids', 'mentions','categories', 'tokens', 'token_categories', 'input_ids', 'labels']:\n",
    "    print(c, ':', tokenized_datasets['tst'][c][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max([len(c) for c in tokenized_datasets['trn']['input_ids']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"albert\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9qywopnIrJH"
   },
   "source": [
    "# 2. Model finetuning\n",
    "\n",
    "[Table of content](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "545PP3o8IrJV"
   },
   "source": [
    "## 2.1 BERT model for Token Classification\n",
    "\n",
    "[Table of content](#TOC)\n",
    "\n",
    "Downloaded checkpoints are by default stored in HF standard cache, which is in our case `C:\\Users\\me\\.cache\\huggingface\\hub`. This directory can be changed with the `cache_dir` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make model creation & training deterministic\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = os.path.join(path_to_save_mlm, base_model_name, 'model')\n",
    "\n",
    "try:\n",
    "    model = AutoModelForTokenClassification.from_pretrained(base_model_path, label2id = label2id, id2label = id2label)\n",
    "    print('Model loaded from local checkpoint.')\n",
    "except:\n",
    "    model = AutoModelForMaskedLM.from_pretrained(hub_model_name)\n",
    "    model.save_pretrained(base_model_path)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(base_model_path, label2id = label2id, id2label = id2label)\n",
    "    print('Model downloaded from Huggingface model hub.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Compute finetuning metrics\n",
    "\n",
    "[Table of content](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    # training args\n",
    "    learning_rate = 2e-5,\n",
    "    weight_decay = 1e-4,\n",
    "    num_train_epochs = 4,\n",
    "    per_device_train_batch_size = batch_size,\n",
    "    per_device_eval_batch_size = batch_size,\n",
    "\n",
    "    # logging args\n",
    "    output_dir = os.path.join(path_to_save_ner, '_checkpoints'),\n",
    "    logging_dir = os.path.join(path_to_logs, final_model_name, run_name),\n",
    "    evaluation_strategy = 'steps',\n",
    "    save_strategy = 'no',\n",
    "    logging_steps = 500,\n",
    "    report_to = ['tensorboard'],\n",
    "    log_level = 'warning',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer),\n",
    "    train_dataset = tokenized_datasets['trn'],\n",
    "    eval_dataset  = tokenized_datasets['dev'],\n",
    "    compute_metrics = lambda p: compute_metrics_finegrained(p, metric, class_labels.names),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some remarks:\n",
    "\n",
    "- The `data_collator` is the object used to batch elements of the training & evaluation datasets.\n",
    "- The `tokenizer` is provided in order to automatically pad the inputs to the maximum length when batching inputs, and to have it saved along the model, which makes it easier to rerun an interrupted training or reuse the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = trainer.evaluate(eval_dataset = tokenized_datasets['tst'], metric_key_prefix = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k, v in test_results.items():\n",
    "    print(str(k) + ' ' + '-'*(30 - len(k)) + ' {:2f}'.format(100*v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Finetune final model\n",
    "\n",
    "[Table of content](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make model creation & training deterministic\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = os.path.join(path_to_save_mlm, base_model_name, 'model')\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(base_model_path, label2id = label2id, id2label = id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    # training args\n",
    "    learning_rate = 2e-5,\n",
    "    weight_decay = 1e-4,\n",
    "    num_train_epochs = 4,\n",
    "    per_device_train_batch_size = batch_size,\n",
    "    per_device_eval_batch_size = batch_size,\n",
    "\n",
    "    # logging args\n",
    "    output_dir = os.path.join(path_to_save_ner, '_checkpoints'),\n",
    "    evaluation_strategy = 'no',\n",
    "    save_strategy = 'no',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer),\n",
    "    train_dataset = tokenized_datasets['all'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(os.path.join(path_to_save_ner, final_model_name, run_name, 'tokenizer'))\n",
    "model.save_pretrained(os.path.join(path_to_save_ner, final_model_name, run_name, 'model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"inference\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Inference\n",
    "\n",
    "[Table of content](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(os.path.join(path_to_save_ner, final_model_name, run_name, 'tokenizer'))\n",
    "model = AutoModelForTokenClassification.from_pretrained(os.path.join(path_to_save_ner, final_model_name, run_name, 'model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = pipeline(\n",
    "    task = 'ner', \n",
    "    model = model, \n",
    "    tokenizer = tokenizer,\n",
    "    framework = 'pt',\n",
    "    aggregation_strategy = 'simple',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'Radiologic examination of chest'\n",
    "\n",
    "\n",
    "df_ents = parse_trf_ner_output(ner(sent))\n",
    "df_ents = correct_entity_boundaries(sent, df_ents)\n",
    "df_ents = remove_entity_overlaps(df_ents)\n",
    "HTML(render_ner_as_html(sent, df_ents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'Participants with at least two of the following: serum M-protein >= 0.5 g/dL, urine M-protein >=200 mg/24 hours, an involved FLC assay >=100 mg/L, and a serum FLC ratio <0.26 or a serum FLC ratio  >1.65'\n",
    "\n",
    "\n",
    "df_ents = parse_trf_ner_output(ner(sent))\n",
    "df_ents = correct_entity_boundaries(sent, df_ents)\n",
    "df_ents = remove_entity_overlaps(df_ents)\n",
    "HTML(render_ner_as_html(sent, df_ents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'Participants with at least two of the following: Serum M-protein >= 0.5 g/dL measured using serum protein immunoelectrophoresis, Urine M-protein >=200 mg/24 hours measured using urine protein immunoelectrophoresis, Serum free light chain (FLC) assay with an involved FLC assay >=100 mg/L, and a serum FLC ratio < 0.26 or a serum FLC ratio  > 1.65'\n",
    "\n",
    "\n",
    "df_ents = parse_trf_ner_output(ner(sent))\n",
    "df_ents = correct_entity_boundaries(sent, df_ents)\n",
    "df_ents = remove_entity_overlaps(df_ents)\n",
    "HTML(render_ner_as_html(sent, df_ents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'Polyneuropathy of other causes, including but not limited to hereditary demyelinating neuropathies, neuropathies secondary to infection or systemic disease, diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor neuropathy, monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory CIDP and acquired demyelinating symmetric (DADS) neuropathy (also known as distal CIDP).'\n",
    "\n",
    "df_ents = parse_trf_ner_output(ner(sent))\n",
    "df_ents = correct_entity_boundaries(sent, df_ents)\n",
    "df_ents = remove_entity_overlaps(df_ents)\n",
    "HTML(render_ner_as_html(sent, df_ents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent = 'Adults and adolescent patients with a physician diagnosis of asthma ' +\\\n",
    "    'for ≥12 months, based on the Global Initiative for Asthma (GINA) 2014 Guidelines ' +\\\n",
    "    'and the followingcriteria: A) Existing treatment with medium to high dose ICS ' +\\\n",
    "    '(≥250 mcg of fluticasone propionate twice daily or equipotent ICS daily dosage ' +\\\n",
    "    'to a maximum of 2000 mcg/day of fluticasone propionate or equivalent) in combination ' +\\\n",
    "    'with a second controller (eg, LABA, LTRA) for at least 3 months with a stable dose ≥1 ' +\\\n",
    "    'month prior to Visit 1.'\n",
    "\n",
    "df_ents = parse_trf_ner_output(ner(sent))\n",
    "df_ents = correct_entity_boundaries(sent, df_ents)\n",
    "df_ents = remove_entity_overlaps(df_ents)\n",
    "HTML(render_ner_as_html(sent, df_ents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'Biologics treatment: Cell-depleting agents, eg. Rituximab Drug within 6 months before baseline or until lymphocyte count returns to normal, Other biologics: within 5 half-lives or 16 weeks prior baseline.'\n",
    "\n",
    "df_ents = parse_trf_ner_output(ner(sent))\n",
    "df_ents = correct_entity_boundaries(sent, df_ents)\n",
    "df_ents = remove_entity_overlaps(df_ents)\n",
    "HTML(render_ner_as_html(sent, df_ents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sents = '''Active hepatitis or patients with positive HBsAg, or patients with positive HBcAb plus positive HBV DNA, or positive HCV antibody (confirmed with presence of HCV RNA if needed) at screening.\n",
    "\n",
    "History of alcohol or drug abuse within 2 years of the screening visit.\n",
    "\n",
    "History of HIV infection or positive HIV serology at screening.\n",
    "\n",
    "History of malignancy within 5 years before screening, except completely treated cervix carcinoma, completely treated and resolved non-metastatic squamous or basal cell carcinoma of the skin.\n",
    "\n",
    "Infection requires systemic antibiotics, antivirals, antiparasitics, antiprotozoals, antifungals treatment within 2 weeks before baseline, superficial skin infections within 1 week before baseline visit.\n",
    "\n",
    "Initiation AD treatment with prescription moisturizers or moisturizers containing  additives such as ceramide, hyaluronic acid, urea, or filaggrin degradation products during the screening period.\n",
    "\n",
    "Known or suspected history of immunosuppression, including history of invasive opportunistic infections, despite infection resolution, or unusually frequent, recurrent, or prolonged infections.\n",
    "'''\n",
    "df_list = []\n",
    "html = ''\n",
    "for sent in sents.split('\\n'):\n",
    "    if sent.strip():\n",
    "        ents = parse_trf_ner_output(ner(sent))\n",
    "        ents = correct_entity_boundaries(sent, ents)\n",
    "        ents = remove_entity_overlaps(ents)\n",
    "        df_list.append(ents)\n",
    "        html += render_ner_as_html(sent, ents)\n",
    "\n",
    "df_ents = pd.concat(df_list, ignore_index = True)\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Table of content](#TOC)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Token Classification",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
