{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Deep Learning for NLP 2.0\n",
    "  </div> \n",
    "\n",
    "\n",
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "    <font color=orange>I - 1</font> Tokenization\n",
    "  </div> \n",
    "\n",
    "  <div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: center; \n",
    "      padding: 15px;\">\n",
    "  </div> \n",
    "\n",
    "\n",
    "  <div style=\" float:right; \n",
    "      font-size: 12px; \n",
    "      line-height: 12px; \n",
    "  padding: 10px 15px 8px;\">\n",
    "  Jean-baptiste AUJOGUE\n",
    "  </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<div style=\"font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "      Table of Content\n",
    "  </div> \n",
    "\n",
    "$\\qquad \\qquad \\bullet$ [Regex](#regex) <br>\n",
    "$\\qquad \\qquad \\bullet$ [NLTK](#nltk) <br>\n",
    "$\\qquad \\qquad \\bullet$ [Spacy](#spacy) <br>\n",
    "$\\qquad \\qquad \\bullet$ [WordPiece](#word_piece) <br>\n",
    "$\\qquad \\qquad \\bullet$ [Byte Pair Encoding](#bpe) <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import unicodedata\n",
    "import torch\n",
    "import inspect\n",
    "import multiprocessing\n",
    "\n",
    "# data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import LineByLineTextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "# tokenizers\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "# base tokenizers\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import (\n",
    "    BPE,\n",
    "    Unigram,\n",
    "    WordLevel,\n",
    "    WordPiece,\n",
    ") \n",
    "# base tokenizer trainers\n",
    "from tokenizers.trainers import (\n",
    "    BpeTrainer,\n",
    "    UnigramTrainer,\n",
    "    WordLevelTrainer,\n",
    "    WordPieceTrainer,\n",
    ")\n",
    "# other special tokenizers, see the list at https://github.com/huggingface/tokenizers/tree/master/bindings/python/py_src/tokenizers/implementations\n",
    "from tokenizers.implementations import (\n",
    "    CharBPETokenizer,          # The original BPE\n",
    "    ByteLevelBPETokenizer,     # The byte level version of the BPE\n",
    "    SentencePieceBPETokenizer, # A BPE implementation compatible with the one used by SentencePiece\n",
    "    BertWordPieceTokenizer,    # The famous Bert tokenizer, using WordPiece\n",
    ") \n",
    "# model-specific tokenizers\n",
    "from transformers import GPT2TokenizerFast, RobertaTokenizerFast, DebertaV2Tokenizer\n",
    "\n",
    "# token embedding\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "\n",
    "# models\n",
    "from transformers import RobertaConfig, RobertaModel, RobertaForMaskedLM, pipeline\n",
    "\n",
    "# model trainer\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version : 3.6.12 |Anaconda, Inc.| (default, Sep  9 2020, 00:29:25) [MSC v.1916 64 bit (AMD64)]\n",
      "pytorch version : 1.5.0\n",
      "DL device : cuda\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import sys\n",
    "import warnings\n",
    "import os\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "from unidecode import unidecode\n",
    "import itertools\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# for special math operation\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "# for manipulating data \n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=np.nan)\n",
    "import pandas as pd\n",
    "import bcolz # see https://bcolz.readthedocs.io/en/latest/intro.html\n",
    "import pickle\n",
    "\n",
    "\n",
    "# for text processing\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "#import spacy\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "# for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print('python version :', sys.version)\n",
    "print('pytorch version :', torch.__version__)\n",
    "print('DL device :', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_repo = os.path.dirname(os.getcwd())\n",
    "path_to_data = os.path.join(path_to_repo, 'data', 'news_auto')\n",
    "path_to_save = os.path.join(path_to_repo, 'saves', 'news_auto')\n",
    "#path_to_save = os.path.join(path_to_repo, 'saves', '20_news_group')\n",
    "\n",
    "path_to_roberta = os.path.join(path_to_save, 'roberta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"corpus\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Corpus\n",
    "\n",
    "[Back to top](#plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\bullet$ News Auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_newsauto = pd.read_excel(os.path.join(path_to_data, \"news_auto_texts.xlsx\"), engine = 'openpyxl')\n",
    "df_newsauto = df_newsauto.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "529 59\n"
     ]
    }
   ],
   "source": [
    "urls_root = list(set(['/'.join(url.split('/')[:3]) for url in df_newsauto.url.tolist()]))\n",
    "\n",
    "urls_trn, urls_tst = train_test_split(urls_root, test_size = 0.1, random_state = 4242)\n",
    "print(len(urls_trn), len(urls_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3590 275\n"
     ]
    }
   ],
   "source": [
    "filter_trn = df_newsauto.url.apply(lambda url: '/'.join(url.split('/')[:3]) in urls_trn)\n",
    "filter_tst = df_newsauto.url.apply(lambda url: '/'.join(url.split('/')[:3]) in urls_tst)\n",
    "\n",
    "texts_trn = [t for t in df_newsauto[filter_trn].text.tolist() if t]\n",
    "texts_tst = [t for t in df_newsauto[filter_tst].text.tolist() if t]\n",
    "print(len(texts_trn), len(texts_tst))\n",
    "\n",
    "texts_trn = unicodedata.normalize(\"NFKD\", '\\n\\n'.join(texts_trn))\n",
    "texts_tst = unicodedata.normalize(\"NFKD\", '\\n\\n'.join(texts_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_to_data, \"news_auto_texts_trn.txt\"), \"w\", encoding='utf-8') as text_file:\n",
    "    text_file.write(texts_trn)\n",
    "\n",
    "with open(os.path.join(path_to_data, \"news_auto_texts_tst.txt\"), \"w\", encoding='utf-8') as text_file:\n",
    "    text_file.write(texts_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-import texts\n",
    "with open(os.path.join(path_to_data, \"news_auto_texts_trn.txt\"), \"r\", encoding='utf-8') as text_file:\n",
    "    texts_trn = text_file.read()\n",
    "    texts_trn = texts_trn.split('\\n\\n')\n",
    "\n",
    "with open(os.path.join(path_to_data, \"news_auto_texts_tst.txt\"), \"r\", encoding='utf-8') as text_file:\n",
    "    texts_tst = text_file.read()\n",
    "    texts_tst = texts_tst.split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3590, 275)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_trn), len(texts_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\bullet$ 20 news group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_trn = fetch_20newsgroups(subset = 'train', remove = ('headers', 'footers', 'quotes'))\n",
    "newsgroups_tst = fetch_20newsgroups(subset = 'test',  remove = ('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip()) # \n",
    "    return s\n",
    "\n",
    "def cleanNum(s) :\n",
    "    s = re.sub('[\\.!?]+ ', ' . ', s)\n",
    "    s = re.sub(',+ ', ' , ', s)\n",
    "    s = re.sub(' [0-9]*\\.[0-9] ', ' FLOAT ', ' ' + s + ' ').strip()\n",
    "    s = re.sub(' [0-9,]*[0-9] ', ' INT ', ' ' + s + ' ').strip()\n",
    "    return s\n",
    "\n",
    "def trueWord(w) :\n",
    "    return len(w)>0 and re.sub('[^a-zA-Z0-9.,]', '', w) != ''\n",
    "\n",
    "def prepareCorpus(corpus) :\n",
    "    corpus = [normalizeString(s) for s in corpus]\n",
    "    corpus = [cleanNum(s) for s in corpus]\n",
    "    corpus = [nltk.tokenize.word_tokenize(s) for s in corpus]\n",
    "    corpus = [[w for w in s if trueWord(w)] for s in corpus]\n",
    "    corpus = [s for s in corpus if len(s) < 1000] # a lot of crapy words are accumulated among few texts\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = prepareCorpus(newsgroups_trn.data)\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tokenizers\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Tokenizers\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "### $\\bullet$ Pretrained tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use pre-fitted tokenizer for DeBERTa (BERT-like adaptation of GPT2 tokenizer, a particular BPE tokenizer)\n",
    "# tokenizer = DebertaV2Tokenizer.from_pretrained(\"microsoft/deberta-v2-xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\bullet$ GPT-2 tokenizer\n",
    "\n",
    "Tokenizer based on a byte-level BPE tokenizer. The list of special tokens is determined by the [GPT2TokenizerFast](https://huggingface.co/transformers/_modules/transformers/models/gpt2/tokenization_gpt2_fast.html#GPT2TokenizerFast) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Jb\\\\Desktop\\\\NLP\\\\BERTology\\\\saves\\\\news_auto\\\\gpt2\\\\vocabulary\\\\vocab.json',\n",
       " 'C:\\\\Users\\\\Jb\\\\Desktop\\\\NLP\\\\BERTology\\\\saves\\\\news_auto\\\\gpt2\\\\vocabulary\\\\merges.txt']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_vocab_gpt2 = os.path.join(path_to_save, 'gpt2', 'vocabulary')\n",
    "\n",
    "# instantiate, fit and export GPT2 tokenizer\n",
    "tokenizer_gpt2 = ByteLevelBPETokenizer()\n",
    "tokenizer_gpt2.train(\n",
    "    files = os.path.join(path_to_data, \"news_auto_texts_trn.txt\"), \n",
    "    vocab_size = 5000,\n",
    "    special_tokens = ['<|endoftext|>'],\n",
    ")\n",
    "tokenizer_gpt2.save_model(path_to_vocab_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-import tokenizer into a more suitable class for model training\n",
    "tokenizer_gpt2 = GPT2TokenizerFast.from_pretrained(path_to_vocab_gpt2, max_len = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, y'all! How are you ?\n",
      "<bound method BatchEncoding.tokens of {'input_ids': [40, 457, 79, 12, 569, 7, 499, 1, 2604, 419, 817, 221, 31], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}>\n"
     ]
    }
   ],
   "source": [
    "# \"begin of word\" is encoded by the special 'ƒ†' character, see\n",
    "# https://discuss.huggingface.co/t/bpe-tokenizers-and-spaces-before-words/475\n",
    "txt = \"Hello, y'all! How are you ?\"\n",
    "output = tokenizer_gpt2(txt)\n",
    "tokens = output.tokens\n",
    "\n",
    "print(txt)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\bullet$ Roberta tokenizer\n",
    "\n",
    "Roberta tokenizer is a subclass of GPT2 tokenizer. The list of special tokens is determined by the [RobertaTokenizerFast](https://huggingface.co/transformers/_modules/transformers/models/roberta/tokenization_roberta_fast.html#RobertaTokenizerFast) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_vocab_roberta = os.path.join(path_to_roberta, 'vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Jb\\\\Desktop\\\\NLP\\\\BERTology\\\\data\\\\vocab\\\\news_auto_roberta\\\\vocab.json',\n",
       " 'C:\\\\Users\\\\Jb\\\\Desktop\\\\NLP\\\\BERTology\\\\data\\\\vocab\\\\news_auto_roberta\\\\merges.txt']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate, fit and export Roberta tokenizer\n",
    "vocab_size = 5000\n",
    "special_tokens = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
    "\n",
    "tokenizer_roberta = ByteLevelBPETokenizer()\n",
    "tokenizer_roberta.train(\n",
    "    files = os.path.join(path_to_data, \"news_auto_texts_trn.txt\"), \n",
    "    vocab_size = vocab_size,\n",
    "    special_tokens = special_tokens,\n",
    ")\n",
    "tokenizer_roberta.save_model(path_to_vocab_roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-import tokenizer into a more suitable class for model training\n",
    "tokenizer_roberta = RobertaTokenizerFast.from_pretrained(path_to_vocab_roberta, max_len = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\Jb\\\\Desktop\\\\NLP\\\\BERTology\\\\saves\\\\Tokenizer\\\\news_auto_roberta\\\\tokenizer_config.json',\n",
       " 'C:\\\\Users\\\\Jb\\\\Desktop\\\\NLP\\\\BERTology\\\\saves\\\\Tokenizer\\\\news_auto_roberta\\\\special_tokens_map.json',\n",
       " 'C:\\\\Users\\\\Jb\\\\Desktop\\\\NLP\\\\BERTology\\\\saves\\\\Tokenizer\\\\news_auto_roberta\\\\vocab.json',\n",
       " 'C:\\\\Users\\\\Jb\\\\Desktop\\\\NLP\\\\BERTology\\\\saves\\\\Tokenizer\\\\news_auto_roberta\\\\merges.txt',\n",
       " 'C:\\\\Users\\\\Jb\\\\Desktop\\\\NLP\\\\BERTology\\\\saves\\\\Tokenizer\\\\news_auto_roberta\\\\added_tokens.json')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_roberta.save_pretrained(os.path.join(path_to_roberta, \"tokenizer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, y'all! How are you ?\n",
      "<bound method BatchEncoding.tokens of {'input_ids': [0, 44, 461, 83, 16, 573, 11, 503, 5, 2608, 423, 821, 225, 35, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}>\n"
     ]
    }
   ],
   "source": [
    "txt = \"Hello, y'all! How are you ?\"\n",
    "output = tokenizer_roberta(txt)\n",
    "tokens = output.tokens\n",
    "\n",
    "print(txt)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\bullet$ Deberta-v2 tokenizer\n",
    "\n",
    "Tokenizer based on _SentencePiece_. The list of special tokens is determined by the [DebertaV2Tokenizer](https://huggingface.co/transformers/_modules/transformers/models/deberta_v2/tokenization_deberta_v2.html#DebertaV2Tokenizer) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Jb\\\\Desktop\\\\NLP\\\\BERTology\\\\saves\\\\news_auto\\\\deberta_v2\\\\vocabulary\\\\vocab.json',\n",
       " 'C:\\\\Users\\\\Jb\\\\Desktop\\\\NLP\\\\BERTology\\\\saves\\\\news_auto\\\\deberta_v2\\\\vocabulary\\\\merges.txt']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_vocab_deberta_v2 = os.path.join(path_to_save, 'deberta_v2', 'vocabulary')\n",
    "\n",
    "# instantiate, fit and export Deberta-v2 tokenizer\n",
    "vocab_size = 5000\n",
    "special_tokens = [\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\"]\n",
    "\n",
    "tokenizer_debertav2 = SentencePieceBPETokenizer()\n",
    "tokenizer_debertav2.train(\n",
    "    files = os.path.join(path_to_data, \"news_auto_texts_trn.txt\"), \n",
    "    vocab_size = vocab_size,\n",
    "    special_tokens = special_tokens,\n",
    ")\n",
    "tokenizer_debertav2.save_model(path_to_vocab_deberta_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #re-import tokenizer into a more suitable class for model training\n",
    "# not working_\n",
    "# tokenizer_debertav2 = DebertaV2Tokenizer.from_pretrained(path_to_vocab_deberta_v2, max_len = 512)\n",
    "# not working either\n",
    "# tokenizer_debertav2 = DebertaV2Tokenizer(vocab_file = os.path.join(path_to_vocab_deberta_v2, 'vocab.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\bullet$ Custom tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tokenizer(Model, Trainer, vocab_size, special_tokens):\n",
    "    model = (\n",
    "        Model(unk_token = \"[UNK]\")\n",
    "        if 'unk_token' in inspect.signature(Model).parameters # for Unigram model\n",
    "        else Model()\n",
    "    )\n",
    "    # instantiate tokenizer, and fit using external trainer\n",
    "    tokenizer = Tokenizer(model)\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    trainer = Trainer(vocab_size = vocab_size, special_tokens = special_tokens)\n",
    "    tokenizer.train(\n",
    "        files = [os.path.join(path_to_data, \"news_auto_texts_trn.txt\")], \n",
    "        trainer = trainer,\n",
    "    )\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "special_tokens = [\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\"]\n",
    "\n",
    "bpe_tokenizer = fit_tokenizer(BPE, BpeTrainer, vocab_size, special_tokens)\n",
    "wdl_tokenizer = fit_tokenizer(WordLevel, WordLevelTrainer, vocab_size, special_tokens)\n",
    "wdp_tokenizer = fit_tokenizer(WordPiece, WordPieceTrainer, vocab_size, special_tokens)\n",
    "uni_tokenizer = fit_tokenizer(Unigram, UnigramTrainer, vocab_size, special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_bpe = os.path.join(path_to_data, 'vocab', 'news_auto_bpe.json')\n",
    "\n",
    "# save\n",
    "bpe_tokenizer.save(path_to_bpe, pretty = True)\n",
    "\n",
    "# load as base Tokenizer\n",
    "bpe_tokenizer = Tokenizer.from_file(path_to_bpe)\n",
    "\n",
    "# load as tokenizer suitable for model training\n",
    "tokenizer_bpe = PreTrainedTokenizerFast(tokenizer_file = path_to_bpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, y'all! How are you üòÅ? wanna work at Toyota?\n",
      "<bound method BatchEncoding.tokens of {'input_ids': [44, 1034, 1067, 16, 93, 11, 1121, 5, 2512, 1092, 1458, 0, 35, 91, 1287, 69, 1223, 1013, 4194, 35], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}>\n"
     ]
    }
   ],
   "source": [
    "txt = \"Hello, y'all! How are you üòÅ? wanna work at Toyota?\"\n",
    "output = tokenizer_bpe(txt)\n",
    "tokens = output.tokens\n",
    "\n",
    "print(txt)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
