{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 35px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "      Tokenization\n",
    "  </div> \n",
    "  \n",
    "<div style=\"\n",
    "      font-weight: normal; \n",
    "      font-size: 25px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "      Pretrained Tokenizers\n",
    "  </div> \n",
    "\n",
    "\n",
    "  <div style=\"\n",
    "      font-size: 15px; \n",
    "      line-height: 12px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Jean-baptiste AUJOGUE\n",
    "  </div> \n",
    "\n",
    "  \n",
    "  <div style=\" float:right; \n",
    "      font-size: 12px; \n",
    "      line-height: 12px; \n",
    "  padding: 10px 15px 8px;\">\n",
    "  December 2022\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<div style=\"font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "      Table of Content\n",
    "  </div> \n",
    "\n",
    "\n",
    "#### References\n",
    "\n",
    "- Huggingface [tutorial notebook](https://github.com/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb) on tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "      Packages\n",
    "  </div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import unicodedata\n",
    "import inspect\n",
    "import multiprocessing\n",
    "\n",
    "# data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import LineByLineTextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "# tokenizers\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "# base tokenizers\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import (\n",
    "    BPE,\n",
    "    Unigram,\n",
    "    WordLevel,\n",
    "    WordPiece,\n",
    ") \n",
    "# base tokenizer trainers\n",
    "from tokenizers.trainers import (\n",
    "    BpeTrainer,\n",
    "    UnigramTrainer,\n",
    "    WordLevelTrainer,\n",
    "    WordPieceTrainer,\n",
    ")\n",
    "# other special tokenizers, see the list at \n",
    "# https://github.com/huggingface/tokenizers/tree/master/bindings/python/py_src/tokenizers/implementations\n",
    "from tokenizers.implementations import (\n",
    "    CharBPETokenizer,          # The original BPE\n",
    "    ByteLevelBPETokenizer,     # The byte level version of the BPE\n",
    "    SentencePieceBPETokenizer, # A BPE implementation compatible with the one used by SentencePiece\n",
    "    BertWordPieceTokenizer,    # The famous Bert tokenizer, using WordPiece\n",
    ") \n",
    "# model-specific tokenizers\n",
    "from transformers import GPT2TokenizerFast, RobertaTokenizerFast, DebertaV2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_repo = os.path.dirname(os.getcwd())\n",
    "path_to_save = os.path.join(path_to_repo, 'saves', '20_news_group')\n",
    "\n",
    "path_to_roberta = os.path.join(path_to_save, 'roberta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"corpus\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Corpus\n",
    "\n",
    "[Back to top](#plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\bullet$ 20 news group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_trn = fetch_20newsgroups(subset = 'train', remove = ('headers', 'footers', 'quotes'))\n",
    "newsgroups_tst = fetch_20newsgroups(subset = 'test',  remove = ('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip()) # \n",
    "    return s\n",
    "\n",
    "def cleanNum(s) :\n",
    "    s = re.sub('[\\.!?]+ ', ' . ', s)\n",
    "    s = re.sub(',+ ', ' , ', s)\n",
    "    s = re.sub(' [0-9]*\\.[0-9] ', ' FLOAT ', ' ' + s + ' ').strip()\n",
    "    s = re.sub(' [0-9,]*[0-9] ', ' INT ', ' ' + s + ' ').strip()\n",
    "    return s\n",
    "\n",
    "def trueWord(w) :\n",
    "    return len(w)>0 and re.sub('[^a-zA-Z0-9.,]', '', w) != ''\n",
    "\n",
    "def prepareCorpus(corpus) :\n",
    "    corpus = [normalizeString(s) for s in corpus]\n",
    "    corpus = [cleanNum(s) for s in corpus]\n",
    "    corpus = [nltk.tokenize.word_tokenize(s) for s in corpus]\n",
    "    corpus = [[w for w in s if trueWord(w)] for s in corpus]\n",
    "    corpus = [s for s in corpus if len(s) < 1000] # a lot of crapy words are accumulated among few texts\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = prepareCorpus(newsgroups_trn.data)\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tokenizers\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Tokenizers\n",
    "\n",
    "[Back to top](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use pre-fitted tokenizer for DeBERTa (BERT-like adaptation of GPT2 tokenizer, a particular BPE tokenizer)\n",
    "# tokenizer = DebertaV2Tokenizer.from_pretrained(\"microsoft/deberta-v2-xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\bullet$ GPT-2 tokenizer\n",
    "\n",
    "Tokenizer based on a byte-level BPE tokenizer. The list of special tokens is determined by the [GPT2TokenizerFast](https://huggingface.co/transformers/_modules/transformers/models/gpt2/tokenization_gpt2_fast.html#GPT2TokenizerFast) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_vocab_gpt2 = os.path.join(path_to_save, 'gpt2', 'vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Jb\\\\Desktop\\\\NLP\\\\BERTology\\\\saves\\\\news_auto\\\\gpt2\\\\vocabulary\\\\vocab.json',\n",
       " 'C:\\\\Users\\\\Jb\\\\Desktop\\\\NLP\\\\BERTology\\\\saves\\\\news_auto\\\\gpt2\\\\vocabulary\\\\merges.txt']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate, fit and export GPT2 tokenizer\n",
    "tokenizer_gpt2 = ByteLevelBPETokenizer()\n",
    "tokenizer_gpt2.train(\n",
    "    files = os.path.join(path_to_data, \"news_auto_texts_trn.txt\"), \n",
    "    vocab_size = 5000,\n",
    "    special_tokens = ['<|endoftext|>'],\n",
    ")\n",
    "tokenizer_gpt2.save_model(path_to_vocab_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-import tokenizer into a more suitable class for model training\n",
    "tokenizer_gpt2 = GPT2TokenizerFast.from_pretrained(path_to_vocab_gpt2, max_len = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, y'all! How are you ?\n",
      "<bound method BatchEncoding.tokens of {'input_ids': [40, 457, 79, 12, 569, 7, 499, 1, 2604, 419, 817, 221, 31], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}>\n"
     ]
    }
   ],
   "source": [
    "# \"begin of word\" is encoded by the special 'Ä ' character, see\n",
    "# https://discuss.huggingface.co/t/bpe-tokenizers-and-spaces-before-words/475\n",
    "txt = \"Hello, y'all! How are you ?\"\n",
    "output = tokenizer_gpt2(txt)\n",
    "tokens = output.tokens\n",
    "\n",
    "print(txt)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\bullet$ Roberta tokenizer\n",
    "\n",
    "Roberta tokenizer is a subclass of GPT2 tokenizer. The list of special tokens is determined by the [RobertaTokenizerFast](https://huggingface.co/transformers/_modules/transformers/models/roberta/tokenization_roberta_fast.html#RobertaTokenizerFast) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_vocab_roberta = os.path.join(path_to_roberta, 'vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Jb\\\\Desktop\\\\NLP\\\\BERTology\\\\data\\\\vocab\\\\news_auto_roberta\\\\vocab.json',\n",
       " 'C:\\\\Users\\\\Jb\\\\Desktop\\\\NLP\\\\BERTology\\\\data\\\\vocab\\\\news_auto_roberta\\\\merges.txt']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate, fit and export Roberta tokenizer\n",
    "vocab_size = 5000\n",
    "special_tokens = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
    "\n",
    "tokenizer_roberta = ByteLevelBPETokenizer()\n",
    "tokenizer_roberta.train(\n",
    "    files = os.path.join(path_to_data, \"news_auto_texts_trn.txt\"), \n",
    "    vocab_size = vocab_size,\n",
    "    special_tokens = special_tokens,\n",
    ")\n",
    "tokenizer_roberta.save_model(path_to_vocab_roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-import tokenizer into a more suitable class for model training\n",
    "tokenizer_roberta = RobertaTokenizerFast.from_pretrained(path_to_vocab_roberta, max_len = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\Jb\\\\Desktop\\\\NLP\\\\BERTology\\\\saves\\\\Tokenizer\\\\news_auto_roberta\\\\tokenizer_config.json',\n",
       " 'C:\\\\Users\\\\Jb\\\\Desktop\\\\NLP\\\\BERTology\\\\saves\\\\Tokenizer\\\\news_auto_roberta\\\\special_tokens_map.json',\n",
       " 'C:\\\\Users\\\\Jb\\\\Desktop\\\\NLP\\\\BERTology\\\\saves\\\\Tokenizer\\\\news_auto_roberta\\\\vocab.json',\n",
       " 'C:\\\\Users\\\\Jb\\\\Desktop\\\\NLP\\\\BERTology\\\\saves\\\\Tokenizer\\\\news_auto_roberta\\\\merges.txt',\n",
       " 'C:\\\\Users\\\\Jb\\\\Desktop\\\\NLP\\\\BERTology\\\\saves\\\\Tokenizer\\\\news_auto_roberta\\\\added_tokens.json')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_roberta.save_pretrained(os.path.join(path_to_roberta, \"tokenizer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, y'all! How are you ?\n",
      "<bound method BatchEncoding.tokens of {'input_ids': [0, 44, 461, 83, 16, 573, 11, 503, 5, 2608, 423, 821, 225, 35, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}>\n"
     ]
    }
   ],
   "source": [
    "txt = \"Hello, y'all! How are you ?\"\n",
    "output = tokenizer_roberta(txt)\n",
    "tokens = output.tokens\n",
    "\n",
    "print(txt)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\bullet$ Deberta-v2 tokenizer\n",
    "\n",
    "Tokenizer based on _SentencePiece_. The list of special tokens is determined by the [DebertaV2Tokenizer](https://huggingface.co/transformers/_modules/transformers/models/deberta_v2/tokenization_deberta_v2.html#DebertaV2Tokenizer) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Jb\\\\Desktop\\\\NLP\\\\BERTology\\\\saves\\\\news_auto\\\\deberta_v2\\\\vocabulary\\\\vocab.json',\n",
       " 'C:\\\\Users\\\\Jb\\\\Desktop\\\\NLP\\\\BERTology\\\\saves\\\\news_auto\\\\deberta_v2\\\\vocabulary\\\\merges.txt']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_vocab_deberta_v2 = os.path.join(path_to_save, 'deberta_v2', 'vocabulary')\n",
    "\n",
    "# instantiate, fit and export Deberta-v2 tokenizer\n",
    "vocab_size = 5000\n",
    "special_tokens = [\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\"]\n",
    "\n",
    "tokenizer_debertav2 = SentencePieceBPETokenizer()\n",
    "tokenizer_debertav2.train(\n",
    "    files = os.path.join(path_to_data, \"news_auto_texts_trn.txt\"), \n",
    "    vocab_size = vocab_size,\n",
    "    special_tokens = special_tokens,\n",
    ")\n",
    "tokenizer_debertav2.save_model(path_to_vocab_deberta_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #re-import tokenizer into a more suitable class for model training\n",
    "# not working_\n",
    "# tokenizer_debertav2 = DebertaV2Tokenizer.from_pretrained(path_to_vocab_deberta_v2, max_len = 512)\n",
    "# not working either\n",
    "# tokenizer_debertav2 = DebertaV2Tokenizer(vocab_file = os.path.join(path_to_vocab_deberta_v2, 'vocab.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\bullet$ Custom tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tokenizer(Model, Trainer, vocab_size, special_tokens):\n",
    "    model = (\n",
    "        Model(unk_token = \"[UNK]\")\n",
    "        if 'unk_token' in inspect.signature(Model).parameters # for Unigram model\n",
    "        else Model()\n",
    "    )\n",
    "    # instantiate tokenizer, and fit using external trainer\n",
    "    tokenizer = Tokenizer(model)\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    trainer = Trainer(vocab_size = vocab_size, special_tokens = special_tokens)\n",
    "    tokenizer.train(\n",
    "        files = [os.path.join(path_to_data, \"news_auto_texts_trn.txt\")], \n",
    "        trainer = trainer,\n",
    "    )\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "special_tokens = [\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\"]\n",
    "\n",
    "bpe_tokenizer = fit_tokenizer(BPE, BpeTrainer, vocab_size, special_tokens)\n",
    "wdl_tokenizer = fit_tokenizer(WordLevel, WordLevelTrainer, vocab_size, special_tokens)\n",
    "wdp_tokenizer = fit_tokenizer(WordPiece, WordPieceTrainer, vocab_size, special_tokens)\n",
    "uni_tokenizer = fit_tokenizer(Unigram, UnigramTrainer, vocab_size, special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_bpe = os.path.join(path_to_data, 'vocab', 'news_auto_bpe.json')\n",
    "\n",
    "# save\n",
    "bpe_tokenizer.save(path_to_bpe, pretty = True)\n",
    "\n",
    "# load as base Tokenizer\n",
    "bpe_tokenizer = Tokenizer.from_file(path_to_bpe)\n",
    "\n",
    "# load as tokenizer suitable for model training\n",
    "tokenizer_bpe = PreTrainedTokenizerFast(tokenizer_file = path_to_bpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, y'all! How are you ð? wanna work at Toyota?\n",
      "<bound method BatchEncoding.tokens of {'input_ids': [44, 1034, 1067, 16, 93, 11, 1121, 5, 2512, 1092, 1458, 0, 35, 91, 1287, 69, 1223, 1013, 4194, 35], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}>\n"
     ]
    }
   ],
   "source": [
    "txt = \"Hello, y'all! How are you ð? wanna work at Toyota?\"\n",
    "output = tokenizer_bpe(txt)\n",
    "tokens = output.tokens\n",
    "\n",
    "print(txt)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
