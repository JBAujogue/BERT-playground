{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4cRE8IbIrIV"
   },
   "source": [
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 35px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "      Sequence Labelling - MLM\n",
    "  </div> \n",
    "  \n",
    "<div style=\"\n",
    "      font-weight: normal; \n",
    "      font-size: 25px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "      Finetuning on Clinical Trials (Albert-base)\n",
    "  </div> \n",
    "\n",
    "\n",
    "  <div style=\"\n",
    "      font-size: 15px; \n",
    "      line-height: 12px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Jean-baptiste AUJOGUE - Hybrid Intelligence\n",
    "  </div> \n",
    "\n",
    "  \n",
    "  <div style=\" float:right; \n",
    "      font-size: 12px; \n",
    "      line-height: 12px; \n",
    "  padding: 10px 15px 8px;\">\n",
    "  December 2022\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"TOC\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table Of Content\n",
    "\n",
    "1. [Dataset](#data) <br>\n",
    "2. [ALBERT finetuning](#albert) <br>\n",
    "3. [Inference](#inference) <br>\n",
    "\n",
    "\n",
    "\n",
    "#### Reference\n",
    "\n",
    "- Hugginface full list of [tutorial notebooks](https://github.com/huggingface/transformers/tree/main/notebooks) (see also [here](https://huggingface.co/docs/transformers/main/notebooks#pytorch-examples))\n",
    "- Huggingface full list of [training scripts](https://github.com/huggingface/transformers/tree/main/examples/pytorch)\n",
    "- Huggingface [tutorial notebook](https://github.com/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch.ipynb) on language models\n",
    "- Huggingface [course](https://huggingface.co/course/chapter7/3?fw=tf) on language models\n",
    "- Huggingface [training script](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py) on language models\n",
    "- Albert [original training protocol](https://github.com/google-research/albert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "import copy\n",
    "import string\n",
    "from itertools import chain\n",
    "\n",
    "# data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import (\n",
    "    Dataset, \n",
    "    DatasetDict,\n",
    "    ClassLabel, \n",
    "    Features, \n",
    "    Sequence, \n",
    "    Value,\n",
    ")\n",
    "from transformers import AlbertConfig, AutoConfig, DataCollatorForLanguageModeling\n",
    "\n",
    "# DL\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForMaskedLM, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    pipeline,\n",
    "    set_seed,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "# viz\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformers settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.22.2'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make training deterministic\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom paths & imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_repo = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "path_to_data = os.path.join(path_to_repo, 'datasets')\n",
    "path_to_save = os.path.join(path_to_repo, 'saves', 'MLM')\n",
    "path_to_src  = os.path.join(path_to_repo, 'src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, path_to_src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'clinical-trials-ctti'\n",
    "base_model_name = \"albert-base-v2\"\n",
    "final_model_name = \"albert-base-clinical-trials\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "# 1. Dataset\n",
    "\n",
    "[Table of content](#TOC)\n",
    "\n",
    "We generate a collection of instances of the `datasets.Dataset` class. \n",
    "\n",
    "Note that these are different from the fairly generic `torch.utils.data.Dataset` class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load Clinical Trials corpus\n",
    "\n",
    "[Table of content](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Description</th>\n",
       "      <th>IE_criteria</th>\n",
       "      <th>Condition</th>\n",
       "      <th>Purpose</th>\n",
       "      <th>Intervention</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NCT0000xxxx/NCT00000102.xml</td>\n",
       "      <td>This study will test the ability of extended r...</td>\n",
       "      <td>This protocol is designed to assess both acute...</td>\n",
       "      <td>diagnosed with Congenital Adrenal Hyperplasia ...</td>\n",
       "      <td>Congenital Adrenal Hyperplasia</td>\n",
       "      <td>Treatment</td>\n",
       "      <td>Nifedipine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NCT0000xxxx/NCT00000104.xml</td>\n",
       "      <td>Inner city children are at an increased risk f...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Lead Poisoning</td>\n",
       "      <td></td>\n",
       "      <td>ERP measures of attention and memory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NCT0000xxxx/NCT00000105.xml</td>\n",
       "      <td>The purpose of this study is to learn how the ...</td>\n",
       "      <td>Patients will receive each vaccine once only c...</td>\n",
       "      <td>Patients must have a diagnosis of cancer of an...</td>\n",
       "      <td>Cancer</td>\n",
       "      <td></td>\n",
       "      <td>Intracel KLH Vaccine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NCT0000xxxx/NCT00000106.xml</td>\n",
       "      <td>Recently a non-toxic system for whole body hyp...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Rheumatic Diseases</td>\n",
       "      <td>Treatment</td>\n",
       "      <td>Whole body hyperthermia unit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NCT0000xxxx/NCT00000107.xml</td>\n",
       "      <td>Adults with cyanotic congenital heart disease ...</td>\n",
       "      <td></td>\n",
       "      <td>Resting blood pressure below 140/90</td>\n",
       "      <td>Heart Defects, Congenital</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NCT0000xxxx/NCT00000108.xml</td>\n",
       "      <td>The purpose of this research is to find out wh...</td>\n",
       "      <td></td>\n",
       "      <td>Postmenopausal and preferably on hormone repla...</td>\n",
       "      <td>Cardiovascular Diseases</td>\n",
       "      <td>Prevention</td>\n",
       "      <td>Exercise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NCT0000xxxx/NCT00000110.xml</td>\n",
       "      <td>The purpose of this pilot investigation is to ...</td>\n",
       "      <td></td>\n",
       "      <td>Healthy volunteers (developmental phase)\\nHeal...</td>\n",
       "      <td>Obesity</td>\n",
       "      <td>Treatment</td>\n",
       "      <td>magnetic resonance spectroscopy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NCT0000xxxx/NCT00000111.xml</td>\n",
       "      <td>The purpose of this study is to see if we can ...</td>\n",
       "      <td></td>\n",
       "      <td>Lack sufficient attached keratinized tissue at...</td>\n",
       "      <td>Mouth Diseases</td>\n",
       "      <td>Treatment</td>\n",
       "      <td>Oral mucosal graft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NCT0000xxxx/NCT00000112.xml</td>\n",
       "      <td>The prevalence of obesity in children is reach...</td>\n",
       "      <td></td>\n",
       "      <td>Obesity: BM +/- 95% for age general good health</td>\n",
       "      <td>Obesity</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NCT0000xxxx/NCT00000113.xml</td>\n",
       "      <td>To evaluate whether progressive addition lense...</td>\n",
       "      <td>Myopia (nearsightedness) is an important publi...</td>\n",
       "      <td></td>\n",
       "      <td>Myopia</td>\n",
       "      <td>Treatment</td>\n",
       "      <td>Progressive Addition Lenses</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Id  \\\n",
       "0  NCT0000xxxx/NCT00000102.xml   \n",
       "1  NCT0000xxxx/NCT00000104.xml   \n",
       "2  NCT0000xxxx/NCT00000105.xml   \n",
       "3  NCT0000xxxx/NCT00000106.xml   \n",
       "4  NCT0000xxxx/NCT00000107.xml   \n",
       "5  NCT0000xxxx/NCT00000108.xml   \n",
       "6  NCT0000xxxx/NCT00000110.xml   \n",
       "7  NCT0000xxxx/NCT00000111.xml   \n",
       "8  NCT0000xxxx/NCT00000112.xml   \n",
       "9  NCT0000xxxx/NCT00000113.xml   \n",
       "\n",
       "                                             Summary  \\\n",
       "0  This study will test the ability of extended r...   \n",
       "1  Inner city children are at an increased risk f...   \n",
       "2  The purpose of this study is to learn how the ...   \n",
       "3  Recently a non-toxic system for whole body hyp...   \n",
       "4  Adults with cyanotic congenital heart disease ...   \n",
       "5  The purpose of this research is to find out wh...   \n",
       "6  The purpose of this pilot investigation is to ...   \n",
       "7  The purpose of this study is to see if we can ...   \n",
       "8  The prevalence of obesity in children is reach...   \n",
       "9  To evaluate whether progressive addition lense...   \n",
       "\n",
       "                                         Description  \\\n",
       "0  This protocol is designed to assess both acute...   \n",
       "1                                                      \n",
       "2  Patients will receive each vaccine once only c...   \n",
       "3                                                      \n",
       "4                                                      \n",
       "5                                                      \n",
       "6                                                      \n",
       "7                                                      \n",
       "8                                                      \n",
       "9  Myopia (nearsightedness) is an important publi...   \n",
       "\n",
       "                                         IE_criteria  \\\n",
       "0  diagnosed with Congenital Adrenal Hyperplasia ...   \n",
       "1                                                      \n",
       "2  Patients must have a diagnosis of cancer of an...   \n",
       "3                                                      \n",
       "4                Resting blood pressure below 140/90   \n",
       "5  Postmenopausal and preferably on hormone repla...   \n",
       "6  Healthy volunteers (developmental phase)\\nHeal...   \n",
       "7  Lack sufficient attached keratinized tissue at...   \n",
       "8    Obesity: BM +/- 95% for age general good health   \n",
       "9                                                      \n",
       "\n",
       "                        Condition     Purpose  \\\n",
       "0  Congenital Adrenal Hyperplasia   Treatment   \n",
       "1                  Lead Poisoning               \n",
       "2                          Cancer               \n",
       "3              Rheumatic Diseases   Treatment   \n",
       "4       Heart Defects, Congenital               \n",
       "5         Cardiovascular Diseases  Prevention   \n",
       "6                         Obesity   Treatment   \n",
       "7                  Mouth Diseases   Treatment   \n",
       "8                         Obesity               \n",
       "9                          Myopia   Treatment   \n",
       "\n",
       "                           Intervention  \n",
       "0                            Nifedipine  \n",
       "1  ERP measures of attention and memory  \n",
       "2                  Intracel KLH Vaccine  \n",
       "3          Whole body hyperthermia unit  \n",
       "4                                        \n",
       "5                              Exercise  \n",
       "6       magnetic resonance spectroscopy  \n",
       "7                    Oral mucosal graft  \n",
       "8                                        \n",
       "9           Progressive Addition Lenses  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trials = pd.read_csv(os.path.join(path_to_data, '{}.tsv'.format(dataset_name)), sep = \"\\t\")\n",
    "df_trials = df_trials.fillna('')\n",
    "\n",
    "\n",
    "df_trials.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(430108, 7)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trials.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "325793"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# texts = df_trials[['Summary', 'Description', 'IE_criteria']].values.tolist()\n",
    "# texts = [t for ts in texts for t in ts if len(t.strip())>=50]\n",
    "texts = df_trials.IE_criteria.tolist()\n",
    "texts = [t for t in texts if len(t.strip())>=50]\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_dict(\n",
    "    {'text': texts}, \n",
    "    features = Features({'text': Value(dtype = 'string')}),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['diagnosed with Congenital Adrenal Hyperplasia (CAH)\\nnormal ECG during baseline evaluation\\nhistory of liver disease, or elevated liver function tests\\nhistory of cardiovascular disease',\n",
       "  'Patients must have a diagnosis of cancer of any histologic type.\\nPatients must have a Karnofsky performance status great or equal to 70%.\\nPatients must have an expected survival for at least four months.\\nNormal healthy volunteers to serve as control for this study.\\nThe occurrence of any type of neurologic symptoms to tetanus vaccine in th past.\\nPatients with a history of seafood allergy are excluded from receiving KLH.',\n",
       "  'Postmenopausal and preferably on hormone replacement therapy\\nIn good general health\\nHave a body mass index (BMI, weight in kg/height in m2) of between 25 and 40\\nExercise less than 20 min/day two days a week']}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVx71GdAIrJH"
   },
   "source": [
    "## 1.2 Build Albert-base tokenizer\n",
    "\n",
    "[Table of content](#TOC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(os.path.join(path_to_save, base_model_name, 'tokenizer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('diagnosed with Congenital Adrenal Hyperplasia (CAH)\\nnormal ECG during baseline evaluation\\nhistory of liver disease, or elevated liver function tests\\nhistory of cardiovascular disease',\n",
       " '[CLS] diagnosed with congenital adrenal hyperplasia (cah) normal ecg during baseline evaluation history of liver disease, or elevated liver function tests history of cardiovascular disease[SEP]')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0], tokenizer.decode(tokenizer(dataset[0][\"text\"])['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\jb\\\\Desktop\\\\NLP\\\\Internal - Transformers for NLP\\\\saves\\\\MLM\\\\albert-base-clinical-trials\\\\tokenizer\\\\tokenizer_config.json',\n",
       " 'C:\\\\Users\\\\jb\\\\Desktop\\\\NLP\\\\Internal - Transformers for NLP\\\\saves\\\\MLM\\\\albert-base-clinical-trials\\\\tokenizer\\\\special_tokens_map.json',\n",
       " 'C:\\\\Users\\\\jb\\\\Desktop\\\\NLP\\\\Internal - Transformers for NLP\\\\saves\\\\MLM\\\\albert-base-clinical-trials\\\\tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(os.path.join(path_to_save, final_model_name, 'tokenizer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVx71GdAIrJH"
   },
   "source": [
    "## 1.3 Tokenize corpus\n",
    "\n",
    "[Table of content](#TOC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use this option because DataCollatorForLanguageModeling (see below) is more efficient when it receives the `special_tokens_mask`.\n",
    "\n",
    "def tokenize_text(examples, tokenizer):\n",
    "    # Remove empty lines\n",
    "    examples['text'] = [\n",
    "        t for t in examples['text'] if len(t) > 0 and not t.isspace()\n",
    "    ]\n",
    "    return tokenizer(examples[\"text\"], return_special_tokens_mask = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDtsaJeVIrJT",
    "outputId": "aa4734bf-4ef5-4437-9948-2c16363da719"
   },
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(lambda examples: tokenize_text(examples, tokenizer), batched = True, remove_columns = [\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By contrast to the generic BIO annotated data, this new data depends on the tokenizer, and is therefore _model-specific_.\n",
    "\n",
    "_Note_: the argument `remove_columns = [\"text\"]` is mandatory, in order to have each item of the dataset have same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(tokenized_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVx71GdAIrJH"
   },
   "source": [
    "## 1.4 Form blocks of constant length\n",
    "\n",
    "[Table of content](#TOC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block_size = tokenizer.model_max_length\n",
    "block_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples, block_size):\n",
    "    # Concatenate all texts.\n",
    "    keys = [k for k in examples.keys() if k != 'text']\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[keys[0]])\n",
    "    \n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    \n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i+block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_dataset = tokenized_dataset.map(\n",
    "    lambda examples: group_texts(examples, block_size),\n",
    "    batched = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "texts[0], texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(mlm_dataset[0][\"input_ids\"]), tokenizer.decode(mlm_dataset[0][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mlm_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mlm_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"albert\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9qywopnIrJH"
   },
   "source": [
    "# 2. ALBERT-base finetuning\n",
    "\n",
    "[Table of content](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "545PP3o8IrJV"
   },
   "source": [
    "## 2.1 Build Albert-base model\n",
    "\n",
    "[Table of content](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(os.path.join(path_to_save, final_model_name, 'model')).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Model finetuning\n",
    "\n",
    "[Table of content](#TOC)\n",
    "\n",
    "`Albert-vase-v2` training parameters as provided in https://github.com/google-research/albert/blob/master/run_pretraining.py : \n",
    "- max_predictions_per_seq = `20`\n",
    "- train_batch_size = `4096`\n",
    "- optimizer = `\"lamb\"`\n",
    "- learning_rate = `0.00176`\n",
    "- poly_power = `1.0`\n",
    "- num_train_steps = `125000`\n",
    "- num_warmup_steps = `3125`\n",
    "- start_warmup_step = `0`\n",
    "- iterations_per_loop = `1000`\n",
    "\n",
    "The original optimizer is `lamb`, which was designed for very large batch size, see the [Lamb paper](https://arxiv.org/pdf/1904.00962.pdf), but we use here the default [AdamW](https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#transformers.AdamW) optimizer with [linear learning rate decay](https://huggingface.co/docs/transformers/v4.23.1/en/main_classes/optimizer_schedules#transformers.get_linear_schedule_with_warmup), as specified in the [Trainer class documentation](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer.optimizers). See the [AdamW paper](https://arxiv.org/pdf/1711.05101.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    os.path.join(path_to_save, f\"{dataset_name}-{model_name}-finetuned\"),\n",
    "    evaluation_strategy = \"no\",\n",
    "    learning_rate = 5e-6, # >= 2e-5 makes the model change too fast, in eg 500 steps\n",
    "    # max_steps = 5000,\n",
    "    num_train_epochs = 1,\n",
    "    warmup_steps = 500,\n",
    "    per_device_train_batch_size = batch_size,\n",
    "    per_device_eval_batch_size = batch_size,\n",
    "    save_strategy = 'no',\n",
    "    logging_steps = 100,\n",
    "    weight_decay = 5e-6,\n",
    "    seed = 42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm_probability = 0.15),\n",
    "    train_dataset = mlm_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some remarks:\n",
    "\n",
    "- The `data_collator` is the object used to batch elements of the training & evaluation datasets.\n",
    "- The `tokenizer` is provided in order to automatically pad the inputs to the maximum length when batching inputs, and to have it saved along the model, which makes it easier to rerun an interrupted training or reuse the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `AlbertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `AlbertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\jb\\miniconda3\\envs\\transformers_nlp\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 219732\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6867\n",
      "You're using a AlbertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4005' max='6867' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4005/6867 25:53 < 18:30, 2.58 it/s, Epoch 0.58/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.337300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.818400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.593000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.437300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.363500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.312000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.235100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.227400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.170400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.165500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>3.090400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.016100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>3.044500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>3.019500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.975800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.997200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.940700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.893700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.913700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.851200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.858900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.858700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>2.897700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.840200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.804800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.840200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>2.848400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.796000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>2.788800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.749800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>2.723000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.713100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>2.712100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.694200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.649300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.643800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>2.596100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>2.621500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>2.614200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.606100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\transformers_nlp\\lib\\site-packages\\transformers\\trainer.py:1521\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1518\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1519\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1520\u001b[0m )\n\u001b[1;32m-> 1521\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1526\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\transformers_nlp\\lib\\site-packages\\transformers\\trainer.py:1763\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1761\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1762\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1763\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1766\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1767\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1768\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1769\u001b[0m ):\n\u001b[0;32m   1770\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1771\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\transformers_nlp\\lib\\site-packages\\transformers\\trainer.py:2517\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2515\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[0;32m   2516\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2517\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2519\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\transformers_nlp\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\transformers_nlp\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in C:\\Users\\jb\\Desktop\\NLP\\Internal - Transformers for NLP\\saves\\MLM\\clinical-trials-albert-base\\model\\config.json\n",
      "Model weights saved in C:\\Users\\jb\\Desktop\\NLP\\Internal - Transformers for NLP\\saves\\MLM\\clinical-trials-albert-base\\model\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(os.path.join(path_to_save, f\"{dataset_name}-{model_name}\", 'model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"inference\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Inference\n",
    "\n",
    "[Table of content](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(os.path.join(path_to_save, f\"{dataset_name}-{model_name}\", 'tokenizer'))\n",
    "model = AutoModelForMaskedLM.from_pretrained(os.path.join(path_to_save, f\"{dataset_name}-{model_name}\", 'model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm = pipeline(\n",
    "    task = 'fill-mask', \n",
    "    model = model, \n",
    "    tokenizer = tokenizer,\n",
    "    framework = 'pt',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'score': 0.399431973695755,\n",
       "   'token': 15,\n",
       "   'token_str': ',',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to, demyelinating neuropathies,[MASK] secondary to infection or systemic[MASK], diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor[MASK], monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'},\n",
       "  {'score': 0.030957819893956184,\n",
       "   'token': 45,\n",
       "   'token_str': ':',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to: demyelinating neuropathies,[MASK] secondary to infection or systemic[MASK], diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor[MASK], monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'},\n",
       "  {'score': 0.030487049371004105,\n",
       "   'token': 14,\n",
       "   'token_str': 'the',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to the demyelinating neuropathies,[MASK] secondary to infection or systemic[MASK], diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor[MASK], monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'},\n",
       "  {'score': 0.029875943437218666,\n",
       "   'token': 27919,\n",
       "   'token_str': 'congenital',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to congenital demyelinating neuropathies,[MASK] secondary to infection or systemic[MASK], diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor[MASK], monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'},\n",
       "  {'score': 0.020367074757814407,\n",
       "   'token': 73,\n",
       "   'token_str': ';',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to; demyelinating neuropathies,[MASK] secondary to infection or systemic[MASK], diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor[MASK], monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'}],\n",
       " [{'score': 0.09790362417697906,\n",
       "   'token': 54,\n",
       "   'token_str': 'or',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to[MASK] demyelinating neuropathies, or secondary to infection or systemic[MASK], diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor[MASK], monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'},\n",
       "  {'score': 0.06781256198883057,\n",
       "   'token': 694,\n",
       "   'token_str': 'either',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to[MASK] demyelinating neuropathies, either secondary to infection or systemic[MASK], diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor[MASK], monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'},\n",
       "  {'score': 0.05387023836374283,\n",
       "   'token': 8802,\n",
       "   'token_str': 'infection',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to[MASK] demyelinating neuropathies, infection secondary to infection or systemic[MASK], diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor[MASK], monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'},\n",
       "  {'score': 0.03116438537836075,\n",
       "   'token': 4782,\n",
       "   'token_str': 'drugs',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to[MASK] demyelinating neuropathies, drugs secondary to infection or systemic[MASK], diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor[MASK], monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'},\n",
       "  {'score': 0.024428777396678925,\n",
       "   'token': 2515,\n",
       "   'token_str': 'disease',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to[MASK] demyelinating neuropathies, disease secondary to infection or systemic[MASK], diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor[MASK], monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'}],\n",
       " [{'score': 0.1088028997182846,\n",
       "   'token': 7697,\n",
       "   'token_str': 'diseases',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to[MASK] demyelinating neuropathies,[MASK] secondary to infection or systemic diseases, diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor[MASK], monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'},\n",
       "  {'score': 0.10855177789926529,\n",
       "   'token': 2515,\n",
       "   'token_str': 'disease',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to[MASK] demyelinating neuropathies,[MASK] secondary to infection or systemic disease, diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor[MASK], monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'},\n",
       "  {'score': 0.06919502466917038,\n",
       "   'token': 11856,\n",
       "   'token_str': 'disorders',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to[MASK] demyelinating neuropathies,[MASK] secondary to infection or systemic disorders, diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor[MASK], monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'},\n",
       "  {'score': 0.06261797994375229,\n",
       "   'token': 7942,\n",
       "   'token_str': 'disorder',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to[MASK] demyelinating neuropathies,[MASK] secondary to infection or systemic disorder, diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor[MASK], monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'},\n",
       "  {'score': 0.04431205987930298,\n",
       "   'token': 8802,\n",
       "   'token_str': 'infection',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to[MASK] demyelinating neuropathies,[MASK] secondary to infection or systemic infection, diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor[MASK], monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'}],\n",
       " [{'score': 0.13063549995422363,\n",
       "   'token': 2515,\n",
       "   'token_str': 'disease',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to[MASK] demyelinating neuropathies,[MASK] secondary to infection or systemic[MASK], diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor disease, monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'},\n",
       "  {'score': 0.0787663459777832,\n",
       "   'token': 8404,\n",
       "   'token_str': 'syndrome',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to[MASK] demyelinating neuropathies,[MASK] secondary to infection or systemic[MASK], diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor syndrome, monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'},\n",
       "  {'score': 0.060384996235370636,\n",
       "   'token': 22880,\n",
       "   'token_str': 'dysfunction',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to[MASK] demyelinating neuropathies,[MASK] secondary to infection or systemic[MASK], diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor dysfunction, monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'},\n",
       "  {'score': 0.04752218723297119,\n",
       "   'token': 7942,\n",
       "   'token_str': 'disorder',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to[MASK] demyelinating neuropathies,[MASK] secondary to infection or systemic[MASK], diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor disorder, monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'},\n",
       "  {'score': 0.039236124604940414,\n",
       "   'token': 2990,\n",
       "   'token_str': 'failure',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to[MASK] demyelinating neuropathies,[MASK] secondary to infection or systemic[MASK], diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor failure, monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'}]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = 'Polyneuropathy of other causes, including but not limited to hereditary demyelinating neuropathies, neuropathies secondary to infection or systemic disease, diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor neuropathy, monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory CIDP and acquired demyelinating symmetric (DADS) neuropathy (also known as distal CIDP).'\n",
    "sent = f'Polyneuropathy of other causes, including but not limited to {mlm.tokenizer.mask_token} demyelinating neuropathies,  {mlm.tokenizer.mask_token} secondary to infection or systemic {mlm.tokenizer.mask_token}, diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor {mlm.tokenizer.mask_token}, monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory CIDP and acquired demyelinating symmetric (DADS) neuropathy (also known as distal CIDP).'\n",
    "mlm(sent, top_k = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Table of content](#TOC)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Token Classification",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
