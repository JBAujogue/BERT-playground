{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 35px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "      Token Embedding\n",
    "  </div> \n",
    "  \n",
    "<div style=\"\n",
    "      font-weight: normal; \n",
    "      font-size: 25px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "      Matrix Factorization methods\n",
    "  </div> \n",
    "\n",
    "\n",
    "\n",
    "  <div style=\"\n",
    "      font-size: 15px; \n",
    "      line-height: 12px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Jean-baptiste AUJOGUE\n",
    "  </div> \n",
    "\n",
    "\n",
    "  <div style=\" float:right; \n",
    "      font-size: 12px; \n",
    "      line-height: 12px; \n",
    "  padding: 10px 15px 8px;\">\n",
    "  December 2022\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"TOC\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table Of Content\n",
    "\n",
    "1. [Corpus](#data) <br>\n",
    "2. [Token Embedding using SGD](#sgd) <br>\n",
    "3. [Token Embedding using Matrix Factorization](#matrix) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "We expose here pretraining methods for the _Token Embedding_ layer of a NLP model. The `transformers` library does not carry components for such pretraining, but it is still a valuable topic and was the center of many papers before transformer models and their contextual embeddings took the advantage.\n",
    "\n",
    "\n",
    "\n",
    "The global purpose of Word Embedding is to represent a _Token_ , a raw string representing a unit of text, as a low dimensional (dense) vector. The way tokens are defined only depends on the method used to split a text into text units : using blank spaces as separators or using classical NLTK or SpaCy's segmentation models leave _words_ as tokens, but splitting protocols yielding _subword units_ , that are half-way between characters and full words, are also investigated :\n",
    "\n",
    "- [Neural Machine Translation of Rare Words with Subword Units (2015)](https://www.aclweb.org/anthology/P16-1162.pdf)\n",
    "- [Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation (2016)](https://arxiv.org/pdf/1609.08144.pdf)). \n",
    "- [BPEmb: Tokenization-free Pre-trained Subword Embeddings in 275 Languages (2018)](https://www.aclweb.org/anthology/L18-1473.pdf)\n",
    "- [SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (2018)](https://arxiv.org/abs/1808.06226)\n",
    "\n",
    "\n",
    "Here we broadly denote by _word_ any such token. Commonly followed approaches for the embedding of words (aka tokens) decompose into three levels of granularity :\n",
    "\n",
    "| Level |  | |\n",
    "|------|------|------|\n",
    "| **Word** | [I.1 Custom model](#word_level_custom) | [I.2 Gensim Model](#gensim) |\n",
    "| **sub-word unit** | [II.1 FastText model](#fastText) |  |\n",
    "| **Character** |  |  |\n",
    "\n",
    "\n",
    "<br>\n",
    "Visualization with TensorBoard : https://www.tensorflow.org/guide/embedding (TODO)\n",
    "\n",
    "# Training objectives\n",
    "\n",
    "#### CBOW training objective\n",
    "\n",
    "Cette méthode de vectorisation est introduite dans \\cite{mikolov2013distributed, mikolov2013efficient}, et consiste à construire pour un vocabulaire de mots une table de vectorisation $T$ contenant un vecteur par mot. La spécificité de cette méthode est que cette vectorisation est faite de façon à pouvoir prédire chaque mot à partir de son contexte. La construction de cette table $T$ passe par la création d'un réseau de neurones, qui sert de modèle pour l'estimation de la probabilité de prédiction d'un mot $w_t$ d'après son contexte $c = w_{t-N}, \\, ... \\, , w_{t-1}$, $w_{t+1}, \\, ... \\, , w_{t+N}$. La table $T$ intégrée au modèle sera optimisée lorsque ce modèle sera entrainé de façon à ce qu'un mot $w_t$ maximise la vraisemblance de la probabilité $P(. \\, | \\, c)$ fournie par le modèle. \n",
    "\n",
    "Le réseau de neurones de décrit de la façon suivante :\n",
    "\n",
    "![cbow](figs/CBOW.png)\n",
    "\n",
    "Un contexte $c = w_{t-N}, \\, ... \\, , w_{t-1}$, $w_{t+1}, \\, ... \\, , w_{t+N}$ est vectorisé via une table $T$ fournissant un ensemble de vecteurs denses (typiquement de dimension comprise entre 50 et 300) $T(w_{t-N}), \\, ... \\, , T(w_{t-1})$, $T(w_{t+1}), \\, ... \\, , T(w_{t+N})$. Chaque vecteur est ensuite transformé via une transformation affine, dont les vecteurs résultants sont superposés en un unique vecteur\n",
    "\n",
    "\\begin{align*}\n",
    "v_c = \\sum _{i = - N}^N M_i T(w_{t+i}) + b_i\n",
    "\\end{align*}\n",
    "\n",
    "Le vecteur $v_c$ est de dimension typiquement égale à la dimension de la vectorisation de mots. Une autre table $T'$ est utilisée pour une nouvelle vectorisation du vocabulaire, de sorte que le mot $w_{t}$ soit transformé en un vecteur $T'(w_{t})$ par cette table, et soit proposé en position $t$ avec probabilité\n",
    "\n",
    "\\begin{align*}\n",
    "P(w_{t} \\, | \\, c\\,) = \\frac{\\exp\\left( T'(w_{t}) \\cdot v_c \\right) }{\\displaystyle \\sum _{w \\in \\mathcal{V}} \\exp\\left(   T'(w) \\cdot v_c \n",
    "\\right) }\n",
    "\\end{align*}\n",
    "\n",
    "Ici $\\cdot$ désigne le produit scalaire entre vecteurs. L'optimisation de ce modèle permet d'ajuster la table $T$ afin que les vecteurs de mots portent suffisamment d'information pour reformer un mot à partir du contexte.\n",
    "\n",
    "\n",
    "#### Skip-Gram training objective\n",
    "\n",
    "\n",
    "Cette méthode de vectorisation est introduite dans \\cite{mikolov2013distributed, mikolov2013efficient} comme version mirroir au Continuous Bag Of Words, et consiste là encore à construire pour un vocabulaire de mots une table de vectorisation $T$ contenant un vecteur par mot. La spécificité de cette méthode est que cette vectorisation est faite non pas de façon prédire un mot central $w$ à partir d'un contexte $c $ comme pour CBOW, mais plutôt de prédire le contexte $c $ à partir du mot central $w$. La construction de cette table $T$ passe par la création d'un réseau de neurones servant de modèle pour l'estimation de la probabilité de prédiction d'un contexte $c = w_{t-N}, \\, ... \\, , w_{t-1}$, $w_{t+1}, \\, ... \\, , w_{t+N}$ à partir d'un mot central $w_t$. La table $T$ intégrée au modèle sera optimisée lorsque ce modèle sera entrainé de façon à ce que le contexte  $ c $ maximise la vraisemblance de la probabilité $P( . \\, | \\, w_t)$ fournie par le modèle.\n",
    "\n",
    "\n",
    "Une implémentation de ce modèle est la suivante : \n",
    "\n",
    "\n",
    "![skipgram](figs/Skipgram.png)\n",
    "\n",
    "\n",
    "Un mot courant $w_t$ est vectorisé par une table $T$ fournissant un vecteur dense (typiquement de dimension comprise entre 50 et 300) $T(w_t)$. Ce vecteur est alors transformé en un ensemble de $2N$ vecteurs\n",
    "\n",
    "\\begin{align*}\n",
    "\\sigma (M_{i} T(w_t) + b_{i}) \\qquad \\qquad i =-N,\\, ...\\, , -1, 1, \\, ...\\, , N\n",
    "\\end{align*}\n",
    "\n",
    "où $N$ désigne la taille de la fenêtre retenue, d'une dimension typiquement égale à la dimension de la vectorisation de mots, et $\\sigma$ une fonction non linéaire (typiquement la _Rectified Linear Unit_ $\\sigma (x) = max (0, x)$). Une autre table $T'$ est utilisée pour une nouvelle vectorisation du vocabulaire, de sorte que chaque mot $w_{t+i}$, transformé en un vecteur $T'(w_{t+i})$ par cette table, soit proposé en position $t+i$ avec probabilité\n",
    "\n",
    "\\begin{align*}\n",
    "P( w_{t+i} | \\, w_t) = \\frac{\\exp\\left(  T'(w_{t+i}) ^\\perp \\sigma \\left( M_i T(w_t) + b_{i}\\right) \\right) }{\\displaystyle \\sum _{w \\in \\mathcal{V}} \\exp\\left(   T'(w) ^\\perp \\sigma \\left( M_i T(w_t) + b_i\\right) \\right) }\n",
    "\\end{align*}\n",
    "\n",
    "On modélise alors la probabilité qu'un ensemble de mots $c = w_{t-N}, \\, ... \\, , w_{t-1}$, $w_{t+1}, \\, ... \\, , w_{t+N}$ soit le contexte d'un mot $w_t$ par le produit\n",
    "\n",
    "\\begin{align*}\n",
    " P( c\\, | \\, w_t) = \\prod _{i = -N}^N P( w_{t+i}\\, | \\, w_t)\n",
    "\\end{align*}\n",
    "\n",
    "Ce modèle de probabilité du contexte d'un mot est naif au sens où les mots de contextes sont considérés comme indépendants deux à deux dès lors que le mot central est connu. Cette approximation rend cependant le calcul d'optimisation beaucoup plus court.\n",
    "\n",
    "\n",
    "\n",
    "L'optimisation de ce modèle permet d'ajuster la table $T$ afin que les vecteurs de mots portent suffisamment d'information pour reformer l'intégralité du contexte à partir de ce seul mot. La vectorisation Skip-Gram est typiquement plus performante que CBOW, car la table $T$ subit plus de contrainte dans son optimisation, et puisque le vecteur d'un mot est obtenu de façon à pouvoir prédire l'utilisation réelle du mot, ici donnée par son contexte. \n",
    "\n",
    "\n",
    "A complete review of methods for learning Token Embeddings is provided in this [PhD thesis, 2018](https://www.skoltech.ru/app/data/uploads/2018/09/Thesis-Fonarev1.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages\n",
    "\n",
    "[Back to top](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "from unidecode import unidecode\n",
    "import multiprocessing\n",
    "\n",
    "# data \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "# models\n",
    "from transformers import AutoTokenizer\n",
    "from gensim.models import Word2Vec, FastText, Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from mittens import GloVe\n",
    "\n",
    "# viz\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom paths & imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_repo = os.path.dirname(os.getcwd())\n",
    "path_to_data = os.path.join(path_to_repo, 'datasets', 'clinical trials CTTI')\n",
    "path_to_save = os.path.join(path_to_repo, 'saves', 'MLM')\n",
    "path_to_src  = os.path.join(path_to_repo, 'src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'clinical-trials-ctti'\n",
    "base_model_name = os.path.join('albert-small-clinical-trials', 'tokenizer')\n",
    "final_model_name = os.path.join('albert-small-clinical-trials', 'w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Corpus\n",
    "\n",
    "[Table of content](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load Clinical Trials corpus\n",
    "\n",
    "[Table of content](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_to_data, '{}.txt'.format(dataset_name)), 'r', encoding = 'utf-8') as f:\n",
    "    texts = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3464685"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Tokenize corpus\n",
    "\n",
    "[Table of content](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(os.path.join(path_to_save, base_model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create & export tokenized corpus (uncoment and run this only once):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_corpus = [tokenizer.tokenize(t) for t in tqdm(texts)]\n",
    "# dataset = Dataset.from_dict({'text': tokenized_corpus})\n",
    "# dataset.save_to_disk(os.path.join(path_to_data, 'tmp', '{}-w2v'.format(dataset_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import back tokenized corpus :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(os.path.join(path_to_data, 'tmp', '{}-w2v'.format(dataset_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 3464685/3464685 [09:56<00:00, 5810.84it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_corpus = [dataset[i]['text'] for i in tqdm(range(len(dataset)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁this',\n",
       " '▁study',\n",
       " '▁will',\n",
       " '▁test',\n",
       " '▁the',\n",
       " '▁ability',\n",
       " '▁of',\n",
       " '▁extended',\n",
       " '▁release',\n",
       " '▁',\n",
       " 'n',\n",
       " 'i',\n",
       " 'f',\n",
       " 'ed',\n",
       " 'i',\n",
       " 'pine',\n",
       " '▁(p',\n",
       " 'roc',\n",
       " 'ard',\n",
       " 'i',\n",
       " 'a',\n",
       " '▁',\n",
       " 'x',\n",
       " 'l',\n",
       " '),',\n",
       " '▁',\n",
       " 'a',\n",
       " '▁blood',\n",
       " '▁pressure',\n",
       " '▁medication',\n",
       " ',',\n",
       " '▁to',\n",
       " '▁per',\n",
       " 'mit',\n",
       " '▁',\n",
       " 'a',\n",
       " '▁decrease',\n",
       " '▁in',\n",
       " '▁the',\n",
       " '▁dose',\n",
       " '▁of',\n",
       " '▁glucocorticoid',\n",
       " '▁medication',\n",
       " '▁children',\n",
       " '▁take',\n",
       " '▁to',\n",
       " '▁treat',\n",
       " '▁congenital',\n",
       " '▁adrenal',\n",
       " '▁hyper',\n",
       " 'plas',\n",
       " 'i',\n",
       " 'a',\n",
       " '▁(c',\n",
       " 'a',\n",
       " 'h',\n",
       " ').']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"matrix\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Token Embedding using Matrix Factorization\n",
    "\n",
    "[Table of content](#TOC)\n",
    "\n",
    "The comparison between SGD-based learning and matrix factorization approaches for token embeddings is developped in [Neural Word Embedding\n",
    "as Implicit Matrix Factorization (2014)](https://papers.nips.cc/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf).\n",
    "\n",
    "\n",
    "The paper [Glove (2015)](https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "\n",
    "Opensource implementations :\n",
    "- original Stanford's [Glove](https://nlp.stanford.edu/projects/glove/), written in C.\n",
    "- [glove-python](https://github.com/maciejkula/glove-python), which fails to build and is hence discarded.\n",
    "- [mittens](https://github.com/roamanalytics/mittens)\n",
    "- various pure python implementations : [here](https://gist.github.com/emaadmanzoor/1d06e0751a3f7d39bc6814941b37531d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token-token distance matrix:\n",
    "\n",
    "For a token _w_ of the vocabulary, and a tokenized text _t_ of the corpus, denote by t[w] the list of indexes of occurences of w in t.\n",
    "\n",
    "$$ M_{i, j} = \\frac{\\displaystyle \\sum_{t \\in T}\\sum_{m \\in t[w_i]}\\sum_{n \\in t[w_j]} d(m, n)}{\\displaystyle \\left( \\sum_{t \\in T}\\#t[w_i] \\right) \\left( \\sum_{t \\in T}\\# t[w_j] \\right)}$$\n",
    "\n",
    "Where typical choices of function _d_ will be \n",
    "\n",
    "$$ d_r(a, b) = 1 \\text{ if } \\lvert a - b \\rvert \\leqslant r \\text{ else } 0 \\;\\; \\text{ (Hard window)} \\qquad \\qquad  d(a, b) = \\frac{1}{1 + \\lvert a - b \\rvert} \\;\\; \\text{(} L^1 \\text{-decrease)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lebesgue_decrease:\n",
    "    def __init__(self, p = 1):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, m, n):\n",
    "        return 1/(1+abs(m-n))**self.p\n",
    "\n",
    "\n",
    "\n",
    "def get_kernel(size, weight_fct, framework = 'scipy'):\n",
    "    # using list comprehension\n",
    "    k = np.array([[weight_fct(i, j) for j in range(size)] for i in range(size)], dtype = 'float32')\n",
    "    if framework == 'scipy':\n",
    "        k = scipy.sparse.csc_matrix(k)\n",
    "    return k\n",
    "\n",
    "\n",
    "\n",
    "def onehot_unfold(indices, n_tokens, framework = 'scipy'):\n",
    "    N = indices.size\n",
    "    if framework == 'scipy':\n",
    "        data = np.ones(N, dtype = int)\n",
    "        v = scipy.sparse.csc_matrix((data, (np.arange(N), indices.ravel())), shape = (N, n_tokens))\n",
    "    elif framework == 'numpy':\n",
    "        v = np.zeros((N, n_tokens))\n",
    "        v[np.arange(N), indices] = 1\n",
    "    else:\n",
    "        raise NotImplementedError('\"framework\" should be \"scipy\" or \"numpy\"')\n",
    "    return v\n",
    "\n",
    "\n",
    "\n",
    "def build_co_occurrence_matrix_from_tokens(tokenized_corpus, token2id, weight_fct):\n",
    "    sim_matrix = scipy.sparse.identity(len(token2id), format = 'lil')\n",
    "    for t in tqdm(tokenized_corpus):\n",
    "        for m, n in itertools.permutations(range(len(t)), r = 2):\n",
    "            sim_matrix[token2id[t[m]], token2id[t[n]]] += weight_fct(m, n)\n",
    "    return sim_matrix\n",
    "\n",
    "\n",
    "\n",
    "def build_co_occurrence_matrix_from_indices(indexed_corpus, n_tokens, weight_fct):\n",
    "    sim_matrix = scipy.sparse.identity(n_tokens, format = 'lil')\n",
    "    for t in tqdm(indexed_corpus):\n",
    "        for m, n in itertools.permutations(range(len(t)), r = 2):\n",
    "            sim_matrix[t[m], t[n]] += weight_fct(m, n)\n",
    "    return sim_matrix\n",
    "\n",
    "\n",
    "\n",
    "# TODOs:\n",
    "# - process by batch\n",
    "# - move sparse tensor operations on gpu with pytorch ?\n",
    "def build_co_occurrence_matrix_from_np_tensors(np_corpus, max_length, n_tokens, weight_fct, framework = 'scipy'):\n",
    "    # init co-occurence matrix\n",
    "    if framework == 'scipy':\n",
    "        m = scipy.sparse.csc_matrix((n_tokens, n_tokens), dtype = 'float32')\n",
    "    elif framework == 'numpy':\n",
    "        m = np.zeros((n_tokens, n_tokens), dtype = 'float32')\n",
    "    else:\n",
    "        raise NotImplementedError('\"framework\" should be \"scipy\" or \"numpy\"')\n",
    "    \n",
    "    # fill co-occurence matrix\n",
    "    k = get_kernel(max_length, weight_fct, framework) # (N, N)\n",
    "    for t in tqdm(np_corpus):\n",
    "        v = onehot_unfold(t, n_tokens, framework)     # (N, n_tokens)\n",
    "        s = v.T @ k @ v                               # (n_tokens, n_tokens)\n",
    "        m += s                                        # (n_tokens, n_tokens)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array([3, 1])\n",
    "n_tokens = 4\n",
    "s = scipy.sparse.identity(n_tokens, format = 'lil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "lol1 = init_similarity_matrix_sparse(len(t), lebesgue_decrease(0.75)) # (N, N)\n",
    "lol2 = onehot_sparse(t, n_tokens) # (N, n_tokens)\n",
    "\n",
    "\n",
    "s += lol2.T @ lol1 @ lol2\n",
    "\n",
    "\n",
    "lol1.todense(), lol2.todense(), s.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_co_occurrence_matrix_from_tokens(dataset[:100]['text'], token2id = tokenizer.get_vocab(), weight_fct = lebesgue_decrease())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lol = [tokenizer(t, return_token_type_ids = False, return_attention_mask = False)['input_ids'][1:-1] for t in texts[:100]]\n",
    "\n",
    "# build_co_occurrence_matrix_from_indices(lol, n_tokens = len(tokenizer.get_vocab()), weight_fct = lebesgue_decrease())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 322.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[13252.5841674,     0.       ,     0.       , ...,     0.       ,\n",
       "             0.       ,     0.       ],\n",
       "        [    0.       ,     0.       ,     0.       , ...,     0.       ,\n",
       "             0.       ,     0.       ],\n",
       "        [    0.       ,     0.       ,     0.       , ...,     0.       ,\n",
       "             0.       ,     0.       ],\n",
       "        ...,\n",
       "        [    0.       ,     0.       ,     0.       , ...,     0.       ,\n",
       "             0.       ,     0.       ],\n",
       "        [    0.       ,     0.       ,     0.       , ...,     0.       ,\n",
       "             0.       ,     0.       ],\n",
       "        [    0.       ,     0.       ,     0.       , ...,     0.       ,\n",
       "             0.       ,     0.       ]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lol = [\n",
    "    tokenizer(\n",
    "        t, \n",
    "        return_token_type_ids = False, \n",
    "        return_attention_mask = False, \n",
    "        padding = 'max_length',\n",
    "        truncation = True,\n",
    "        max_length = 256+2,\n",
    "        return_tensors = 'np',\n",
    "    )['input_ids'][0][1:-1] \n",
    "    for t in texts[:10]\n",
    "]\n",
    "\n",
    "build_co_occurrence_matrix_from_np_tensors(\n",
    "    lol, \n",
    "    max_length = 256, \n",
    "    n_tokens = len(tokenizer.get_vocab()), \n",
    "    weight_fct = lebesgue_decrease(),\n",
    "    framework = 'scipy',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
