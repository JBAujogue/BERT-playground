{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4cRE8IbIrIV"
   },
   "source": [
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 35px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "      Language Modeling\n",
    "  </div> \n",
    "  \n",
    "<div style=\"\n",
    "      font-weight: normal; \n",
    "      font-size: 25px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "      Encoder pretraining using Masked Language Modeling task\n",
    "  </div> \n",
    "\n",
    "\n",
    "  <div style=\"\n",
    "      font-size: 15px; \n",
    "      line-height: 12px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Jean-baptiste AUJOGUE - Hybrid Intelligence\n",
    "  </div> \n",
    "\n",
    "  \n",
    "  <div style=\" float:right; \n",
    "      font-size: 12px; \n",
    "      line-height: 12px; \n",
    "  padding: 10px 15px 8px;\">\n",
    "  December 2022\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"TOC\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table Of Content\n",
    "\n",
    "1. [Dataset](#data) <br>\n",
    "2. [ALBERT finetuning](#albert) <br>\n",
    "3. [Inference](#inference) <br>\n",
    "\n",
    "\n",
    "\n",
    "#### Reference\n",
    "\n",
    "- Hugginface full list of [tutorial notebooks](https://github.com/huggingface/transformers/tree/main/notebooks) (see also [here](https://huggingface.co/docs/transformers/main/notebooks#pytorch-examples))\n",
    "- Huggingface full list of [training scripts](https://github.com/huggingface/transformers/tree/main/examples/pytorch)\n",
    "- Huggingface [tutorial notebook](https://github.com/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch.ipynb) on language models\n",
    "- Huggingface [course](https://huggingface.co/course/chapter7/3?fw=tf) on language models\n",
    "- Huggingface [training script](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py) on language models\n",
    "- Albert [original training protocol](https://github.com/google-research/albert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jb\\miniconda3\\envs\\transformers_nlp\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "import copy\n",
    "import string\n",
    "from itertools import chain\n",
    "\n",
    "# data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import (\n",
    "    Dataset,  \n",
    "    DatasetDict,\n",
    "    ClassLabel, \n",
    "    Features, \n",
    "    Sequence, \n",
    "    Value,\n",
    "    load_from_disk,\n",
    ")\n",
    "from transformers import AlbertConfig, AutoConfig, DataCollatorForLanguageModeling\n",
    "\n",
    "# DL\n",
    "import torch\n",
    "from gensim.models import Word2Vec\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel,\n",
    "    AutoModelForMaskedLM, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    pipeline,\n",
    "    set_seed,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "# viz\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformers settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.22.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make training deterministic\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom paths & imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_repo = os.path.dirname(os.getcwd())\n",
    "path_to_data = os.path.join(path_to_repo, 'datasets', 'clinical trials ICTRP')\n",
    "path_to_save = os.path.join(path_to_repo, 'saves', 'MLM')\n",
    "path_to_src  = os.path.join(path_to_repo, 'src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, path_to_src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'clinical-trials-ictrp'\n",
    "final_dataset_name = 'clinical-trials-ictrp-tokenized-blocks'\n",
    "base_model_name = \"albert-base-v2\"\n",
    "final_model_name = \"albert-small-ictrp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "# 1. Dataset\n",
    "\n",
    "[Table of content](#TOC)\n",
    "\n",
    "We generate a collection of instances of the `datasets.Dataset` class. \n",
    "\n",
    "Note that these are different from the fairly generic `torch.utils.data.Dataset` class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load Clinical Trials corpus\n",
    "\n",
    "[Table of content](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_to_data, '{}.txt'.format(dataset_name)), 'r', encoding = 'utf-8') as f:\n",
    "    texts = [t.strip() for t in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_dict({'text': texts}, features = Features({'text': Value(dtype = 'string')}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1081670"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': [\"Capable of giving signed informed consent. Has received, been intolerant to, or been ineligible for all treatment options proven, to confer clinical benefit. Measurable disease per Response Evaluation Criteria in Solid Tumors (RECIST) 1.1. Eastern Cooperative Oncology Group (ECOG) Performance status (PS) of 0 or 1. Adequate organ function. Male individuals and female individuals of childbearing potential who engage in, heterosexual intercourse must agree to use methods of contraception. Female participants are eligible if they are not pregnant, not breastfeeding or not a, Woman of childbearing potential (WOCBP). Inclusion criterion for the dose-escalation: Individuals with histologically or, cytologically confirmed, advanced or metastatic solid tumors. Inclusion criterion for disease-specific combination expansion: Individuals with, histologically or cytologically confirmed Triple-negative breast cancer (TNBC), Non-small cell lung cancer (NSCLC), Head and neck squamous cell carcinoma (HNSCC), or, melanoma. Inclusion criterion for the monotherapy-MoA expansion: Individuals with histologically, or cytologically confirmed NSCLC. Willingness and medical feasibility (as per Investigator assessment) to undergo paired, tumor biopsies with a non-significant risk. A known additional malignancy that is progressing or has required active treatment, within the past 3 years. Primary central nervous system malignancy. Major surgery = 28 days before start of study treatment. Any unresolved toxicity of Grade = 2, not otherwise specified in other eligibility, criteria, from previous anticancer treatment, except for alopecia and skin, pigmentation. Uncontrolled intercurrent illness requiring systemic treatment or solid organ, transplant. Known hypersensitivity to study treatment or any drugs similar in structure or class, including severe hypersensitivity (= Grade 3) to pembrolizumab and/or any of its, excipients. Any toxicity (Grade 3 or 4) related to prior immunotherapy leading to prior treatment, discontinuation. History of congestive heart failure New York Heart Association (NYHA) >II. Medical history of (non-infectious) pneumonitis/interstitial lung disease (ILD), drug-induced ILD, radiation pneumonitis which required steroid treatment, or any, evidence of clinically-active pneumonitis/ILD. HIV-infection with a history of Kaposi sarcoma and/or Multicentric Castleman Disease. Known psychiatric or substance abuse disorder that would interfere with the, participant's ability to cooperate with the requirements of the study. History or current evidence of any condition, therapy, laboratory abnormality, or, other circumstance that might confound the results of the study or interfere with the, participant's participation for the full duration of the study, such that it is not in, the best interest of the participant to participate, in the opinion of the treating, Investigator. Any other history, condition, therapy, or uncontrolled intercurrent illness which, could in the opinion of the Investigator affect compliance with study requirements. New brain metastases on screening brain MRI/CT; previously treated brain metastases, that are progressive at screening or leptomeningeal disease. Prior therapy with a C-C motif chemokine receptor 8 (CCR8) depleting antibody. Prior allogeneic tissue/solid organ transplant. Radiation therapy to the lung that is > 30 Gy within 6 months before the start of, study treatment. Diagnosis of immunodeficiency or current chronic systemic steroid therapy (in dosing, exceeding 10 mg daily of prednisone equivalent). Active autoimmune disease that has required systemic treatment in the past 2 years.\",\n",
       "  \"1. Patients with ischemic symptoms and/or evidence of myocardial ischemia, 2. Presence of = 50% restenosis after prior implantation of drug-eluting stents in native, coronary vessels. 3. Availability of an OCT-pullback of the target lesion, 4. Written informed consent by the patient for participation in the study. 5. Age = 18 years, 1. Cardiogenic shock, 2. Acute ST-elevation myocardial infarction within 48 hours from symptom onset. 3. Target lesion located in left main trunk or bypass graft. 4. Previous treatment of the target lesion due to restenosis. 5. Severe renal insufficiency (glomerular filtration rate = 30 ml/min), 6. Contraindications to any components of the investigational devices or dual, antiplatelet therapy, 7. Pregnancy (present, suspected or planned) or positive pregnancy test. 8. Previous enrollment in this trial or participation in any other study at the time of, enrollment. 9. Malignancies or other comorbid conditions with life expectancy less than 12 months or, that may result in protocol non-compliance. 10. Patient's inability to fully comply with the study protocol\",\n",
       "  'Bilateral age-related cataract for which phacoemulsification extraction and posterior, IOL implantation has planned, Age 50-90, Visual potential in both eyes of 20/30 or better as determined by investigators, estimation, Normal findings in medical history and physical examination unless the investigator, considers an abnormality to be clinically irrelevant, Preceding ocular surgery or trauma, Relevant other ophthalmic diseases (such as retinal degenerations, etc.), Uncontrolled systemic or ocular disease']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load Clinical-Albert-small tokenizer\n",
    "\n",
    "[Table of content](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(os.path.join(path_to_save, final_model_name, 'tokenizer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Tokenize corpus\n",
    "\n",
    "[Table of content](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use this option because DataCollatorForLanguageModeling (see below) is more efficient \n",
    "# when it receives the `special_tokens_mask`.\n",
    "def tokenize_text(examples, tokenizer):\n",
    "    # Remove empty lines\n",
    "    examples['text'] = [\n",
    "        t for t in examples['text'] if len(t) > 0 and not t.isspace()\n",
    "    ]\n",
    "    return tokenizer(examples[\"text\"], return_special_tokens_mask = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1082/1082 [04:52<00:00,  3.70ba/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    lambda examples: tokenize_text(examples, tokenizer), \n",
    "    batched = True, \n",
    "    remove_columns = [\"text\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By contrast to the generic BIO annotated data, this new data depends on the tokenizer, and is therefore _model-specific_.\n",
    "\n",
    "_Note_: the argument `remove_columns = [\"text\"]` is mandatory, in order to have each item of the dataset have same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "740"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_dataset[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2,\n",
       "  857,\n",
       "  8,\n",
       "  1377,\n",
       "  343,\n",
       "  90,\n",
       "  78,\n",
       "  7,\n",
       "  72,\n",
       "  166,\n",
       "  6,\n",
       "  172,\n",
       "  2225,\n",
       "  13,\n",
       "  6,\n",
       "  10,\n",
       "  172,\n",
       "  1311,\n",
       "  20,\n",
       "  159,\n",
       "  56,\n",
       "  3610,\n",
       "  1085,\n",
       "  6,\n",
       "  13,\n",
       "  8934,\n",
       "  103,\n",
       "  1432,\n",
       "  7,\n",
       "  483,\n",
       "  42,\n",
       "  196,\n",
       "  515,\n",
       "  473,\n",
       "  96,\n",
       "  21,\n",
       "  881,\n",
       "  586,\n",
       "  5,\n",
       "  12,\n",
       "  2191,\n",
       "  11,\n",
       "  5,\n",
       "  18,\n",
       "  7,\n",
       "  18,\n",
       "  7,\n",
       "  826,\n",
       "  734,\n",
       "  702,\n",
       "  337,\n",
       "  5,\n",
       "  12,\n",
       "  682,\n",
       "  11,\n",
       "  335,\n",
       "  203,\n",
       "  5,\n",
       "  12,\n",
       "  84,\n",
       "  15,\n",
       "  11,\n",
       "  8,\n",
       "  5,\n",
       "  92,\n",
       "  10,\n",
       "  5,\n",
       "  18,\n",
       "  7,\n",
       "  261,\n",
       "  423,\n",
       "  176,\n",
       "  7,\n",
       "  189,\n",
       "  621,\n",
       "  17,\n",
       "  137,\n",
       "  621,\n",
       "  8,\n",
       "  242,\n",
       "  139,\n",
       "  54,\n",
       "  2721,\n",
       "  21,\n",
       "  6,\n",
       "  1784,\n",
       "  1257,\n",
       "  69,\n",
       "  309,\n",
       "  13,\n",
       "  82,\n",
       "  402,\n",
       "  8,\n",
       "  188,\n",
       "  7,\n",
       "  137,\n",
       "  209,\n",
       "  59,\n",
       "  222,\n",
       "  5,\n",
       "  27,\n",
       "  39,\n",
       "  348,\n",
       "  59,\n",
       "  60,\n",
       "  123,\n",
       "  6,\n",
       "  60,\n",
       "  482,\n",
       "  10,\n",
       "  60,\n",
       "  5,\n",
       "  14,\n",
       "  6,\n",
       "  738,\n",
       "  8,\n",
       "  242,\n",
       "  139,\n",
       "  5,\n",
       "  12,\n",
       "  1961,\n",
       "  11,\n",
       "  7,\n",
       "  215,\n",
       "  966,\n",
       "  20,\n",
       "  9,\n",
       "  118,\n",
       "  16,\n",
       "  10937,\n",
       "  38,\n",
       "  621,\n",
       "  19,\n",
       "  531,\n",
       "  10,\n",
       "  6,\n",
       "  1129,\n",
       "  213,\n",
       "  6,\n",
       "  524,\n",
       "  10,\n",
       "  399,\n",
       "  881,\n",
       "  586,\n",
       "  7,\n",
       "  215,\n",
       "  966,\n",
       "  20,\n",
       "  42,\n",
       "  16,\n",
       "  1049,\n",
       "  737,\n",
       "  3269,\n",
       "  38,\n",
       "  621,\n",
       "  19,\n",
       "  6,\n",
       "  531,\n",
       "  10,\n",
       "  1129,\n",
       "  213,\n",
       "  3825,\n",
       "  16,\n",
       "  1761,\n",
       "  249,\n",
       "  114,\n",
       "  5,\n",
       "  12,\n",
       "  11975,\n",
       "  11,\n",
       "  6,\n",
       "  122,\n",
       "  16,\n",
       "  1998,\n",
       "  177,\n",
       "  380,\n",
       "  114,\n",
       "  5,\n",
       "  12,\n",
       "  4487,\n",
       "  11,\n",
       "  6,\n",
       "  764,\n",
       "  17,\n",
       "  900,\n",
       "  601,\n",
       "  177,\n",
       "  224,\n",
       "  5,\n",
       "  12,\n",
       "  12998,\n",
       "  11,\n",
       "  6,\n",
       "  10,\n",
       "  6,\n",
       "  1335,\n",
       "  7,\n",
       "  215,\n",
       "  966,\n",
       "  20,\n",
       "  9,\n",
       "  2168,\n",
       "  16,\n",
       "  66,\n",
       "  30,\n",
       "  14,\n",
       "  3269,\n",
       "  38,\n",
       "  621,\n",
       "  19,\n",
       "  531,\n",
       "  6,\n",
       "  10,\n",
       "  1129,\n",
       "  213,\n",
       "  1226,\n",
       "  7,\n",
       "  691,\n",
       "  17,\n",
       "  112,\n",
       "  5,\n",
       "  39,\n",
       "  22,\n",
       "  14,\n",
       "  15,\n",
       "  7207,\n",
       "  5,\n",
       "  12,\n",
       "  14,\n",
       "  15,\n",
       "  196,\n",
       "  106,\n",
       "  451,\n",
       "  11,\n",
       "  13,\n",
       "  588,\n",
       "  5,\n",
       "  10194,\n",
       "  6,\n",
       "  256,\n",
       "  2005,\n",
       "  15,\n",
       "  19,\n",
       "  5,\n",
       "  14,\n",
       "  122,\n",
       "  16,\n",
       "  2456,\n",
       "  182,\n",
       "  7,\n",
       "  5,\n",
       "  14,\n",
       "  99,\n",
       "  678,\n",
       "  271,\n",
       "  67,\n",
       "  53,\n",
       "  2164,\n",
       "  88,\n",
       "  10,\n",
       "  72,\n",
       "  306,\n",
       "  100,\n",
       "  56,\n",
       "  6,\n",
       "  41,\n",
       "  9,\n",
       "  161,\n",
       "  5,\n",
       "  28,\n",
       "  49,\n",
       "  7,\n",
       "  255,\n",
       "  401,\n",
       "  630,\n",
       "  310,\n",
       "  271,\n",
       "  7,\n",
       "  245,\n",
       "  108,\n",
       "  5,\n",
       "  40,\n",
       "  5,\n",
       "  376,\n",
       "  81,\n",
       "  117,\n",
       "  417,\n",
       "  8,\n",
       "  26,\n",
       "  56,\n",
       "  7,\n",
       "  44,\n",
       "  495,\n",
       "  2272,\n",
       "  5,\n",
       "  940,\n",
       "  8,\n",
       "  272,\n",
       "  5,\n",
       "  40,\n",
       "  5,\n",
       "  24,\n",
       "  6,\n",
       "  60,\n",
       "  1207,\n",
       "  701,\n",
       "  21,\n",
       "  63,\n",
       "  934,\n",
       "  6,\n",
       "  96,\n",
       "  6,\n",
       "  93,\n",
       "  130,\n",
       "  1331,\n",
       "  56,\n",
       "  6,\n",
       "  430,\n",
       "  20,\n",
       "  1609,\n",
       "  17,\n",
       "  237,\n",
       "  6,\n",
       "  5,\n",
       "  5484,\n",
       "  7,\n",
       "  198,\n",
       "  1702,\n",
       "  243,\n",
       "  231,\n",
       "  153,\n",
       "  56,\n",
       "  10,\n",
       "  881,\n",
       "  423,\n",
       "  6,\n",
       "  556,\n",
       "  7,\n",
       "  99,\n",
       "  5,\n",
       "  264,\n",
       "  13,\n",
       "  26,\n",
       "  56,\n",
       "  10,\n",
       "  44,\n",
       "  158,\n",
       "  1132,\n",
       "  21,\n",
       "  1800,\n",
       "  10,\n",
       "  447,\n",
       "  6,\n",
       "  141,\n",
       "  105,\n",
       "  5,\n",
       "  264,\n",
       "  5,\n",
       "  12,\n",
       "  40,\n",
       "  272,\n",
       "  5,\n",
       "  28,\n",
       "  11,\n",
       "  13,\n",
       "  3638,\n",
       "  17,\n",
       "  25,\n",
       "  87,\n",
       "  44,\n",
       "  8,\n",
       "  896,\n",
       "  6,\n",
       "  852,\n",
       "  7,\n",
       "  44,\n",
       "  5,\n",
       "  940,\n",
       "  5,\n",
       "  12,\n",
       "  1143,\n",
       "  5,\n",
       "  28,\n",
       "  10,\n",
       "  5,\n",
       "  46,\n",
       "  11,\n",
       "  445,\n",
       "  13,\n",
       "  34,\n",
       "  928,\n",
       "  1315,\n",
       "  88,\n",
       "  13,\n",
       "  34,\n",
       "  56,\n",
       "  6,\n",
       "  1280,\n",
       "  7,\n",
       "  50,\n",
       "  8,\n",
       "  516,\n",
       "  143,\n",
       "  171,\n",
       "  476,\n",
       "  801,\n",
       "  143,\n",
       "  660,\n",
       "  5,\n",
       "  12,\n",
       "  978,\n",
       "  11,\n",
       "  5,\n",
       "  55,\n",
       "  27,\n",
       "  27,\n",
       "  7,\n",
       "  112,\n",
       "  50,\n",
       "  8,\n",
       "  5,\n",
       "  12,\n",
       "  1222,\n",
       "  16,\n",
       "  2781,\n",
       "  11,\n",
       "  1590,\n",
       "  15,\n",
       "  25,\n",
       "  6013,\n",
       "  380,\n",
       "  42,\n",
       "  5,\n",
       "  12,\n",
       "  3681,\n",
       "  11,\n",
       "  6,\n",
       "  94,\n",
       "  16,\n",
       "  986,\n",
       "  5,\n",
       "  3681,\n",
       "  6,\n",
       "  393,\n",
       "  1590,\n",
       "  15,\n",
       "  157,\n",
       "  306,\n",
       "  847,\n",
       "  56,\n",
       "  6,\n",
       "  10,\n",
       "  44,\n",
       "  6,\n",
       "  170,\n",
       "  8,\n",
       "  149,\n",
       "  16,\n",
       "  1329,\n",
       "  1590,\n",
       "  15,\n",
       "  25,\n",
       "  3681,\n",
       "  7,\n",
       "  278,\n",
       "  16,\n",
       "  2111,\n",
       "  19,\n",
       "  5,\n",
       "  14,\n",
       "  50,\n",
       "  8,\n",
       "  5205,\n",
       "  2174,\n",
       "  17,\n",
       "  25,\n",
       "  87,\n",
       "  9239,\n",
       "  6099,\n",
       "  43,\n",
       "  22,\n",
       "  2194,\n",
       "  42,\n",
       "  7,\n",
       "  99,\n",
       "  257,\n",
       "  10,\n",
       "  540,\n",
       "  266,\n",
       "  147,\n",
       "  67,\n",
       "  210,\n",
       "  285,\n",
       "  19,\n",
       "  9,\n",
       "  6,\n",
       "  286,\n",
       "  140,\n",
       "  15,\n",
       "  304,\n",
       "  13,\n",
       "  1342,\n",
       "  19,\n",
       "  9,\n",
       "  379,\n",
       "  8,\n",
       "  9,\n",
       "  26,\n",
       "  7,\n",
       "  50,\n",
       "  10,\n",
       "  144,\n",
       "  170,\n",
       "  8,\n",
       "  44,\n",
       "  160,\n",
       "  6,\n",
       "  71,\n",
       "  6,\n",
       "  218,\n",
       "  609,\n",
       "  6,\n",
       "  10,\n",
       "  6,\n",
       "  63,\n",
       "  4481,\n",
       "  67,\n",
       "  666,\n",
       "  1022,\n",
       "  9,\n",
       "  372,\n",
       "  8,\n",
       "  9,\n",
       "  26,\n",
       "  10,\n",
       "  285,\n",
       "  19,\n",
       "  9,\n",
       "  6,\n",
       "  286,\n",
       "  140,\n",
       "  15,\n",
       "  151,\n",
       "  20,\n",
       "  9,\n",
       "  810,\n",
       "  338,\n",
       "  8,\n",
       "  9,\n",
       "  26,\n",
       "  6,\n",
       "  165,\n",
       "  67,\n",
       "  5,\n",
       "  27,\n",
       "  35,\n",
       "  53,\n",
       "  60,\n",
       "  21,\n",
       "  6,\n",
       "  9,\n",
       "  1646,\n",
       "  2095,\n",
       "  8,\n",
       "  9,\n",
       "  286,\n",
       "  13,\n",
       "  195,\n",
       "  6,\n",
       "  21,\n",
       "  9,\n",
       "  217,\n",
       "  8,\n",
       "  9,\n",
       "  1038,\n",
       "  6,\n",
       "  106,\n",
       "  7,\n",
       "  44,\n",
       "  63,\n",
       "  50,\n",
       "  6,\n",
       "  160,\n",
       "  6,\n",
       "  71,\n",
       "  6,\n",
       "  10,\n",
       "  198,\n",
       "  1702,\n",
       "  243,\n",
       "  157,\n",
       "  6,\n",
       "  412,\n",
       "  21,\n",
       "  9,\n",
       "  217,\n",
       "  8,\n",
       "  9,\n",
       "  106,\n",
       "  405,\n",
       "  533,\n",
       "  19,\n",
       "  26,\n",
       "  379,\n",
       "  7,\n",
       "  476,\n",
       "  316,\n",
       "  383,\n",
       "  5,\n",
       "  30,\n",
       "  33,\n",
       "  73,\n",
       "  316,\n",
       "  448,\n",
       "  25,\n",
       "  45,\n",
       "  35,\n",
       "  29,\n",
       "  394,\n",
       "  181,\n",
       "  316,\n",
       "  383,\n",
       "  6,\n",
       "  67,\n",
       "  59,\n",
       "  722,\n",
       "  32,\n",
       "  73,\n",
       "  10,\n",
       "  1888,\n",
       "  42,\n",
       "  7,\n",
       "  34,\n",
       "  71,\n",
       "  19,\n",
       "  5,\n",
       "  14,\n",
       "  5,\n",
       "  45,\n",
       "  16,\n",
       "  45,\n",
       "  5,\n",
       "  66,\n",
       "  30,\n",
       "  35,\n",
       "  27,\n",
       "  39,\n",
       "  2641,\n",
       "  4636,\n",
       "  22,\n",
       "  838,\n",
       "  5,\n",
       "  97,\n",
       "  5,\n",
       "  12,\n",
       "  45,\n",
       "  45,\n",
       "  58,\n",
       "  97,\n",
       "  11,\n",
       "  5,\n",
       "  6533,\n",
       "  88,\n",
       "  480,\n",
       "  7,\n",
       "  34,\n",
       "  1373,\n",
       "  528,\n",
       "  25,\n",
       "  6227,\n",
       "  423,\n",
       "  556,\n",
       "  7,\n",
       "  393,\n",
       "  71,\n",
       "  13,\n",
       "  9,\n",
       "  380,\n",
       "  67,\n",
       "  53,\n",
       "  5,\n",
       "  55,\n",
       "  5,\n",
       "  116,\n",
       "  5,\n",
       "  48,\n",
       "  70,\n",
       "  41,\n",
       "  5,\n",
       "  57,\n",
       "  65,\n",
       "  117,\n",
       "  9,\n",
       "  417,\n",
       "  8,\n",
       "  6,\n",
       "  26,\n",
       "  56,\n",
       "  7,\n",
       "  131,\n",
       "  8,\n",
       "  5,\n",
       "  429,\n",
       "  10,\n",
       "  144,\n",
       "  134,\n",
       "  153,\n",
       "  847,\n",
       "  71,\n",
       "  5,\n",
       "  12,\n",
       "  214,\n",
       "  579,\n",
       "  6,\n",
       "  2044,\n",
       "  5,\n",
       "  95,\n",
       "  168,\n",
       "  492,\n",
       "  8,\n",
       "  927,\n",
       "  557,\n",
       "  11,\n",
       "  7,\n",
       "  100,\n",
       "  532,\n",
       "  42,\n",
       "  67,\n",
       "  72,\n",
       "  306,\n",
       "  153,\n",
       "  56,\n",
       "  21,\n",
       "  9,\n",
       "  161,\n",
       "  5,\n",
       "  24,\n",
       "  49,\n",
       "  7,\n",
       "  3],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'special_tokens_mask': [1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVx71GdAIrJH"
   },
   "source": [
    "## 1.4 Form blocks of constant length\n",
    "\n",
    "[Table of content](#TOC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples, block_size):\n",
    "    # Concatenate all texts.\n",
    "    keys = [k for k in examples.keys() if k != 'text']\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[keys[0]])\n",
    "    \n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    \n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i+block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1082/1082 [09:32<00:00,  1.89ba/s]\n"
     ]
    }
   ],
   "source": [
    "# run only once\n",
    "# mlm_dataset = tokenized_dataset.map(lambda examples: group_texts(examples, block_size), batched = True)\n",
    "# mlm_dataset.save_to_disk(os.path.join(path_to_data, final_dataset_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_dataset = load_from_disk(os.path.join(path_to_data, final_dataset_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "595255"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mlm_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 857, 8, 1377, 343, 90, 78, 7, 72, 166, 6, 172, 2225, 13, 6, 10, 172, 1311, 20, 159, 56, 3610, 1085, 6, 13, 8934, 103, 1432, 7, 483, 42, 196, 515, 473, 96, 21, 881, 586, 5, 12, 2191, 11, 5, 18, 7, 18, 7, 826, 734, 702, 337, 5, 12, 682, 11, 335, 203, 5, 12, 84, 15, 11, 8, 5, 92, 10, 5, 18, 7, 261, 423, 176, 7, 189, 621, 17, 137, 621, 8, 242, 139, 54, 2721, 21, 6, 1784, 1257, 69, 309, 13, 82, 402, 8, 188, 7, 137, 209, 59, 222, 5, 27, 39, 348, 59, 60, 123, 6, 60, 482, 10, 60, 5, 14, 6, 738, 8, 242, 139, 5, 12, 1961, 11, 7, 215, 966, 20, 9, 118, 16, 10937, 38, 621, 19, 531, 10, 6, 1129, 213, 6, 524, 10, 399, 881, 586, 7, 215, 966, 20, 42, 16, 1049, 737, 3269, 38, 621, 19, 6, 531, 10, 1129, 213, 3825, 16, 1761, 249, 114, 5, 12, 11975, 11, 6, 122, 16, 1998, 177, 380, 114, 5, 12, 4487, 11, 6, 764, 17, 900, 601, 177, 224, 5, 12, 12998, 11, 6, 10, 6, 1335, 7, 215, 966, 20, 9, 2168, 16, 66, 30, 14, 3269, 38, 621, 19, 531, 6, 10, 1129, 213, 1226, 7, 691, 17, 112, 5, 39, 22, 14, 15, 7207, 5, 12, 14, 15, 196, 106, 451, 11, 13, 588, 5, 10194, 6, 256, 2005, 15, 19, 5, 14, 122, 16, 2456, 182, 7, 5, 14, 99, 678, 271, 67, 53, 2164, 88, 10, 72, 306, 100, 56, 6, 41, 9, 161, 5, 28, 49, 7, 255, 401, 630, 310, 271, 7, 245, 108, 5, 40, 5, 376, 81, 117, 417, 8, 26, 56, 7, 44, 495, 2272, 5, 940, 8, 272, 5, 40, 5, 24, 6, 60, 1207, 701, 21, 63, 934, 6, 96, 6, 93, 130, 1331, 56, 6, 430, 20, 1609, 17, 237, 6, 5, 5484, 7, 198, 1702, 243, 231, 153, 56, 10, 881, 423, 6, 556, 7, 99, 5, 264, 13, 26, 56, 10, 44, 158, 1132, 21, 1800, 10, 447, 6, 141, 105, 5, 264, 5, 12, 40, 272, 5, 28, 11, 13, 3638, 17, 25, 87, 44, 8, 896, 6, 852, 7, 44, 5, 940, 5, 12, 1143, 5, 28, 10, 5, 46, 11, 445, 13, 34, 928, 1315, 88, 13, 34, 56, 6, 1280, 7, 50, 8, 516, 143, 171, 476, 801, 143, 660, 5, 12, 978, 11, 5, 55, 27, 27, 7, 112, 50, 8, 5, 12, 1222, 16, 2781, 11, 1590, 15, 25, 6013, 380, 42, 5, 12, 3681, 11, 6, 94, 16, 986, 5, 3681, 6, 393, 1590, 15, 157, 306, 847, 56, 6, 10, 44, 6, 170, 8, 149, 16, 1329, 1590, 15, 25, 3681, 7, 278, 16, 2111, 19, 5, 14, 50, 8, 5205, 2174, 17, 25, 87, 9239, 6099, 43, 22, 2194, 42, 7, 99, 257, 10, 540, 266, 147, 67, 210, 285, 19, 9, 6, 286, 140, 15, 304, 13, 1342, 19, 9, 379, 8, 9, 26, 7, 50, 10, 144], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'special_tokens_mask': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [2, 857, 8, 1377, 343, 90, 78, 7, 72, 166, 6, 172, 2225, 13, 6, 10, 172, 1311, 20, 159, 56, 3610, 1085, 6, 13, 8934, 103, 1432, 7, 483, 42, 196, 515, 473, 96, 21, 881, 586, 5, 12, 2191, 11, 5, 18, 7, 18, 7, 826, 734, 702, 337, 5, 12, 682, 11, 335, 203, 5, 12, 84, 15, 11, 8, 5, 92, 10, 5, 18, 7, 261, 423, 176, 7, 189, 621, 17, 137, 621, 8, 242, 139, 54, 2721, 21, 6, 1784, 1257, 69, 309, 13, 82, 402, 8, 188, 7, 137, 209, 59, 222, 5, 27, 39, 348, 59, 60, 123, 6, 60, 482, 10, 60, 5, 14, 6, 738, 8, 242, 139, 5, 12, 1961, 11, 7, 215, 966, 20, 9, 118, 16, 10937, 38, 621, 19, 531, 10, 6, 1129, 213, 6, 524, 10, 399, 881, 586, 7, 215, 966, 20, 42, 16, 1049, 737, 3269, 38, 621, 19, 6, 531, 10, 1129, 213, 3825, 16, 1761, 249, 114, 5, 12, 11975, 11, 6, 122, 16, 1998, 177, 380, 114, 5, 12, 4487, 11, 6, 764, 17, 900, 601, 177, 224, 5, 12, 12998, 11, 6, 10, 6, 1335, 7, 215, 966, 20, 9, 2168, 16, 66, 30, 14, 3269, 38, 621, 19, 531, 6, 10, 1129, 213, 1226, 7, 691, 17, 112, 5, 39, 22, 14, 15, 7207, 5, 12, 14, 15, 196, 106, 451, 11, 13, 588, 5, 10194, 6, 256, 2005, 15, 19, 5, 14, 122, 16, 2456, 182, 7, 5, 14, 99, 678, 271, 67, 53, 2164, 88, 10, 72, 306, 100, 56, 6, 41, 9, 161, 5, 28, 49, 7, 255, 401, 630, 310, 271, 7, 245, 108, 5, 40, 5, 376, 81, 117, 417, 8, 26, 56, 7, 44, 495, 2272, 5, 940, 8, 272, 5, 40, 5, 24, 6, 60, 1207, 701, 21, 63, 934, 6, 96, 6, 93, 130, 1331, 56, 6, 430, 20, 1609, 17, 237, 6, 5, 5484, 7, 198, 1702, 243, 231, 153, 56, 10, 881, 423, 6, 556, 7, 99, 5, 264, 13, 26, 56, 10, 44, 158, 1132, 21, 1800, 10, 447, 6, 141, 105, 5, 264, 5, 12, 40, 272, 5, 28, 11, 13, 3638, 17, 25, 87, 44, 8, 896, 6, 852, 7, 44, 5, 940, 5, 12, 1143, 5, 28, 10, 5, 46, 11, 445, 13, 34, 928, 1315, 88, 13, 34, 56, 6, 1280, 7, 50, 8, 516, 143, 171, 476, 801, 143, 660, 5, 12, 978, 11, 5, 55, 27, 27, 7, 112, 50, 8, 5, 12, 1222, 16, 2781, 11, 1590, 15, 25, 6013, 380, 42, 5, 12, 3681, 11, 6, 94, 16, 986, 5, 3681, 6, 393, 1590, 15, 157, 306, 847, 56, 6, 10, 44, 6, 170, 8, 149, 16, 1329, 1590, 15, 25, 3681, 7, 278, 16, 2111, 19, 5, 14, 50, 8, 5205, 2174, 17, 25, 87, 9239, 6099, 43, 22, 2194, 42, 7, 99, 257, 10, 540, 266, 147, 67, 210, 285, 19, 9, 6, 286, 140, 15, 304, 13, 1342, 19, 9, 379, 8, 9, 26, 7, 50, 10, 144]}\n"
     ]
    }
   ],
   "source": [
    "print(mlm_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] capable of giving signed informed consent. has received, been intolerant to, or been ineligible for all treatment options proven, to confer clinical benefit. measurable disease per response evaluation criteria in solid tumors (recist) 1.1. eastern cooperative oncology group (ecog) performance status (ps) of 0 or 1. adequate organ function. male individuals and female individuals of childbearing potential who engage in, heterosexual intercourse must agree to use methods of contraception. female participants are eligible if they are not pregnant, not breastfeeding or not a, woman of childbearing potential (wocbp). inclusion criterion for the dose-escalation: individuals with histologically or, cytologically confirmed, advanced or metastatic solid tumors. inclusion criterion for disease-specific combination expansion: individuals with, histologically or cytologically confirmed triple-negative breast cancer (tnbc), non-small cell lung cancer (nsclc), head and neck squamous cell carcinoma (hnscc), or, melanoma. inclusion criterion for the monotherapy-moa expansion: individuals with histologically, or cytologically confirmed nsclc. willingness and medical feasibility (as per investigator assessment) to undergo paired, tumor biopsies with a non-significant risk. a known additional malignancy that is progressing or has required active treatment, within the past 3 years. primary central nervous system malignancy. major surgery = 28 days before start of study treatment. any unresolved toxicity of grade = 2, not otherwise specified in other eligibility, criteria, from previous anticancer treatment, except for alopecia and skin, pigmentation. uncontrolled intercurrent illness requiring systemic treatment or solid organ, transplant. known hypersensitivity to study treatment or any drugs similar in structure or class, including severe hypersensitivity (= grade 3) to pembrolizumab and/or any of its, excipients. any toxicity (grade 3 or 4) related to prior immunotherapy leading to prior treatment, discontinuation. history of congestive heart failure new york heart association (nyha) >ii. medical history of (non-infectious) pneumonitis/interstitial lung disease (ild), drug-induced ild, radiation pneumonitis which required steroid treatment, or any, evidence of clinically-active pneumonitis/ild. hiv-infection with a history of kaposi sarcoma and/or multicentric castleman disease. known psychiatric or substance abuse disorder that would interfere with the, participant's ability to cooperate with the requirements of the study. history or current [CLS] capable of giving signed informed consent. has received, been intolerant to, or been ineligible for all treatment options proven, to confer clinical benefit. measurable disease per response evaluation criteria in solid tumors (recist) 1.1. eastern cooperative oncology group (ecog) performance status (ps) of 0 or 1. adequate organ function. male individuals and female individuals of childbearing potential who engage in, heterosexual intercourse must agree to use methods of contraception. female participants are eligible if they are not pregnant, not breastfeeding or not a, woman of childbearing potential (wocbp). inclusion criterion for the dose-escalation: individuals with histologically or, cytologically confirmed, advanced or metastatic solid tumors. inclusion criterion for disease-specific combination expansion: individuals with, histologically or cytologically confirmed triple-negative breast cancer (tnbc), non-small cell lung cancer (nsclc), head and neck squamous cell carcinoma (hnscc), or, melanoma. inclusion criterion for the monotherapy-moa expansion: individuals with histologically, or cytologically confirmed nsclc. willingness and medical feasibility (as per investigator assessment) to undergo paired, tumor biopsies with a non-significant risk. a known additional malignancy that is progressing or has required active treatment, within the past 3 years. primary central nervous system malignancy. major surgery = 28 days before start of study treatment. any unresolved toxicity of grade = 2, not otherwise specified in other eligibility, criteria, from previous anticancer treatment, except for alopecia and skin, pigmentation. uncontrolled intercurrent illness requiring systemic treatment or solid organ, transplant. known hypersensitivity to study treatment or any drugs similar in structure or class, including severe hypersensitivity (= grade 3) to pembrolizumab and/or any of its, excipients. any toxicity (grade 3 or 4) related to prior immunotherapy leading to prior treatment, discontinuation. history of congestive heart failure new york heart association (nyha) >ii. medical history of (non-infectious) pneumonitis/interstitial lung disease (ild), drug-induced ild, radiation pneumonitis which required steroid treatment, or any, evidence of clinically-active pneumonitis/ild. hiv-infection with a history of kaposi sarcoma and/or multicentric castleman disease. known psychiatric or substance abuse disorder that would interfere with the, participant's ability to cooperate with the requirements of the study. history or current\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(mlm_dataset[0][\"input_ids\"]), tokenizer.decode(mlm_dataset[0][\"labels\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"albert\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9qywopnIrJH"
   },
   "source": [
    "# 2. ALBERT-small training\n",
    "\n",
    "[Table of content](#TOC)\n",
    "\n",
    "#### Different training strategies\n",
    "\n",
    "1. Fully randomly initiated model\n",
    "2. Model with pre-trained token embedding for both encoder and LM head, with weight tying\n",
    "3. Model with frozen pre-trained token embedding for both encoder and LM head, with weight tying\n",
    "4. Model with frozen pre-trained token embedding for the encoder, and untied fully learnable LM head (recommended lr = 5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "545PP3o8IrJV"
   },
   "source": [
    "## 2.1 Build Clinical-Albert-small model\n",
    "\n",
    "[Table of content](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlbertConfig {\n",
       "  \"_name_or_path\": \"albert-base-v2\",\n",
       "  \"architectures\": [\n",
       "    \"AlbertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0,\n",
       "  \"bos_token_id\": 2,\n",
       "  \"classifier_dropout_prob\": 0.1,\n",
       "  \"down_scale_factor\": 1,\n",
       "  \"embedding_size\": 128,\n",
       "  \"eos_token_id\": 3,\n",
       "  \"gap_size\": 0,\n",
       "  \"hidden_act\": \"gelu_new\",\n",
       "  \"hidden_dropout_prob\": 0,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"inner_group_num\": 1,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"albert\",\n",
       "  \"net_structure_type\": 0,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_groups\": 1,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"num_memory_blocks\": 0,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.22.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 30000\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original Albert config\n",
    "config = AutoConfig.from_pretrained(base_model_name)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # a smaller Albert config\n",
    "config = AlbertConfig(\n",
    "    attention_probs_dropout_prob = 0,\n",
    "    pad_token_id = 0,\n",
    "    bos_token_id = 2,\n",
    "    eos_token_id = 3,\n",
    "    classifier_dropout_prob = 0.1,\n",
    "    down_scale_factor = 1,\n",
    "    embedding_size = 128,\n",
    "    gap_size = 0,\n",
    "    hidden_act = 'gelu_new',\n",
    "    hidden_dropout_prob = 0,\n",
    "    hidden_size = 512, # 768,\n",
    "    initializer_range = 0.02,\n",
    "    inner_group_num = 1,\n",
    "    intermediate_size = 2048, # 3072,\n",
    "    layer_norm_eps = 1e-12,\n",
    "    max_position_embeddings = 512,\n",
    "    model_type = 'albert',\n",
    "    net_structure_type = 0,\n",
    "    num_attention_heads = 8, # 12\n",
    "    num_hidden_groups = 1,\n",
    "    num_hidden_layers = 8, # 12\n",
    "    num_memory_blocks = 0,\n",
    "    position_embedding_type = 'absolute',\n",
    "    transformers_version = '4.22.2',\n",
    "    type_vocab_size = 2,\n",
    "    vocab_size = 15000, # 30000,\n",
    "    tie_word_embeddings = False, # True,\n",
    ")\n",
    "model = AutoModelForMaskedLM.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7205400"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Load pre-trained token embedding matrix\n",
    "\n",
    "[Table of content](#TOC)\n",
    "\n",
    "We initiate word embeddings with a pre-trained table.<br>\n",
    "As a by-product, word embedding weights are no longer shared between encoder and classification head, in line with more recent models such as [T5v1.1](https://huggingface.co/docs/transformers/model_doc/t5v1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0043,  0.0418, -0.0339,  0.0004, -0.0023, -0.0130, -0.0010,  0.0021,\n",
      "        -0.0221, -0.0111], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([-0.0141, -0.0102, -0.0209, -0.0094, -0.0127,  0.0317,  0.0097,  0.0114,\n",
      "         0.0061, -0.0220], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.albert.embeddings.word_embeddings._parameters['weight'][1][:10])\n",
    "print(model.predictions.decoder._parameters['weight'][1][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = Word2Vec.load(os.path.join(path_to_save, final_model_name, 'w2v', 'sgram')).wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reindex rows in embedding table\n",
    "base_id2w = {v: k for k, v in tokenizer.get_vocab().items()}\n",
    "reindexing = [wv.key_to_index[base_id2w[i]] for i in range(len(base_id2w))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14995, 14997, 14998, 14999, 14996]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reindexing[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, reind_i in enumerate(reindexing[:5]):\n",
    "#     print(reind_i)\n",
    "#     print(wv.index_to_key[reind_i])\n",
    "#     print(base_id2w[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 128)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings = wv.vectors[reindexing] # wv.get_normed_vectors()[reindexing]\n",
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.2151859700679779,\n",
       " 0.22248247265815735,\n",
       " 0.2173531949520111,\n",
       " 0.213483527302742,\n",
       " 0.21391838788986206,\n",
       " 0.22306038439273834,\n",
       " 0.22632883489131927,\n",
       " 0.24100326001644135,\n",
       " 0.19945749640464783,\n",
       " 0.21178671717643738,\n",
       " 0.22909106314182281,\n",
       " 0.22435908019542694,\n",
       " 0.22845502197742462,\n",
       " 0.21465665102005005,\n",
       " 0.21138404309749603,\n",
       " 0.2195754200220108,\n",
       " 0.2280760258436203,\n",
       " 0.23290695250034332,\n",
       " 0.22167114913463593,\n",
       " 0.20815923810005188,\n",
       " 0.25412580370903015,\n",
       " 0.20920330286026,\n",
       " 0.20746605098247528,\n",
       " 0.21184080839157104,\n",
       " 0.22953960299491882,\n",
       " 0.24783912301063538,\n",
       " 0.2605670392513275,\n",
       " 0.25160589814186096,\n",
       " 0.24019977450370789,\n",
       " 0.2148076742887497,\n",
       " 0.22413262724876404,\n",
       " 0.2092655599117279,\n",
       " 0.20790372788906097,\n",
       " 0.23224221169948578,\n",
       " 0.20780456066131592,\n",
       " 0.2235240638256073,\n",
       " 0.21616925299167633,\n",
       " 0.242834210395813,\n",
       " 0.23521959781646729,\n",
       " 0.21442723274230957,\n",
       " 0.23593056201934814,\n",
       " 0.2229703962802887,\n",
       " 0.22761991620063782,\n",
       " 0.22058455646038055,\n",
       " 0.2191784828901291,\n",
       " 0.2490020990371704,\n",
       " 0.22951342165470123,\n",
       " 0.22735236585140228,\n",
       " 0.2055913209915161,\n",
       " 0.23076149821281433,\n",
       " 0.21534490585327148,\n",
       " 0.20892350375652313,\n",
       " 0.2226126790046692,\n",
       " 0.22245383262634277,\n",
       " 0.24533431231975555,\n",
       " 0.23856772482395172,\n",
       " 0.2135763317346573,\n",
       " 0.24803601205348969,\n",
       " 0.21380358934402466,\n",
       " 0.22110868990421295,\n",
       " 0.2515327036380768,\n",
       " 0.20348559319972992,\n",
       " 0.23351773619651794,\n",
       " 0.2306825965642929,\n",
       " 0.23700957000255585,\n",
       " 0.24221867322921753,\n",
       " 0.2025860846042633,\n",
       " 0.2582257390022278,\n",
       " 0.22141467034816742,\n",
       " 0.2410629540681839,\n",
       " 0.22911657392978668,\n",
       " 0.2184671312570572,\n",
       " 0.2306516468524933,\n",
       " 0.21918398141860962,\n",
       " 0.22958765923976898,\n",
       " 0.24634738266468048,\n",
       " 0.20349326729774475,\n",
       " 0.2267981618642807,\n",
       " 0.22140105068683624,\n",
       " 0.23076802492141724,\n",
       " 0.23081502318382263,\n",
       " 0.24432699382305145,\n",
       " 0.21681807935237885,\n",
       " 0.2130548059940338,\n",
       " 0.22233669459819794,\n",
       " 0.2577102780342102,\n",
       " 0.20060276985168457,\n",
       " 0.2647543251514435,\n",
       " 0.22114185988903046,\n",
       " 0.20817825198173523,\n",
       " 0.2194453775882721,\n",
       " 0.25053954124450684,\n",
       " 0.22501271963119507,\n",
       " 0.21731407940387726,\n",
       " 0.2476443499326706,\n",
       " 0.22744955122470856,\n",
       " 0.21891430020332336,\n",
       " 0.21585847437381744,\n",
       " 0.21552368998527527,\n",
       " 0.21052348613739014,\n",
       " 0.23917299509048462,\n",
       " 0.19172115623950958,\n",
       " 0.2284780591726303,\n",
       " 0.2411741316318512,\n",
       " 0.22655817866325378,\n",
       " 0.2263883501291275,\n",
       " 0.22743049263954163,\n",
       " 0.24084827303886414,\n",
       " 0.23281346261501312,\n",
       " 0.2572219967842102,\n",
       " 0.2573132812976837,\n",
       " 0.210923433303833,\n",
       " 0.2015799731016159,\n",
       " 0.21651220321655273,\n",
       " 0.2490474432706833,\n",
       " 0.20857374370098114,\n",
       " 0.2172238528728485,\n",
       " 0.20590396225452423,\n",
       " 0.23107865452766418,\n",
       " 0.22939160466194153,\n",
       " 0.21644724905490875,\n",
       " 0.23653128743171692,\n",
       " 0.21532826125621796,\n",
       " 0.23204267024993896,\n",
       " 0.2175026535987854,\n",
       " 0.22815151512622833,\n",
       " 0.22724319994449615,\n",
       " 0.24597854912281036,\n",
       " 0.20698979496955872,\n",
       " 0.22469554841518402,\n",
       " 0.22379769384860992,\n",
       " 0.2048204392194748,\n",
       " 0.2338057905435562,\n",
       " 0.2230023741722107,\n",
       " 0.20337805151939392,\n",
       " 0.21291467547416687,\n",
       " 0.2256823182106018,\n",
       " 0.20818841457366943,\n",
       " 0.23909975588321686,\n",
       " 0.24102093279361725,\n",
       " 0.2402641624212265,\n",
       " 0.21325351297855377,\n",
       " 0.2326643168926239,\n",
       " 0.22968459129333496,\n",
       " 0.24817785620689392,\n",
       " 0.2442881315946579,\n",
       " 0.23542551696300507,\n",
       " 0.21476814150810242,\n",
       " 0.22842632234096527,\n",
       " 0.19396068155765533,\n",
       " 0.215022474527359,\n",
       " 0.1860700398683548,\n",
       " 0.22168956696987152,\n",
       " 0.22636698186397552,\n",
       " 0.22725793719291687,\n",
       " 0.22012880444526672,\n",
       " 0.21747955679893494,\n",
       " 0.2562275528907776,\n",
       " 0.22689451277256012,\n",
       " 0.23656632006168365,\n",
       " 0.22440846264362335,\n",
       " 0.2388148456811905,\n",
       " 0.2122838944196701,\n",
       " 0.2223619520664215,\n",
       " 0.19250746071338654,\n",
       " 0.2024020552635193,\n",
       " 0.22147786617279053,\n",
       " 0.2124641239643097,\n",
       " 0.21170201897621155,\n",
       " 0.22159652411937714,\n",
       " 0.24013203382492065,\n",
       " 0.20041655004024506,\n",
       " 0.2079661786556244,\n",
       " 0.21192573010921478,\n",
       " 0.22163444757461548,\n",
       " 0.21462799608707428,\n",
       " 0.2079252451658249,\n",
       " 0.21109533309936523,\n",
       " 0.2301810383796692,\n",
       " 0.21334978938102722,\n",
       " 0.216155007481575,\n",
       " 0.21581922471523285,\n",
       " 0.21995075047016144,\n",
       " 0.22037138044834137,\n",
       " 0.2562761604785919,\n",
       " 0.2359931766986847,\n",
       " 0.2265973836183548,\n",
       " 0.22503262758255005,\n",
       " 0.22967205941677094,\n",
       " 0.2043282687664032,\n",
       " 0.2231203019618988,\n",
       " 0.2593057155609131,\n",
       " 0.21908511221408844,\n",
       " 0.21877562999725342,\n",
       " 0.22786903381347656,\n",
       " 0.22728726267814636,\n",
       " 0.19479964673519135,\n",
       " 0.23600801825523376,\n",
       " 0.23161672055721283,\n",
       " 0.21654236316680908,\n",
       " 0.2510797083377838,\n",
       " 0.20174312591552734,\n",
       " 0.20562635362148285,\n",
       " 0.23772631585597992,\n",
       " 0.21664521098136902,\n",
       " 0.20387372374534607,\n",
       " 0.21125385165214539,\n",
       " 0.24756774306297302,\n",
       " 0.21203292906284332,\n",
       " 0.22254088521003723,\n",
       " 0.23545612394809723,\n",
       " 0.2240162193775177,\n",
       " 0.2329706996679306,\n",
       " 0.236534982919693,\n",
       " 0.20407621562480927,\n",
       " 0.22733044624328613,\n",
       " 0.21835266053676605,\n",
       " 0.2184167057275772,\n",
       " 0.21887686848640442,\n",
       " 0.2006015032529831,\n",
       " 0.2148260772228241,\n",
       " 0.22921936213970184,\n",
       " 0.22808775305747986,\n",
       " 0.2074413150548935,\n",
       " 0.23631222546100616,\n",
       " 0.23002469539642334,\n",
       " 0.2376554012298584,\n",
       " 0.21065784990787506,\n",
       " 0.22835753858089447,\n",
       " 0.25000566244125366,\n",
       " 0.22727511823177338,\n",
       " 0.2611750364303589,\n",
       " 0.2355557084083557,\n",
       " 0.19861720502376556,\n",
       " 0.22635148465633392,\n",
       " 0.22681689262390137,\n",
       " 0.21464693546295166,\n",
       " 0.235317662358284,\n",
       " 0.20194445550441742,\n",
       " 0.23112687468528748,\n",
       " 0.20565852522850037,\n",
       " 0.24578146636486053,\n",
       " 0.2280297726392746,\n",
       " 0.23772764205932617,\n",
       " 0.23321039974689484,\n",
       " 0.22550153732299805,\n",
       " 0.2302856594324112,\n",
       " 0.22588621079921722,\n",
       " 0.2267511934041977,\n",
       " 0.2275295853614807,\n",
       " 0.22005288302898407,\n",
       " 0.23384222388267517,\n",
       " 0.24905532598495483,\n",
       " 0.21153292059898376,\n",
       " 0.22319921851158142,\n",
       " 0.20039932429790497,\n",
       " 0.22173619270324707,\n",
       " 0.21920642256736755,\n",
       " 0.2255934625864029,\n",
       " 0.19406241178512573,\n",
       " 0.2042856365442276,\n",
       " 0.22520293295383453,\n",
       " 0.20737099647521973,\n",
       " 0.23164013028144836,\n",
       " 0.2267998605966568,\n",
       " 0.2044191211462021,\n",
       " 0.20512115955352783,\n",
       " 0.21613122522830963,\n",
       " 0.237483412027359,\n",
       " 0.2077844738960266,\n",
       " 0.20420503616333008,\n",
       " 0.23136581480503082,\n",
       " 0.20820127427577972,\n",
       " 0.24849897623062134,\n",
       " 0.2311844527721405,\n",
       " 0.22106802463531494,\n",
       " 0.24341902136802673,\n",
       " 0.21147683262825012,\n",
       " 0.21987350285053253,\n",
       " 0.2134910672903061,\n",
       " 0.21924984455108643,\n",
       " 0.22341158986091614,\n",
       " 0.21240288019180298,\n",
       " 0.21962767839431763,\n",
       " 0.22689175605773926,\n",
       " 0.24661868810653687,\n",
       " 0.2206670194864273,\n",
       " 0.20676352083683014,\n",
       " 0.23025469481945038,\n",
       " 0.21862901747226715,\n",
       " 0.23303748667240143,\n",
       " 0.21659232676029205,\n",
       " 0.23270665109157562,\n",
       " 0.23835591971874237,\n",
       " 0.2119237333536148,\n",
       " 0.20900079607963562,\n",
       " 0.233119398355484,\n",
       " 0.2125292867422104,\n",
       " 0.21593481302261353,\n",
       " 0.19361038506031036,\n",
       " 0.22757838666439056,\n",
       " 0.22743602097034454,\n",
       " 0.24003060162067413,\n",
       " 0.24352897703647614,\n",
       " 0.22761404514312744,\n",
       " 0.23698854446411133,\n",
       " 0.24820028245449066,\n",
       " 0.21696160733699799,\n",
       " 0.22299401462078094,\n",
       " 0.21837134659290314,\n",
       " 0.22609031200408936,\n",
       " 0.21489094197750092,\n",
       " 0.21948833763599396,\n",
       " 0.21378369629383087,\n",
       " 0.2395956963300705,\n",
       " 0.23001816868782043,\n",
       " 0.23146872222423553,\n",
       " 0.2299959659576416,\n",
       " 0.20915281772613525,\n",
       " 0.24931252002716064,\n",
       " 0.226993590593338,\n",
       " 0.22626152634620667,\n",
       " 0.19950461387634277,\n",
       " 0.22083286941051483,\n",
       " 0.2227640450000763,\n",
       " 0.23344577848911285,\n",
       " 0.26640841364860535,\n",
       " 0.22412848472595215,\n",
       " 0.22092843055725098,\n",
       " 0.20854069292545319,\n",
       " 0.23520790040493011,\n",
       " 0.2341574877500534,\n",
       " 0.21920153498649597,\n",
       " 0.21487262845039368,\n",
       " 0.23226013779640198,\n",
       " 0.2133740484714508,\n",
       " 0.21396216750144958,\n",
       " 0.22592821717262268,\n",
       " 0.21517039835453033,\n",
       " 0.21857206523418427,\n",
       " 0.2080845981836319,\n",
       " 0.2655041813850403,\n",
       " 0.23535802960395813,\n",
       " 0.21954083442687988,\n",
       " 0.24094825983047485,\n",
       " 0.23302291333675385,\n",
       " 0.22892607748508453,\n",
       " 0.23967503011226654,\n",
       " 0.22812433540821075,\n",
       " 0.22988303005695343,\n",
       " 0.20761096477508545,\n",
       " 0.21849209070205688,\n",
       " 0.21932004392147064,\n",
       " 0.22438852488994598,\n",
       " 0.2354903370141983,\n",
       " 0.20612508058547974,\n",
       " 0.21588535606861115,\n",
       " 0.23396721482276917,\n",
       " 0.22826974093914032,\n",
       " 0.23654045164585114,\n",
       " 0.23621433973312378,\n",
       " 0.253482460975647,\n",
       " 0.2296476662158966,\n",
       " 0.21239829063415527,\n",
       " 0.23784677684307098,\n",
       " 0.21512313187122345,\n",
       " 0.24846121668815613,\n",
       " 0.21980218589305878,\n",
       " 0.22005893290042877,\n",
       " 0.22100646793842316,\n",
       " 0.23581606149673462,\n",
       " 0.20404192805290222,\n",
       " 0.22098812460899353,\n",
       " 0.24026888608932495,\n",
       " 0.2247944176197052,\n",
       " 0.24310627579689026,\n",
       " 0.2657853960990906,\n",
       " 0.2301255315542221,\n",
       " 0.21885013580322266,\n",
       " 0.22735045850276947,\n",
       " 0.23350080847740173,\n",
       " 0.19584575295448303,\n",
       " 0.22564463317394257,\n",
       " 0.23730947077274323,\n",
       " 0.23188552260398865,\n",
       " 0.23594163358211517,\n",
       " 0.23595112562179565,\n",
       " 0.23172403872013092,\n",
       " 0.23152945935726166,\n",
       " 0.2312736064195633,\n",
       " 0.23346967995166779,\n",
       " 0.2405254989862442,\n",
       " 0.227574422955513,\n",
       " 0.22407491505146027,\n",
       " 0.21024276316165924,\n",
       " 0.1986730396747589,\n",
       " 0.200042262673378,\n",
       " 0.2243732511997223,\n",
       " 0.22820162773132324,\n",
       " 0.2290467917919159,\n",
       " 0.24331587553024292,\n",
       " 0.23867212235927582,\n",
       " 0.21755579113960266,\n",
       " 0.21013081073760986,\n",
       " 0.2209862917661667,\n",
       " 0.2264425903558731,\n",
       " 0.19946759939193726,\n",
       " 0.23275445401668549,\n",
       " 0.22836750745773315,\n",
       " 0.2013774812221527,\n",
       " 0.2451719343662262,\n",
       " 0.218720942735672,\n",
       " 0.2281871885061264,\n",
       " 0.22449660301208496,\n",
       " 0.21999453008174896,\n",
       " 0.23376025259494781,\n",
       " 0.22614842653274536,\n",
       " 0.24180328845977783,\n",
       " 0.23165194690227509,\n",
       " 0.20711743831634521,\n",
       " 0.2259518951177597,\n",
       " 0.22615014016628265,\n",
       " 0.2387595772743225,\n",
       " 0.22991175949573517,\n",
       " 0.23261401057243347,\n",
       " 0.21775175631046295,\n",
       " 0.2524675726890564,\n",
       " 0.24147279560565948,\n",
       " 0.20929384231567383,\n",
       " 0.20797480642795563,\n",
       " 0.23153725266456604,\n",
       " 0.24065116047859192,\n",
       " 0.23675093054771423,\n",
       " 0.22017991542816162,\n",
       " 0.21854771673679352,\n",
       " 0.21413768827915192,\n",
       " 0.24413740634918213,\n",
       " 0.21808601915836334,\n",
       " 0.22810322046279907,\n",
       " 0.22014747560024261,\n",
       " 0.25109535455703735,\n",
       " 0.2537468373775482,\n",
       " 0.24269753694534302,\n",
       " 0.21376435458660126,\n",
       " 0.2529924511909485,\n",
       " 0.24754010140895844,\n",
       " 0.22048580646514893,\n",
       " 0.2368381768465042,\n",
       " 0.2574221193790436,\n",
       " 0.22494909167289734,\n",
       " 0.22518804669380188,\n",
       " 0.250761479139328,\n",
       " 0.23163431882858276,\n",
       " 0.24510236084461212,\n",
       " 0.2297460436820984,\n",
       " 0.22570307552814484,\n",
       " 0.26164740324020386,\n",
       " 0.22748523950576782,\n",
       " 0.23877479135990143,\n",
       " 0.23919744789600372,\n",
       " 0.2285565286874771,\n",
       " 0.2511500418186188,\n",
       " 0.2231573462486267,\n",
       " 0.23126493394374847,\n",
       " 0.24237291514873505,\n",
       " 0.23198159039020538,\n",
       " 0.23600274324417114,\n",
       " 0.22951176762580872,\n",
       " 0.20975476503372192,\n",
       " 0.2219950258731842,\n",
       " 0.23640120029449463,\n",
       " 0.24391642212867737,\n",
       " 0.2211761325597763,\n",
       " 0.24684232473373413,\n",
       " 0.20260991156101227,\n",
       " 0.22133588790893555,\n",
       " 0.19998380541801453,\n",
       " 0.20609115064144135,\n",
       " 0.23361989855766296,\n",
       " 0.24572594463825226,\n",
       " 0.23479196429252625,\n",
       " 0.21236437559127808,\n",
       " 0.2117680162191391,\n",
       " 0.22127658128738403,\n",
       " 0.26414957642555237,\n",
       " 0.22378787398338318,\n",
       " 0.24631737172603607,\n",
       " 0.24422405660152435,\n",
       " 0.22283628582954407,\n",
       " 0.21930472552776337,\n",
       " 0.23286394774913788,\n",
       " 0.22033612430095673,\n",
       " 0.2239038497209549,\n",
       " 0.2025865614414215,\n",
       " 0.20736141502857208,\n",
       " 0.23607003688812256,\n",
       " 0.22486039996147156,\n",
       " 0.21252888441085815,\n",
       " 0.22157512605190277,\n",
       " 0.20787423849105835,\n",
       " 0.23817382752895355,\n",
       " 0.25485503673553467,\n",
       " 0.2359311878681183,\n",
       " 0.2229325920343399,\n",
       " 0.22899767756462097,\n",
       " 0.22931697964668274,\n",
       " 0.23036958277225494,\n",
       " 0.21511568129062653,\n",
       " 0.19388072192668915,\n",
       " 0.24294425547122955,\n",
       " 0.22246114909648895,\n",
       " 0.24684515595436096,\n",
       " 0.2232116311788559,\n",
       " 0.2267305850982666,\n",
       " 0.21443358063697815,\n",
       " 0.21698349714279175,\n",
       " 0.2029625028371811,\n",
       " 0.23147253692150116,\n",
       " 0.22408108413219452,\n",
       " 0.23065894842147827,\n",
       " 0.2553519904613495,\n",
       " 0.23198574781417847,\n",
       " 0.21100616455078125,\n",
       " 0.19627416133880615,\n",
       " 0.2263656109571457,\n",
       " 0.21925435960292816,\n",
       " 0.2267608940601349,\n",
       " 0.22733411192893982,\n",
       " 0.20926962792873383,\n",
       " 0.24121791124343872,\n",
       " 0.23217497766017914,\n",
       " 0.2322424054145813,\n",
       " 0.21633701026439667,\n",
       " 0.22038817405700684,\n",
       " 0.20802542567253113,\n",
       " 0.22148282825946808,\n",
       " 0.22115086019039154,\n",
       " 0.2207215577363968,\n",
       " 0.25032034516334534,\n",
       " 0.22081314027309418,\n",
       " 0.22905758023262024,\n",
       " 0.23671281337738037,\n",
       " 0.22054383158683777,\n",
       " 0.2204495370388031,\n",
       " 0.22537925839424133,\n",
       " 0.2367863953113556,\n",
       " 0.2226579189300537,\n",
       " 0.23589710891246796,\n",
       " 0.23215502500534058,\n",
       " 0.22443066537380219,\n",
       " 0.21300944685935974,\n",
       " 0.22205786406993866,\n",
       " 0.24481026828289032,\n",
       " 0.23767010867595673,\n",
       " 0.20444655418395996,\n",
       " 0.2132881134748459,\n",
       " 0.20648086071014404,\n",
       " 0.25255507230758667,\n",
       " 0.22987568378448486,\n",
       " 0.22365708649158478,\n",
       " 0.22791576385498047,\n",
       " 0.21041053533554077,\n",
       " 0.21600307524204254,\n",
       " 0.23810340464115143,\n",
       " 0.2511461079120636,\n",
       " 0.22852379083633423,\n",
       " 0.21547386050224304,\n",
       " 0.22151991724967957,\n",
       " 0.2435879111289978,\n",
       " 0.24392686784267426,\n",
       " 0.2526116669178009,\n",
       " 0.2359631508588791,\n",
       " 0.2256391942501068,\n",
       " 0.2255057394504547,\n",
       " 0.2210191935300827,\n",
       " 0.22117629647254944,\n",
       " 0.23746304214000702,\n",
       " 0.24682678282260895,\n",
       " 0.20786947011947632,\n",
       " 0.23811855912208557,\n",
       " 0.22083713114261627,\n",
       " 0.217362642288208,\n",
       " 0.21999801695346832,\n",
       " 0.218714639544487,\n",
       " 0.20708341896533966,\n",
       " 0.21940401196479797,\n",
       " 0.2507357597351074,\n",
       " 0.21026694774627686,\n",
       " 0.25008252263069153,\n",
       " 0.20825955271720886,\n",
       " 0.25016605854034424,\n",
       " 0.23632557690143585,\n",
       " 0.22115671634674072,\n",
       " 0.21528737246990204,\n",
       " 0.23237934708595276,\n",
       " 0.2268059402704239,\n",
       " 0.20565882325172424,\n",
       " 0.21313878893852234,\n",
       " 0.2079441100358963,\n",
       " 0.20840045809745789,\n",
       " 0.23224137723445892,\n",
       " 0.23875084519386292,\n",
       " 0.23641693592071533,\n",
       " 0.22827520966529846,\n",
       " 0.2310781329870224,\n",
       " 0.24340012669563293,\n",
       " 0.19890467822551727,\n",
       " 0.22873975336551666,\n",
       " 0.2354162186384201,\n",
       " 0.23372089862823486,\n",
       " 0.21291671693325043,\n",
       " 0.19777189195156097,\n",
       " 0.24319495260715485,\n",
       " 0.1968216747045517,\n",
       " 0.2124105989933014,\n",
       " 0.24018096923828125,\n",
       " 0.2262190580368042,\n",
       " 0.21747511625289917,\n",
       " 0.22748680412769318,\n",
       " 0.224797323346138,\n",
       " 0.2230488359928131,\n",
       " 0.22256158292293549,\n",
       " 0.209795743227005,\n",
       " 0.24362605810165405,\n",
       " 0.2249973714351654,\n",
       " 0.2426944375038147,\n",
       " 0.22014932334423065,\n",
       " 0.22104620933532715,\n",
       " 0.22366461157798767,\n",
       " 0.21076366305351257,\n",
       " 0.23920364677906036,\n",
       " 0.21127311885356903,\n",
       " 0.23026753962039948,\n",
       " 0.2250807136297226,\n",
       " 0.2136710286140442,\n",
       " 0.2504541277885437,\n",
       " 0.2511330246925354,\n",
       " 0.2309359312057495,\n",
       " 0.2307586520910263,\n",
       " 0.22583475708961487,\n",
       " 0.23023362457752228,\n",
       " 0.21983923017978668,\n",
       " 0.22425054013729095,\n",
       " 0.21884828805923462,\n",
       " 0.23817887902259827,\n",
       " 0.21278619766235352,\n",
       " 0.21517278254032135,\n",
       " 0.2014443427324295,\n",
       " 0.22750899195671082,\n",
       " 0.21893846988677979,\n",
       " 0.23140862584114075,\n",
       " 0.21996112167835236,\n",
       " 0.2639485001564026,\n",
       " 0.19853660464286804,\n",
       " 0.2399023026227951,\n",
       " 0.23173421621322632,\n",
       " 0.23237532377243042,\n",
       " 0.2281327247619629,\n",
       " 0.22041593492031097,\n",
       " 0.23271125555038452,\n",
       " 0.2436462640762329,\n",
       " 0.2237168550491333,\n",
       " 0.22371771931648254,\n",
       " 0.22668738663196564,\n",
       " 0.23343385756015778,\n",
       " 0.22938914597034454,\n",
       " 0.22611218690872192,\n",
       " 0.21397899091243744,\n",
       " 0.2153676599264145,\n",
       " 0.22495023906230927,\n",
       " 0.22532042860984802,\n",
       " 0.21949319541454315,\n",
       " 0.218505397439003,\n",
       " 0.22612310945987701,\n",
       " 0.23945075273513794,\n",
       " 0.20919306576251984,\n",
       " 0.2530095875263214,\n",
       " 0.23029078543186188,\n",
       " 0.24083895981311798,\n",
       " 0.2101047933101654,\n",
       " 0.2476334422826767,\n",
       " 0.22895994782447815,\n",
       " 0.22708414494991302,\n",
       " 0.23276282846927643,\n",
       " 0.21691903471946716,\n",
       " 0.22040784358978271,\n",
       " 0.21432223916053772,\n",
       " 0.2210736870765686,\n",
       " 0.2331196516752243,\n",
       " 0.20968849956989288,\n",
       " 0.2222762107849121,\n",
       " 0.24695761501789093,\n",
       " 0.22626322507858276,\n",
       " 0.22808566689491272,\n",
       " 0.2070009410381317,\n",
       " 0.20344717800617218,\n",
       " 0.22966954112052917,\n",
       " 0.2373114675283432,\n",
       " 0.22297103703022003,\n",
       " 0.23106563091278076,\n",
       " 0.23948781192302704,\n",
       " 0.19132037460803986,\n",
       " 0.21942055225372314,\n",
       " 0.23120597004890442,\n",
       " 0.20317132771015167,\n",
       " 0.23905900120735168,\n",
       " 0.2189026176929474,\n",
       " 0.2315826565027237,\n",
       " 0.2170935571193695,\n",
       " 0.22049856185913086,\n",
       " 0.225014790892601,\n",
       " 0.22046588361263275,\n",
       " 0.24411173164844513,\n",
       " 0.2246079444885254,\n",
       " 0.23754554986953735,\n",
       " 0.23199127614498138,\n",
       " 0.22125734388828278,\n",
       " 0.2327444702386856,\n",
       " 0.21755358576774597,\n",
       " 0.22298835217952728,\n",
       " 0.22504505515098572,\n",
       " 0.21683067083358765,\n",
       " 0.24076978862285614,\n",
       " 0.23536652326583862,\n",
       " 0.2020762413740158,\n",
       " 0.20466218888759613,\n",
       " 0.22856837511062622,\n",
       " 0.21147799491882324,\n",
       " 0.21067358553409576,\n",
       " 0.23030884563922882,\n",
       " 0.23522880673408508,\n",
       " 0.23542632162570953,\n",
       " 0.21059376001358032,\n",
       " 0.2368832230567932,\n",
       " 0.22040174901485443,\n",
       " 0.22759732604026794,\n",
       " 0.25146907567977905,\n",
       " 0.2294049859046936,\n",
       " 0.22524859011173248,\n",
       " 0.23001235723495483,\n",
       " 0.23133741319179535,\n",
       " 0.21997174620628357,\n",
       " 0.23477260768413544,\n",
       " 0.21721626818180084,\n",
       " 0.21399980783462524,\n",
       " 0.2173713594675064,\n",
       " 0.21175530552864075,\n",
       " 0.2427447885274887,\n",
       " 0.23234440386295319,\n",
       " 0.22105616331100464,\n",
       " 0.2288200408220291,\n",
       " 0.19555459916591644,\n",
       " 0.23654618859291077,\n",
       " 0.2525549829006195,\n",
       " 0.19646111130714417,\n",
       " 0.19776302576065063,\n",
       " 0.23190060257911682,\n",
       " 0.24524392187595367,\n",
       " 0.23449915647506714,\n",
       " 0.24111810326576233,\n",
       " 0.2460017055273056,\n",
       " 0.2565038502216339,\n",
       " 0.20338904857635498,\n",
       " 0.21684157848358154,\n",
       " 0.2108643352985382,\n",
       " 0.21024635434150696,\n",
       " 0.23529432713985443,\n",
       " 0.21420630812644958,\n",
       " 0.20468670129776,\n",
       " 0.24811410903930664,\n",
       " 0.23055750131607056,\n",
       " 0.20670272409915924,\n",
       " 0.2389398068189621,\n",
       " 0.24000777304172516,\n",
       " 0.227608323097229,\n",
       " 0.21162614226341248,\n",
       " 0.2389487773180008,\n",
       " 0.2266434133052826,\n",
       " 0.2575764060020447,\n",
       " 0.20623467862606049,\n",
       " 0.21942965686321259,\n",
       " 0.22130028903484344,\n",
       " 0.21491743624210358,\n",
       " 0.21740902960300446,\n",
       " 0.20133228600025177,\n",
       " 0.2493501603603363,\n",
       " 0.2385552078485489,\n",
       " 0.24938149750232697,\n",
       " 0.2201651930809021,\n",
       " 0.2333301305770874,\n",
       " 0.20984333753585815,\n",
       " 0.2338302582502365,\n",
       " 0.21960897743701935,\n",
       " 0.22394369542598724,\n",
       " 0.22680748999118805,\n",
       " 0.21575628221035004,\n",
       " 0.22809410095214844,\n",
       " 0.2305939644575119,\n",
       " 0.19528008997440338,\n",
       " 0.21840889751911163,\n",
       " 0.24263863265514374,\n",
       " 0.2120116800069809,\n",
       " 0.20649529993534088,\n",
       " 0.22008992731571198,\n",
       " 0.22199822962284088,\n",
       " 0.22813618183135986,\n",
       " 0.19998608529567719,\n",
       " 0.27515724301338196,\n",
       " 0.22277894616127014,\n",
       " 0.21811875700950623,\n",
       " 0.22800441086292267,\n",
       " 0.23988968133926392,\n",
       " 0.23554261028766632,\n",
       " 0.1961219161748886,\n",
       " 0.22218696773052216,\n",
       " 0.24122384190559387,\n",
       " 0.22324764728546143,\n",
       " 0.23850740492343903,\n",
       " 0.2634213864803314,\n",
       " 0.20609013736248016,\n",
       " 0.21479453146457672,\n",
       " 0.24649789929389954,\n",
       " 0.275409460067749,\n",
       " 0.20691272616386414,\n",
       " 0.23283106088638306,\n",
       " 0.22983373701572418,\n",
       " 0.1933174431324005,\n",
       " 0.23402529954910278,\n",
       " 0.24168135225772858,\n",
       " 0.22393840551376343,\n",
       " 0.2166014164686203,\n",
       " 0.2304520159959793,\n",
       " 0.21610715985298157,\n",
       " 0.22183476388454437,\n",
       " 0.21827815473079681,\n",
       " 0.21768613159656525,\n",
       " 0.20182177424430847,\n",
       " 0.22949516773223877,\n",
       " 0.22301001846790314,\n",
       " 0.20794153213500977,\n",
       " 0.20242591202259064,\n",
       " 0.22989442944526672,\n",
       " 0.20969034731388092,\n",
       " 0.2516777813434601,\n",
       " 0.23385711014270782,\n",
       " 0.24769824743270874,\n",
       " 0.22009998559951782,\n",
       " 0.2331341952085495,\n",
       " 0.2568815052509308,\n",
       " 0.20403268933296204,\n",
       " 0.24078990519046783,\n",
       " 0.22590339183807373,\n",
       " 0.2502899765968323,\n",
       " 0.20977994799613953,\n",
       " 0.2354739010334015,\n",
       " 0.2033657729625702,\n",
       " 0.21998126804828644,\n",
       " 0.22414599359035492,\n",
       " 0.24105685949325562,\n",
       " 0.24252764880657196,\n",
       " 0.20562849938869476,\n",
       " 0.21781064569950104,\n",
       " 0.21254397928714752,\n",
       " 0.2183549702167511,\n",
       " 0.2212759405374527,\n",
       " 0.22803954780101776,\n",
       " 0.19896207749843597,\n",
       " 0.22627504169940948,\n",
       " 0.231027752161026,\n",
       " 0.22421571612358093,\n",
       " 0.21788249909877777,\n",
       " 0.22848562896251678,\n",
       " 0.23553654551506042,\n",
       " 0.2250872254371643,\n",
       " 0.2177855372428894,\n",
       " 0.2280479520559311,\n",
       " 0.20292985439300537,\n",
       " 0.24915286898612976,\n",
       " 0.21352118253707886,\n",
       " 0.2078026533126831,\n",
       " 0.20632041990756989,\n",
       " 0.22181707620620728,\n",
       " 0.2147190421819687,\n",
       " 0.23225714266300201,\n",
       " 0.24383904039859772,\n",
       " 0.2358860820531845,\n",
       " 0.2045721411705017,\n",
       " 0.21460247039794922,\n",
       " 0.18376052379608154,\n",
       " 0.22106485068798065,\n",
       " 0.2454606294631958,\n",
       " 0.20754937827587128,\n",
       " 0.2278570532798767,\n",
       " 0.23084473609924316,\n",
       " 0.23468279838562012,\n",
       " 0.23584307730197906,\n",
       " 0.21461142599582672,\n",
       " 0.2202359139919281,\n",
       " 0.23649823665618896,\n",
       " 0.23431965708732605,\n",
       " 0.20240485668182373,\n",
       " 0.2437608689069748,\n",
       " 0.22249989211559296,\n",
       " 0.241465225815773,\n",
       " 0.2078370302915573,\n",
       " 0.21440130472183228,\n",
       " 0.2209801971912384,\n",
       " 0.20412199199199677,\n",
       " 0.2252272367477417,\n",
       " 0.21022680401802063,\n",
       " 0.23049794137477875,\n",
       " 0.22087685763835907,\n",
       " 0.2388318032026291,\n",
       " 0.22794382274150848,\n",
       " 0.2229042798280716,\n",
       " 0.22267252206802368,\n",
       " 0.22205482423305511,\n",
       " 0.267670601606369,\n",
       " 0.2369103878736496,\n",
       " 0.2193409502506256,\n",
       " 0.2280000001192093,\n",
       " 0.23939017951488495,\n",
       " 0.23395957052707672,\n",
       " 0.23429560661315918,\n",
       " 0.22261011600494385,\n",
       " 0.22525820136070251,\n",
       " 0.21097829937934875,\n",
       " 0.2065979391336441,\n",
       " 0.21824407577514648,\n",
       " 0.2240530252456665,\n",
       " 0.21140436828136444,\n",
       " 0.2215060442686081,\n",
       " 0.2000940442085266,\n",
       " 0.2123384028673172,\n",
       " 0.22501008212566376,\n",
       " 0.19802768528461456,\n",
       " 0.21774038672447205,\n",
       " 0.2517838478088379,\n",
       " 0.2234807163476944,\n",
       " 0.22947503626346588,\n",
       " 0.20892617106437683,\n",
       " 0.2196258306503296,\n",
       " 0.23404645919799805,\n",
       " 0.21952387690544128,\n",
       " 0.2290262132883072,\n",
       " 0.22351603209972382,\n",
       " 0.22896862030029297,\n",
       " 0.22379635274410248,\n",
       " 0.21056653559207916,\n",
       " 0.22492659091949463,\n",
       " 0.22184565663337708,\n",
       " 0.2338510900735855,\n",
       " 0.21032416820526123,\n",
       " 0.2199302464723587,\n",
       " 0.21237336099147797,\n",
       " 0.20559392869472504,\n",
       " 0.24555926024913788,\n",
       " 0.25032973289489746,\n",
       " 0.23769304156303406,\n",
       " 0.23503902554512024,\n",
       " 0.23143857717514038,\n",
       " 0.20903445780277252,\n",
       " 0.19012735784053802,\n",
       " 0.2282424420118332,\n",
       " 0.2063664048910141,\n",
       " 0.22316253185272217,\n",
       " 0.21363672614097595,\n",
       " 0.21756009757518768,\n",
       " 0.2091357558965683,\n",
       " 0.21629205346107483,\n",
       " 0.20239931344985962,\n",
       " 0.21709664165973663,\n",
       " 0.2134818583726883,\n",
       " 0.21580512821674347,\n",
       " 0.2170925885438919,\n",
       " 0.22416266798973083,\n",
       " 0.2155873030424118,\n",
       " 0.2454712837934494,\n",
       " 0.22169700264930725,\n",
       " 0.22176305949687958,\n",
       " 0.20711149275302887,\n",
       " 0.2218272089958191,\n",
       " 0.19906914234161377,\n",
       " 0.21986836194992065,\n",
       " 0.22499988973140717,\n",
       " 0.20120787620544434,\n",
       " 0.22302313148975372,\n",
       " 0.23222219944000244,\n",
       " 0.2385740727186203,\n",
       " 0.22766485810279846,\n",
       " 0.20925676822662354,\n",
       " 0.22130297124385834,\n",
       " 0.23875851929187775,\n",
       " 0.21284443140029907,\n",
       " 0.2287505865097046,\n",
       " 0.21172145009040833,\n",
       " 0.23184578120708466,\n",
       " 0.20565694570541382,\n",
       " 0.21096478402614594,\n",
       " ...]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.albert.embeddings.word_embeddings._parameters['weight'].norm(dim = -1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.05029847100377083,\n",
       " 0.04985176399350166,\n",
       " 0.04812698811292648,\n",
       " 0.05405382812023163,\n",
       " 0.05249904468655586,\n",
       " 1.7227306365966797,\n",
       " 1.8393648862838745,\n",
       " 1.9135942459106445,\n",
       " 2.096674919128418,\n",
       " 1.9929710626602173,\n",
       " 1.8200165033340454,\n",
       " 1.8425917625427246,\n",
       " 2.1484005451202393,\n",
       " 2.278388261795044,\n",
       " 1.9871857166290283,\n",
       " 2.1129376888275146,\n",
       " 2.4063568115234375,\n",
       " 1.9516831636428833,\n",
       " 2.530545473098755,\n",
       " 2.0679008960723877,\n",
       " 2.5778214931488037,\n",
       " 2.2181642055511475,\n",
       " 2.1268270015716553,\n",
       " 2.284655809402466,\n",
       " 2.182380199432373,\n",
       " 2.623713254928589,\n",
       " 2.6132137775421143,\n",
       " 2.0911061763763428,\n",
       " 2.2124714851379395,\n",
       " 1.9270639419555664,\n",
       " 2.1363606452941895,\n",
       " 2.244401216506958,\n",
       " 2.4619646072387695,\n",
       " 2.2495038509368896,\n",
       " 2.8109073638916016,\n",
       " 2.169473886489868,\n",
       " 2.06711483001709,\n",
       " 2.4892654418945312,\n",
       " 2.5276334285736084,\n",
       " 3.243286609649658,\n",
       " 3.1745095252990723,\n",
       " 2.8597164154052734,\n",
       " 2.6179184913635254,\n",
       " 2.607361078262329,\n",
       " 2.1849875450134277,\n",
       " 2.2061665058135986,\n",
       " 1.8428137302398682,\n",
       " 2.382831573486328,\n",
       " 2.3931705951690674,\n",
       " 3.8257932662963867,\n",
       " 2.5819013118743896,\n",
       " 2.481210231781006,\n",
       " 3.314995765686035,\n",
       " 2.43825364112854,\n",
       " 2.6429271697998047,\n",
       " 3.2433857917785645,\n",
       " 2.240114688873291,\n",
       " 1.998594880104065,\n",
       " 2.2221524715423584,\n",
       " 2.749586582183838,\n",
       " 2.4157965183258057,\n",
       " 4.05672550201416,\n",
       " 2.6516194343566895,\n",
       " 2.365020275115967,\n",
       " 2.9914095401763916,\n",
       " 3.0224215984344482,\n",
       " 2.376946449279785,\n",
       " 2.5638504028320312,\n",
       " 3.7187955379486084,\n",
       " 3.1468610763549805,\n",
       " 2.4506943225860596,\n",
       " 2.581793785095215,\n",
       " 2.7387425899505615,\n",
       " 2.602600574493408,\n",
       " 3.22462797164917,\n",
       " 2.2174642086029053,\n",
       " 4.101142883300781,\n",
       " 2.4439263343811035,\n",
       " 3.3495969772338867,\n",
       " 2.9875435829162598,\n",
       " 3.364081859588623,\n",
       " 3.2313003540039062,\n",
       " 2.9812161922454834,\n",
       " 2.7614760398864746,\n",
       " 2.2899930477142334,\n",
       " 3.2222023010253906,\n",
       " 2.7740423679351807,\n",
       " 2.4287960529327393,\n",
       " 2.451256275177002,\n",
       " 1.8632500171661377,\n",
       " 3.5800936222076416,\n",
       " 2.5021092891693115,\n",
       " 3.1164026260375977,\n",
       " 2.694136619567871,\n",
       " 2.8418259620666504,\n",
       " 2.165971517562866,\n",
       " 3.246857166290283,\n",
       " 1.9127788543701172,\n",
       " 2.793134927749634,\n",
       " 2.5762574672698975,\n",
       " 2.908902406692505,\n",
       " 3.3543145656585693,\n",
       " 6.08054256439209,\n",
       " 2.7333953380584717,\n",
       " 2.2157514095306396,\n",
       " 2.674950122833252,\n",
       " 3.2096102237701416,\n",
       " 2.6239655017852783,\n",
       " 2.853532552719116,\n",
       " 2.105985164642334,\n",
       " 3.0961508750915527,\n",
       " 2.4360239505767822,\n",
       " 2.7745227813720703,\n",
       " 2.6870365142822266,\n",
       " 3.1638708114624023,\n",
       " 3.430668592453003,\n",
       " 3.2261085510253906,\n",
       " 2.6677920818328857,\n",
       " 3.6528444290161133,\n",
       " 6.100477695465088,\n",
       " 2.79036021232605,\n",
       " 3.1813206672668457,\n",
       " 2.792954921722412,\n",
       " 3.2698068618774414,\n",
       " 3.2877509593963623,\n",
       " 3.104398250579834,\n",
       " 3.06449818611145,\n",
       " 2.985886812210083,\n",
       " 3.3775758743286133,\n",
       " 3.4209213256835938,\n",
       " 2.271279811859131,\n",
       " 2.96205997467041,\n",
       " 3.6202521324157715,\n",
       " 2.6653168201446533,\n",
       " 2.7281441688537598,\n",
       " 2.8550655841827393,\n",
       " 2.7709500789642334,\n",
       " 3.7374002933502197,\n",
       " 3.289964437484741,\n",
       " 3.8142082691192627,\n",
       " 2.983691692352295,\n",
       " 2.468766689300537,\n",
       " 3.5176501274108887,\n",
       " 3.6213645935058594,\n",
       " 2.2839787006378174,\n",
       " 2.9173319339752197,\n",
       " 3.4015915393829346,\n",
       " 3.072787284851074,\n",
       " 2.5849058628082275,\n",
       " 2.6652872562408447,\n",
       " 2.7468159198760986,\n",
       " 3.123645305633545,\n",
       " 3.08168625831604,\n",
       " 2.800933361053467,\n",
       " 3.3404040336608887,\n",
       " 3.1747262477874756,\n",
       " 3.4530680179595947,\n",
       " 2.596580982208252,\n",
       " 2.910609483718872,\n",
       " 2.7811715602874756,\n",
       " 2.896390914916992,\n",
       " 2.9060285091400146,\n",
       " 2.510700225830078,\n",
       " 3.301072120666504,\n",
       " 3.096916675567627,\n",
       " 2.83139967918396,\n",
       " 3.2303426265716553,\n",
       " 3.7566614151000977,\n",
       " 4.290234565734863,\n",
       " 3.7500827312469482,\n",
       " 2.782181739807129,\n",
       " 3.258282423019409,\n",
       " 3.3201262950897217,\n",
       " 2.436147451400757,\n",
       " 2.2424492835998535,\n",
       " 2.7321648597717285,\n",
       " 3.353652238845825,\n",
       " 4.297788143157959,\n",
       " 4.290884017944336,\n",
       " 2.150052070617676,\n",
       " 4.195757865905762,\n",
       " 3.3935294151306152,\n",
       " 3.5348429679870605,\n",
       " 3.289613723754883,\n",
       " 3.6514289379119873,\n",
       " 3.9919872283935547,\n",
       " 3.588750123977661,\n",
       " 2.7543532848358154,\n",
       " 4.30449914932251,\n",
       " 3.974189281463623,\n",
       " 3.100846767425537,\n",
       " 2.5358500480651855,\n",
       " 6.80050802230835,\n",
       " 2.6350209712982178,\n",
       " 3.1627848148345947,\n",
       " 3.647386312484741,\n",
       " 3.189051628112793,\n",
       " 2.8488967418670654,\n",
       " 3.102717638015747,\n",
       " 3.197258234024048,\n",
       " 2.6917731761932373,\n",
       " 3.8508379459381104,\n",
       " 3.0060811042785645,\n",
       " 3.465197801589966,\n",
       " 2.9287140369415283,\n",
       " 5.361431121826172,\n",
       " 3.1886651515960693,\n",
       " 3.233105182647705,\n",
       " 3.146580457687378,\n",
       " 2.3946123123168945,\n",
       " 3.5759494304656982,\n",
       " 4.4433064460754395,\n",
       " 4.414612770080566,\n",
       " 3.1530160903930664,\n",
       " 2.6793458461761475,\n",
       " 2.9888126850128174,\n",
       " 3.676847457885742,\n",
       " 3.6832683086395264,\n",
       " 3.8683903217315674,\n",
       " 3.4602668285369873,\n",
       " 2.613201379776001,\n",
       " 3.2847836017608643,\n",
       " 3.1720542907714844,\n",
       " 2.522085189819336,\n",
       " 4.2009172439575195,\n",
       " 2.3814353942871094,\n",
       " 3.504366636276245,\n",
       " 2.816455841064453,\n",
       " 4.286692142486572,\n",
       " 2.5865442752838135,\n",
       " 2.569631338119507,\n",
       " 3.0301551818847656,\n",
       " 3.5104787349700928,\n",
       " 3.263514757156372,\n",
       " 2.704155445098877,\n",
       " 4.369734764099121,\n",
       " 3.8161497116088867,\n",
       " 3.7460081577301025,\n",
       " 2.620211124420166,\n",
       " 2.7548136711120605,\n",
       " 3.228548765182495,\n",
       " 4.636429309844971,\n",
       " 4.161596298217773,\n",
       " 2.9786643981933594,\n",
       " 2.832467555999756,\n",
       " 3.0466384887695312,\n",
       " 3.6390326023101807,\n",
       " 2.57781982421875,\n",
       " 3.7177624702453613,\n",
       " 3.474929094314575,\n",
       " 2.7576193809509277,\n",
       " 3.0915324687957764,\n",
       " 2.066267490386963,\n",
       " 2.7050626277923584,\n",
       " 3.55849289894104,\n",
       " 2.815124750137329,\n",
       " 3.084308385848999,\n",
       " 3.1691346168518066,\n",
       " 3.7719297409057617,\n",
       " 2.915125846862793,\n",
       " 3.416707754135132,\n",
       " 3.7457401752471924,\n",
       " 3.4030442237854004,\n",
       " 3.555040121078491,\n",
       " 3.476203441619873,\n",
       " 3.133307456970215,\n",
       " 3.5187759399414062,\n",
       " 3.3760616779327393,\n",
       " 4.143619537353516,\n",
       " 3.0797762870788574,\n",
       " 6.138646125793457,\n",
       " 3.2758312225341797,\n",
       " 3.6335020065307617,\n",
       " 3.464358329772949,\n",
       " 3.6288366317749023,\n",
       " 3.109440326690674,\n",
       " 4.4986748695373535,\n",
       " 3.1818833351135254,\n",
       " 3.3026723861694336,\n",
       " 2.596081495285034,\n",
       " 2.9590182304382324,\n",
       " 3.0554654598236084,\n",
       " 2.5283021926879883,\n",
       " 3.3695454597473145,\n",
       " 3.123779773712158,\n",
       " 4.217005252838135,\n",
       " 2.754108190536499,\n",
       " 2.4517338275909424,\n",
       " 2.8261799812316895,\n",
       " 3.8625073432922363,\n",
       " 2.7368977069854736,\n",
       " 3.2644450664520264,\n",
       " 2.7993130683898926,\n",
       " 4.3428635597229,\n",
       " 3.404346227645874,\n",
       " 2.6383137702941895,\n",
       " 3.1104607582092285,\n",
       " 2.7118053436279297,\n",
       " 2.2892255783081055,\n",
       " 2.7664051055908203,\n",
       " 2.653047561645508,\n",
       " 3.1740505695343018,\n",
       " 2.7101097106933594,\n",
       " 2.7941348552703857,\n",
       " 3.7335011959075928,\n",
       " 3.3566787242889404,\n",
       " 2.933058738708496,\n",
       " 2.6712396144866943,\n",
       " 2.6496448516845703,\n",
       " 4.294884204864502,\n",
       " 3.467111825942993,\n",
       " 3.8730435371398926,\n",
       " 3.828571081161499,\n",
       " 3.318730115890503,\n",
       " 3.4877097606658936,\n",
       " 1.9463133811950684,\n",
       " 3.491654634475708,\n",
       " 3.2739999294281006,\n",
       " 4.45891809463501,\n",
       " 3.0789568424224854,\n",
       " 3.3522768020629883,\n",
       " 4.858767509460449,\n",
       " 3.565861225128174,\n",
       " 2.8623971939086914,\n",
       " 2.400231122970581,\n",
       " 2.8547372817993164,\n",
       " 3.1298928260803223,\n",
       " 2.727768659591675,\n",
       " 3.083951711654663,\n",
       " 3.367790460586548,\n",
       " 4.259933948516846,\n",
       " 6.622640132904053,\n",
       " 3.718940258026123,\n",
       " 2.456939458847046,\n",
       " 3.104611873626709,\n",
       " 4.10465669631958,\n",
       " 2.826045274734497,\n",
       " 3.122885227203369,\n",
       " 3.0211334228515625,\n",
       " 3.4579060077667236,\n",
       " 6.974114418029785,\n",
       " 3.305424928665161,\n",
       " 3.099062204360962,\n",
       " 3.862684965133667,\n",
       " 3.1202025413513184,\n",
       " 4.264381408691406,\n",
       " 4.278669834136963,\n",
       " 3.1555778980255127,\n",
       " 3.3908851146698,\n",
       " 3.4454777240753174,\n",
       " 4.227512836456299,\n",
       " 3.590527057647705,\n",
       " 3.1000123023986816,\n",
       " 2.1028809547424316,\n",
       " 2.5154242515563965,\n",
       " 2.9924514293670654,\n",
       " 4.185580730438232,\n",
       " 3.1049578189849854,\n",
       " 3.6021804809570312,\n",
       " 3.068962812423706,\n",
       " 2.989145040512085,\n",
       " 3.0580224990844727,\n",
       " 4.139751434326172,\n",
       " 4.1240458488464355,\n",
       " 2.7429144382476807,\n",
       " 3.6058695316314697,\n",
       " 3.7110767364501953,\n",
       " 3.5249831676483154,\n",
       " 3.4363274574279785,\n",
       " 4.021237373352051,\n",
       " 3.430722236633301,\n",
       " 4.123775482177734,\n",
       " 3.0657811164855957,\n",
       " 2.726651906967163,\n",
       " 4.20978307723999,\n",
       " 3.0358524322509766,\n",
       " 3.119058847427368,\n",
       " 4.009148120880127,\n",
       " 3.596169948577881,\n",
       " 3.6491143703460693,\n",
       " 3.3915886878967285,\n",
       " 4.04298210144043,\n",
       " 2.2208218574523926,\n",
       " 4.209404945373535,\n",
       " 3.352696180343628,\n",
       " 4.413347244262695,\n",
       " 2.637565851211548,\n",
       " 2.8057353496551514,\n",
       " 3.439694881439209,\n",
       " 2.388659715652466,\n",
       " 3.0183112621307373,\n",
       " 3.014939308166504,\n",
       " 3.016566038131714,\n",
       " 3.5949997901916504,\n",
       " 2.9060802459716797,\n",
       " 4.3351240158081055,\n",
       " 4.325774192810059,\n",
       " 3.1231188774108887,\n",
       " 3.5264787673950195,\n",
       " 3.8246469497680664,\n",
       " 3.414302349090576,\n",
       " 3.3924221992492676,\n",
       " 4.545205116271973,\n",
       " 3.713568925857544,\n",
       " 3.5817174911499023,\n",
       " 3.8035531044006348,\n",
       " 3.4194750785827637,\n",
       " 2.4713380336761475,\n",
       " 2.654583692550659,\n",
       " 3.896723985671997,\n",
       " 3.0900702476501465,\n",
       " 3.0310611724853516,\n",
       " 3.1751708984375,\n",
       " 3.924445629119873,\n",
       " 3.123133897781372,\n",
       " 3.4340128898620605,\n",
       " 3.1876020431518555,\n",
       " 2.666996479034424,\n",
       " 4.352166175842285,\n",
       " 3.4017844200134277,\n",
       " 3.3186111450195312,\n",
       " 2.8699748516082764,\n",
       " 3.975222110748291,\n",
       " 3.4737355709075928,\n",
       " 2.136911392211914,\n",
       " 3.0925114154815674,\n",
       " 3.2482049465179443,\n",
       " 2.8124117851257324,\n",
       " 3.4384565353393555,\n",
       " 4.245534896850586,\n",
       " 3.1576478481292725,\n",
       " 2.9477152824401855,\n",
       " 3.1756246089935303,\n",
       " 3.462745189666748,\n",
       " 3.116363763809204,\n",
       " 2.8427183628082275,\n",
       " 3.0997917652130127,\n",
       " 4.055290699005127,\n",
       " 3.427823066711426,\n",
       " 3.1588022708892822,\n",
       " 3.385425329208374,\n",
       " 3.0136818885803223,\n",
       " 3.0372507572174072,\n",
       " 4.190882682800293,\n",
       " 2.9889943599700928,\n",
       " 2.7198116779327393,\n",
       " 3.894425392150879,\n",
       " 3.819181203842163,\n",
       " 3.4167442321777344,\n",
       " 2.963270902633667,\n",
       " 3.4595415592193604,\n",
       " 2.6704676151275635,\n",
       " 2.5150370597839355,\n",
       " 3.2173640727996826,\n",
       " 5.2257466316223145,\n",
       " 3.246469497680664,\n",
       " 3.1410114765167236,\n",
       " 3.9381279945373535,\n",
       " 2.9327893257141113,\n",
       " 3.4175751209259033,\n",
       " 4.62965726852417,\n",
       " 2.9958865642547607,\n",
       " 3.3043479919433594,\n",
       " 3.584071636199951,\n",
       " 3.289992332458496,\n",
       " 3.458413600921631,\n",
       " 3.1655325889587402,\n",
       " 3.2036497592926025,\n",
       " 3.0601048469543457,\n",
       " 3.773930311203003,\n",
       " 3.8856475353240967,\n",
       " 3.2635793685913086,\n",
       " 4.3520965576171875,\n",
       " 2.834719181060791,\n",
       " 3.7708311080932617,\n",
       " 3.32334566116333,\n",
       " 3.2618935108184814,\n",
       " 2.3931140899658203,\n",
       " 2.5557122230529785,\n",
       " 2.9977734088897705,\n",
       " 3.978633403778076,\n",
       " 2.3391401767730713,\n",
       " 3.268477201461792,\n",
       " 4.397780895233154,\n",
       " 2.3183419704437256,\n",
       " 3.219766139984131,\n",
       " 2.5059561729431152,\n",
       " 3.606219530105591,\n",
       " 4.532040596008301,\n",
       " 2.9259183406829834,\n",
       " 3.2658753395080566,\n",
       " 3.232100009918213,\n",
       " 3.31113600730896,\n",
       " 3.282771348953247,\n",
       " 4.0443501472473145,\n",
       " 2.9913628101348877,\n",
       " 4.5812087059021,\n",
       " 4.927351951599121,\n",
       " 3.1127328872680664,\n",
       " 3.1787168979644775,\n",
       " 4.372841835021973,\n",
       " 3.1630465984344482,\n",
       " 3.389624834060669,\n",
       " 3.8037919998168945,\n",
       " 3.8385350704193115,\n",
       " 3.2533469200134277,\n",
       " 2.4275012016296387,\n",
       " 3.530089855194092,\n",
       " 3.273970365524292,\n",
       " 3.0106239318847656,\n",
       " 3.5288760662078857,\n",
       " 4.517776012420654,\n",
       " 4.947826862335205,\n",
       " 4.26676607131958,\n",
       " 4.662677764892578,\n",
       " 3.7450122833251953,\n",
       " 4.097971439361572,\n",
       " 3.2703237533569336,\n",
       " 5.015269756317139,\n",
       " 3.4825263023376465,\n",
       " 2.2979655265808105,\n",
       " 4.814347743988037,\n",
       " 2.9664833545684814,\n",
       " 2.699594736099243,\n",
       " 3.3988840579986572,\n",
       " 3.847283363342285,\n",
       " 3.2734053134918213,\n",
       " 4.1889119148254395,\n",
       " 3.5928807258605957,\n",
       " 2.9435274600982666,\n",
       " 3.187274217605591,\n",
       " 4.3453288078308105,\n",
       " 3.4173879623413086,\n",
       " 4.124042987823486,\n",
       " 3.469759464263916,\n",
       " 4.4897074699401855,\n",
       " 3.0751843452453613,\n",
       " 3.6595895290374756,\n",
       " 3.6616973876953125,\n",
       " 3.15885329246521,\n",
       " 3.296182870864868,\n",
       " 3.7035961151123047,\n",
       " 3.785154342651367,\n",
       " 2.4053714275360107,\n",
       " 3.155258893966675,\n",
       " 3.1478285789489746,\n",
       " 3.66239070892334,\n",
       " 3.1834299564361572,\n",
       " 3.101222515106201,\n",
       " 3.624929189682007,\n",
       " 4.123587131500244,\n",
       " 2.993412733078003,\n",
       " 3.4483697414398193,\n",
       " 4.245351791381836,\n",
       " 3.0462167263031006,\n",
       " 3.8816585540771484,\n",
       " 3.4436588287353516,\n",
       " 3.721165657043457,\n",
       " 5.234457492828369,\n",
       " 3.932180881500244,\n",
       " 3.652465581893921,\n",
       " 3.1334948539733887,\n",
       " 4.13407564163208,\n",
       " 3.1988255977630615,\n",
       " 3.404494047164917,\n",
       " 3.168269395828247,\n",
       " 3.211743116378784,\n",
       " 3.7208797931671143,\n",
       " 2.6054952144622803,\n",
       " 3.291837692260742,\n",
       " 2.9712717533111572,\n",
       " 3.1359801292419434,\n",
       " 2.982630729675293,\n",
       " 3.0727756023406982,\n",
       " 3.1750805377960205,\n",
       " 3.0597481727600098,\n",
       " 3.1008081436157227,\n",
       " 3.4318037033081055,\n",
       " 3.75128436088562,\n",
       " 2.781946897506714,\n",
       " 3.0153868198394775,\n",
       " 3.8195528984069824,\n",
       " 3.564427614212036,\n",
       " 3.4691390991210938,\n",
       " 2.946410894393921,\n",
       " 3.0675930976867676,\n",
       " 3.7066102027893066,\n",
       " 4.52545690536499,\n",
       " 3.4107980728149414,\n",
       " 3.1838903427124023,\n",
       " 3.1787478923797607,\n",
       " 2.4724299907684326,\n",
       " 3.954094409942627,\n",
       " 3.8238658905029297,\n",
       " 3.654463291168213,\n",
       " 4.097415924072266,\n",
       " 2.8992552757263184,\n",
       " 2.8484506607055664,\n",
       " 4.308858871459961,\n",
       " 3.5696346759796143,\n",
       " 5.332550048828125,\n",
       " 4.993974208831787,\n",
       " 3.634056329727173,\n",
       " 2.7723662853240967,\n",
       " 3.2312188148498535,\n",
       " 2.9464111328125,\n",
       " 2.512726068496704,\n",
       " 4.340458393096924,\n",
       " 3.1501615047454834,\n",
       " 3.3226540088653564,\n",
       " 4.096237659454346,\n",
       " 3.4228320121765137,\n",
       " 3.390075922012329,\n",
       " 3.0384202003479004,\n",
       " 3.753166675567627,\n",
       " 4.179754257202148,\n",
       " 4.187178611755371,\n",
       " 3.4652111530303955,\n",
       " 4.08944034576416,\n",
       " 4.064539432525635,\n",
       " 3.563605308532715,\n",
       " 2.5958759784698486,\n",
       " 4.240665435791016,\n",
       " 4.174561500549316,\n",
       " 3.071971893310547,\n",
       " 3.536783456802368,\n",
       " 4.995552062988281,\n",
       " 2.388106346130371,\n",
       " 3.3886337280273438,\n",
       " 4.603221893310547,\n",
       " 3.771387815475464,\n",
       " 3.8737552165985107,\n",
       " 2.764209508895874,\n",
       " 4.285094738006592,\n",
       " 2.862138032913208,\n",
       " 5.031185626983643,\n",
       " 3.014380693435669,\n",
       " 3.660600185394287,\n",
       " 3.92997670173645,\n",
       " 4.103261947631836,\n",
       " 3.764838695526123,\n",
       " 3.9981777667999268,\n",
       " 3.5552175045013428,\n",
       " 3.370774269104004,\n",
       " 3.846609354019165,\n",
       " 3.268519878387451,\n",
       " 3.7655084133148193,\n",
       " 3.974670886993408,\n",
       " 3.290419578552246,\n",
       " 2.909388303756714,\n",
       " 4.063843727111816,\n",
       " 3.7519443035125732,\n",
       " 3.4966225624084473,\n",
       " 4.064993381500244,\n",
       " 3.68522047996521,\n",
       " 2.470667839050293,\n",
       " 3.9684600830078125,\n",
       " 3.870706081390381,\n",
       " 2.8977980613708496,\n",
       " 3.410681962966919,\n",
       " 4.461213111877441,\n",
       " 3.639803886413574,\n",
       " 2.8420896530151367,\n",
       " 3.4389615058898926,\n",
       " 3.812816619873047,\n",
       " 3.0295522212982178,\n",
       " 3.7219998836517334,\n",
       " 3.5223817825317383,\n",
       " 3.7371883392333984,\n",
       " 3.1939685344696045,\n",
       " 3.3440446853637695,\n",
       " 4.081580638885498,\n",
       " 3.025113105773926,\n",
       " 3.075611114501953,\n",
       " 3.7356910705566406,\n",
       " 4.61278772354126,\n",
       " 3.3392515182495117,\n",
       " 2.800644636154175,\n",
       " 3.0019028186798096,\n",
       " 3.7677228450775146,\n",
       " 2.9412777423858643,\n",
       " 3.170163631439209,\n",
       " 5.139147758483887,\n",
       " 3.1269290447235107,\n",
       " 3.0434937477111816,\n",
       " 3.145850658416748,\n",
       " 4.3536882400512695,\n",
       " 3.3394980430603027,\n",
       " 2.7649035453796387,\n",
       " 3.5586657524108887,\n",
       " 4.378036022186279,\n",
       " 3.6318471431732178,\n",
       " 3.969916820526123,\n",
       " 3.1122002601623535,\n",
       " 3.35021710395813,\n",
       " 3.0902581214904785,\n",
       " 3.4237594604492188,\n",
       " 3.1839213371276855,\n",
       " 3.5065228939056396,\n",
       " 3.178269624710083,\n",
       " 4.7897796630859375,\n",
       " 3.8823437690734863,\n",
       " 4.8019328117370605,\n",
       " 2.6527700424194336,\n",
       " 4.793079853057861,\n",
       " 2.840881109237671,\n",
       " 3.524885892868042,\n",
       " 3.3125734329223633,\n",
       " 4.760566234588623,\n",
       " 4.490409851074219,\n",
       " 3.8988325595855713,\n",
       " 3.6127467155456543,\n",
       " 3.6543803215026855,\n",
       " 4.668917655944824,\n",
       " 2.9797587394714355,\n",
       " 3.9537158012390137,\n",
       " 3.9007534980773926,\n",
       " 3.0384490489959717,\n",
       " 3.088974714279175,\n",
       " 5.843083381652832,\n",
       " 3.5153872966766357,\n",
       " 3.0044782161712646,\n",
       " 3.338177442550659,\n",
       " 3.2752296924591064,\n",
       " 3.0452842712402344,\n",
       " 3.722852945327759,\n",
       " 3.5855085849761963,\n",
       " 3.887531280517578,\n",
       " 3.597592353820801,\n",
       " 4.526500701904297,\n",
       " 3.5646419525146484,\n",
       " 5.606600284576416,\n",
       " 3.2266299724578857,\n",
       " 3.3180832862854004,\n",
       " 4.478261947631836,\n",
       " 2.8735740184783936,\n",
       " 3.5116288661956787,\n",
       " 3.2509796619415283,\n",
       " 3.595966100692749,\n",
       " 3.38096284866333,\n",
       " 3.4099843502044678,\n",
       " 2.577587366104126,\n",
       " 5.1310954093933105,\n",
       " 2.933281421661377,\n",
       " 3.454634189605713,\n",
       " 4.735561370849609,\n",
       " 3.148746967315674,\n",
       " 3.5746114253997803,\n",
       " 3.1642847061157227,\n",
       " 3.1919972896575928,\n",
       " 3.187229871749878,\n",
       " 3.815138816833496,\n",
       " 2.4350364208221436,\n",
       " 3.6282265186309814,\n",
       " 2.5857276916503906,\n",
       " 3.507920503616333,\n",
       " 2.4110159873962402,\n",
       " 2.9571619033813477,\n",
       " 3.8615264892578125,\n",
       " 2.929111957550049,\n",
       " 3.1378984451293945,\n",
       " 4.913702487945557,\n",
       " 3.4076480865478516,\n",
       " 4.598446846008301,\n",
       " 3.409022569656372,\n",
       " 3.6738414764404297,\n",
       " 3.4702603816986084,\n",
       " 2.9479761123657227,\n",
       " 2.8153371810913086,\n",
       " 4.725472927093506,\n",
       " 2.933990716934204,\n",
       " 2.837662935256958,\n",
       " 3.5867669582366943,\n",
       " 3.769352674484253,\n",
       " 5.362363338470459,\n",
       " 3.087348461151123,\n",
       " 4.264885425567627,\n",
       " 3.631376028060913,\n",
       " 3.0346925258636475,\n",
       " 3.847186326980591,\n",
       " 3.570688247680664,\n",
       " 3.471754312515259,\n",
       " 4.8604817390441895,\n",
       " 3.266308546066284,\n",
       " 4.184177398681641,\n",
       " 3.9465341567993164,\n",
       " 2.778308153152466,\n",
       " 3.363875150680542,\n",
       " 3.661841630935669,\n",
       " 2.9754927158355713,\n",
       " 4.52148962020874,\n",
       " 3.336181640625,\n",
       " 3.6679508686065674,\n",
       " 3.2555906772613525,\n",
       " 3.6092286109924316,\n",
       " 3.637204170227051,\n",
       " 3.471637725830078,\n",
       " 3.2183666229248047,\n",
       " 3.2979843616485596,\n",
       " 4.279727935791016,\n",
       " 3.728699207305908,\n",
       " 5.123340129852295,\n",
       " 3.5669243335723877,\n",
       " 3.5995309352874756,\n",
       " 4.3819193840026855,\n",
       " 3.5484602451324463,\n",
       " 4.3517374992370605,\n",
       " 5.227676868438721,\n",
       " 3.0154457092285156,\n",
       " 4.025684833526611,\n",
       " 3.235548496246338,\n",
       " 3.7062342166900635,\n",
       " 4.72833776473999,\n",
       " 3.752885341644287,\n",
       " 3.729191541671753,\n",
       " 2.81994891166687,\n",
       " 4.039932727813721,\n",
       " 3.8956105709075928,\n",
       " 2.7096054553985596,\n",
       " 5.176691055297852,\n",
       " 2.956075429916382,\n",
       " 3.694995164871216,\n",
       " 2.99001145362854,\n",
       " 3.667311906814575,\n",
       " 3.5581140518188477,\n",
       " 3.66703200340271,\n",
       " 4.484950065612793,\n",
       " 3.0128626823425293,\n",
       " 3.608649969100952,\n",
       " 3.109863042831421,\n",
       " 3.0829546451568604,\n",
       " 3.913416862487793,\n",
       " 3.2564938068389893,\n",
       " 3.717195749282837,\n",
       " 3.919811248779297,\n",
       " 3.063204526901245,\n",
       " 3.1025285720825195,\n",
       " 3.3004682064056396,\n",
       " 4.265435218811035,\n",
       " 4.454503536224365,\n",
       " 4.176027297973633,\n",
       " 3.3413913249969482,\n",
       " 2.94035267829895,\n",
       " 5.566734313964844,\n",
       " 3.754849433898926,\n",
       " 4.719997406005859,\n",
       " 3.3835806846618652,\n",
       " 3.3656880855560303,\n",
       " 3.4778501987457275,\n",
       " 2.610274314880371,\n",
       " 4.128383159637451,\n",
       " 3.9539144039154053,\n",
       " 4.034056186676025,\n",
       " 3.042243003845215,\n",
       " 3.559004306793213,\n",
       " 4.01839017868042,\n",
       " 3.1897013187408447,\n",
       " 4.1848673820495605,\n",
       " 3.8473761081695557,\n",
       " 3.3437397480010986,\n",
       " 3.776655673980713,\n",
       " 3.3072457313537598,\n",
       " 4.1096343994140625,\n",
       " 2.8768348693847656,\n",
       " 4.167577266693115,\n",
       " 5.629001140594482,\n",
       " 3.869128704071045,\n",
       " 4.038875579833984,\n",
       " 3.262883424758911,\n",
       " 2.984302043914795,\n",
       " 3.7519962787628174,\n",
       " 3.6559088230133057,\n",
       " 3.2345852851867676,\n",
       " 4.547745227813721,\n",
       " 3.587549924850464,\n",
       " 4.423497200012207,\n",
       " 3.178809881210327,\n",
       " 4.799380779266357,\n",
       " 3.8416008949279785,\n",
       " 3.2192959785461426,\n",
       " 3.235185384750366,\n",
       " 4.712989330291748,\n",
       " 4.474794387817383,\n",
       " 4.72671365737915,\n",
       " 3.3815886974334717,\n",
       " 3.002497911453247,\n",
       " 3.2813427448272705,\n",
       " 3.7322707176208496,\n",
       " 3.289327621459961,\n",
       " 3.4607789516448975,\n",
       " 5.033384799957275,\n",
       " 3.5999391078948975,\n",
       " 4.95499324798584,\n",
       " 3.9563918113708496,\n",
       " 3.1316919326782227,\n",
       " 2.704028844833374,\n",
       " 2.885101556777954,\n",
       " 3.498438596725464,\n",
       " 3.4679312705993652,\n",
       " 4.519292831420898,\n",
       " 3.6675853729248047,\n",
       " 3.654433012008667,\n",
       " 3.6952688694000244,\n",
       " 3.3033993244171143,\n",
       " 3.5234594345092773,\n",
       " 3.292090654373169,\n",
       " 3.0727717876434326,\n",
       " 3.2134313583374023,\n",
       " 2.9887595176696777,\n",
       " 2.926907539367676,\n",
       " 2.869741678237915,\n",
       " 4.301468849182129,\n",
       " 4.722286701202393,\n",
       " 4.100219249725342,\n",
       " 4.029407978057861,\n",
       " 3.842815637588501,\n",
       " 2.6922671794891357,\n",
       " 3.332327127456665,\n",
       " 3.592224359512329,\n",
       " 3.2695207595825195,\n",
       " 4.306662559509277,\n",
       " 3.2284131050109863,\n",
       " 4.704573154449463,\n",
       " 2.5061798095703125,\n",
       " 3.5249264240264893,\n",
       " 3.3747730255126953,\n",
       " 3.5337777137756348,\n",
       " 4.821375846862793,\n",
       " 3.6021392345428467,\n",
       " 3.612968921661377,\n",
       " 3.0700223445892334,\n",
       " 3.0752317905426025,\n",
       " 4.850182056427002,\n",
       " 3.808856725692749,\n",
       " 2.9608683586120605,\n",
       " 3.4096059799194336,\n",
       " 4.419686794281006,\n",
       " 3.466529369354248,\n",
       " 3.315812110900879,\n",
       " 3.558452606201172,\n",
       " 3.7349793910980225,\n",
       " 2.9230246543884277,\n",
       " 4.482275485992432,\n",
       " 3.763821601867676,\n",
       " 3.2834665775299072,\n",
       " 5.17568302154541,\n",
       " 2.2880585193634033,\n",
       " 3.3990511894226074,\n",
       " 4.4585700035095215,\n",
       " 3.3467659950256348,\n",
       " 3.3255622386932373,\n",
       " 3.784653902053833,\n",
       " 3.374939203262329,\n",
       " 4.099897384643555,\n",
       " 4.121765613555908,\n",
       " 4.381776809692383,\n",
       " 3.5213217735290527,\n",
       " 3.5890800952911377,\n",
       " 3.0968079566955566,\n",
       " 3.881743907928467,\n",
       " 2.936870813369751,\n",
       " 3.7871387004852295,\n",
       " 3.6218769550323486,\n",
       " 3.7058515548706055,\n",
       " 3.3138654232025146,\n",
       " 4.4941792488098145,\n",
       " 3.322420835494995,\n",
       " 3.5666704177856445,\n",
       " 4.118414878845215,\n",
       " 3.363574981689453,\n",
       " 4.419640064239502,\n",
       " 2.762772798538208,\n",
       " 4.126655578613281,\n",
       " 3.676604986190796,\n",
       " 4.1444172859191895,\n",
       " 3.2460122108459473,\n",
       " 3.87874698638916,\n",
       " 3.7273471355438232,\n",
       " 5.017288684844971,\n",
       " 3.7386209964752197,\n",
       " 5.330987930297852,\n",
       " 3.0679867267608643,\n",
       " 4.436997890472412,\n",
       " 3.587216854095459,\n",
       " 3.561785936355591,\n",
       " 3.8151674270629883,\n",
       " 3.8296775817871094,\n",
       " 4.555716514587402,\n",
       " 5.0296950340271,\n",
       " 3.94492506980896,\n",
       " 3.497617483139038,\n",
       " 4.185582160949707,\n",
       " 3.831894874572754,\n",
       " 4.823349475860596,\n",
       " 2.9357922077178955,\n",
       " 3.881889581680298,\n",
       " 4.048800468444824,\n",
       " 2.8254828453063965,\n",
       " 3.3527684211730957,\n",
       " 4.648612976074219,\n",
       " ...]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(token_embeddings, axis = -1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is tempting to try to resize vectors so that they have norm at least one,\n",
    "# but it is actually very harmful, so don't do this\n",
    "\n",
    "# resizing = np.maximum(1, (1/np.linalg.norm(token_embeddings, axis = -1))).reshape(-1, 1).repeat(128, axis = 1)\n",
    "# token_embeddings = token_embeddings * resizing\n",
    "# np.linalg.norm(token_embeddings, axis = -1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remark: if weights were tied, then the model is subsequently in the following state:\n",
    "# - weights are now untied\n",
    "# - encoder weights are overwritten by new weights\n",
    "# - decoder weights are left as they were initially (e.g not affected by loading pretrained weights)\n",
    "model.albert.embeddings.word_embeddings = model.albert.embeddings.word_embeddings.from_pretrained(\n",
    "    torch.tensor(token_embeddings), \n",
    "    padding_idx = tokenizer._pad_token_type_id,\n",
    "    freeze = True, # default is True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7205400 5285400\n",
      "tensor([-0.0031,  0.0003,  0.0060,  0.0042,  0.0035, -0.0001, -0.0077, -0.0075,\n",
      "         0.0052, -0.0036])\n",
      "tensor([-0.0141, -0.0102, -0.0209, -0.0094, -0.0127,  0.0317,  0.0097,  0.0114,\n",
      "         0.0061, -0.0220], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# if there was a weight tying with LM head, then loading pretrained word embeddings broke it\n",
    "print(model.num_parameters(), sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "# only encoder weights were affected, decoder weights are left as is\n",
    "print(model.albert.embeddings.word_embeddings._parameters['weight'][1][:10])\n",
    "print(model.predictions.decoder._parameters['weight'][1][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7205400 5285400\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# # if we want to freeze \"manually\" the word embedding layer\n",
    "# for param in model.albert.embeddings.word_embeddings.parameters():\n",
    "#     param.requires_grad = False\n",
    "    \n",
    "print(model.num_parameters(), sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "# decoder word embeddings are frozen only when encoder is frozen and weights are tied (except bias vector)\n",
    "for param in model.predictions.decoder.parameters():\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Model training\n",
    "\n",
    "[Table of content](#TOC)\n",
    "\n",
    "`Albert-vase-v2` training parameters as provided in https://github.com/google-research/albert/blob/master/run_pretraining.py : \n",
    "- max_predictions_per_seq = `20`\n",
    "- train_batch_size = `4096`\n",
    "- optimizer = `\"lamb\"`\n",
    "- learning_rate = `0.00176`\n",
    "- poly_power = `1.0`\n",
    "- num_train_steps = `125000`\n",
    "- num_warmup_steps = `3125`\n",
    "- start_warmup_step = `0`\n",
    "- iterations_per_loop = `1000`\n",
    "\n",
    "The original optimizer is `lamb`, which was designed for very large batch size, see the [Lamb paper](https://arxiv.org/pdf/1904.00962.pdf), but we use here the default [AdamW](https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#transformers.AdamW) optimizer with [linear learning rate decay](https://huggingface.co/docs/transformers/v4.23.1/en/main_classes/optimizer_schedules#transformers.get_linear_schedule_with_warmup), as specified in the [Trainer class documentation](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer.optimizers). See the [AdamW paper](https://arxiv.org/pdf/1711.05101.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16 # <= 16 for batch_size = 512 and GPU RAM = 8GB\n",
    "\n",
    "args = TrainingArguments(\n",
    "    os.path.join(path_to_save, '_checkpoints'),\n",
    "    evaluation_strategy = \"no\",\n",
    "    learning_rate = 5e-4,\n",
    "    num_train_epochs = 3,\n",
    "    lr_scheduler_type = 'linear', # 'constant_with_warmup',\n",
    "    warmup_steps = 1000,\n",
    "    gradient_accumulation_steps = 1,\n",
    "    per_device_train_batch_size = batch_size,\n",
    "    per_device_eval_batch_size = batch_size,\n",
    "    save_strategy = 'no',\n",
    "    logging_steps = 100,\n",
    "    seed = 42,\n",
    "    data_seed = 23,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm_probability = 0.15),\n",
    "    train_dataset = mlm_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some remarks:\n",
    "\n",
    "- The `data_collator` is the object used to batch elements of the training & evaluation datasets.\n",
    "- The `tokenizer` is provided in order to automatically pad the inputs to the maximum length when batching inputs, and to have it saved along the model, which makes it easier to rerun an interrupted training or reuse the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pretrained token embedding, frozen, untied, lr = 5e-4\n",
    "# pretrained token embedding, tied, lr = 5e-4\n",
    "# pretrained token embedding, frozen, tied, lr = 5e-4\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(os.path.join(path_to_save, final_model_name, 'model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Model training with unfrozen token embedding table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(os.path.join(path_to_save, final_model_name, 'model'))\n",
    "model = model.to(device).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7205400 7205400\n"
     ]
    }
   ],
   "source": [
    "# loading pretrained word embeddings has unfrozen word embeddings\n",
    "# print(model.num_parameters(), sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "# freeze word embeddings if desired\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = True\n",
    "    \n",
    "print(model.num_parameters(), sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 # <= 16 for batch_size = 512 and GPU RAM = 8GB\n",
    "\n",
    "args = TrainingArguments(\n",
    "    os.path.join(path_to_save, '_checkpoints'),\n",
    "    evaluation_strategy = \"no\",\n",
    "    learning_rate = 5e-5,\n",
    "    num_train_epochs = 1,\n",
    "    lr_scheduler_type = 'linear',\n",
    "    warmup_steps = 0,\n",
    "    gradient_accumulation_steps = 1,\n",
    "    per_device_train_batch_size = batch_size,\n",
    "    per_device_eval_batch_size = batch_size,\n",
    "    save_strategy = 'no',\n",
    "    logging_steps = 100,\n",
    "    seed = 42,\n",
    "    data_seed = 23,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm_probability = 0.15),\n",
    "    train_dataset = mlm_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `AlbertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `AlbertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\jb\\miniconda3\\envs\\transformers_nlp\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 595255\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 37204\n",
      "You're using a AlbertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='37204' max='37204' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [37204/37204 3:29:41, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.081600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.056400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.063100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.072400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.072400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.051400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.065100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.080400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.072400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.084000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.052100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.054700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.066200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.063700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.063100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.072800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.083100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.069100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.051400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.071800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.075200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.072200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.084400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.066000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.064200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.063800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.075700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.065000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.062100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.044600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.084800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.074000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.079800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.064200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.086000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.048800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.070100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.065900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>1.069200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.070500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>1.063900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.062600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.060900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.073100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>1.065500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.083000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>1.068300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.062900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>1.058100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.071300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>1.065400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>1.076300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.070900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>1.062000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>1.065100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>1.064100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>1.069800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.065500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>1.054300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>1.084800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>1.067500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>1.059400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.055200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>1.067200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>1.047500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>1.072000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>1.088100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.085800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>1.073800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>1.087300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>1.047400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>1.067100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.056700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>1.053800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>1.053200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>1.042600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>1.072800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.051200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>1.073300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>1.052600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>1.058300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>1.063500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.068900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>1.068200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>1.061800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>1.056500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>1.045100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.059400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>1.042600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>1.068600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>1.052400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>1.060700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.070500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>1.055900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>1.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>1.071700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>1.042400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.075900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>1.047400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>1.054000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>1.055600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>1.060300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.041900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>1.074100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>1.048400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>1.044200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>1.063800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.033800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>1.067300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>1.085400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>1.058400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>1.061300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.064000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>1.015800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>1.054300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>1.053800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>1.035700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.050300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>1.067500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>1.057400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>1.033500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>1.050300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>1.059900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>1.055300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12700</td>\n",
       "      <td>1.067100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>1.056400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>1.059400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.062100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13100</td>\n",
       "      <td>1.068700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>1.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13300</td>\n",
       "      <td>1.061100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>1.052700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>1.049200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>1.064900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13700</td>\n",
       "      <td>1.063200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>1.035300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13900</td>\n",
       "      <td>1.029800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.050300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>1.050200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>1.051800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14300</td>\n",
       "      <td>1.049800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>1.057600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>1.072100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>1.037700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>1.058300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>1.049000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14900</td>\n",
       "      <td>1.056900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.042800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15100</td>\n",
       "      <td>1.044800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>1.042700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15300</td>\n",
       "      <td>1.046300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>1.048800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>1.057500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>1.058300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15700</td>\n",
       "      <td>1.025200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>1.037400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15900</td>\n",
       "      <td>1.080600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.044900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16100</td>\n",
       "      <td>1.035800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>1.039200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16300</td>\n",
       "      <td>1.049900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>1.068600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>1.054800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>1.052900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16700</td>\n",
       "      <td>1.005800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>1.042900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16900</td>\n",
       "      <td>1.051900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>1.042300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17100</td>\n",
       "      <td>1.046400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>1.037300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17300</td>\n",
       "      <td>1.048600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>1.030500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>1.041200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>1.052500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17700</td>\n",
       "      <td>1.044700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>1.055900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17900</td>\n",
       "      <td>1.047800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>1.054200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18100</td>\n",
       "      <td>1.034200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>1.040300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18300</td>\n",
       "      <td>1.049900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>1.048200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>1.042700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>1.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18700</td>\n",
       "      <td>1.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>1.018300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18900</td>\n",
       "      <td>1.046000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>1.041700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19100</td>\n",
       "      <td>1.018900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>1.021600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19300</td>\n",
       "      <td>1.037400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19400</td>\n",
       "      <td>1.057800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>1.052800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>1.018400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19700</td>\n",
       "      <td>1.034700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19800</td>\n",
       "      <td>1.054800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19900</td>\n",
       "      <td>1.059000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>1.057900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20100</td>\n",
       "      <td>1.036700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200</td>\n",
       "      <td>1.055300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20300</td>\n",
       "      <td>1.037400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20400</td>\n",
       "      <td>1.023100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>1.034100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20600</td>\n",
       "      <td>1.045400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20700</td>\n",
       "      <td>1.046200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20800</td>\n",
       "      <td>1.032600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20900</td>\n",
       "      <td>1.029600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>1.059800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21100</td>\n",
       "      <td>1.037200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21200</td>\n",
       "      <td>1.038100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21300</td>\n",
       "      <td>1.050700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21400</td>\n",
       "      <td>1.023500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>1.030600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21600</td>\n",
       "      <td>1.032100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21700</td>\n",
       "      <td>1.036900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21800</td>\n",
       "      <td>1.047800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21900</td>\n",
       "      <td>1.051100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>1.036700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22100</td>\n",
       "      <td>1.028400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22200</td>\n",
       "      <td>1.054500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22300</td>\n",
       "      <td>1.060200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22400</td>\n",
       "      <td>1.045800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>1.015300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22600</td>\n",
       "      <td>1.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22700</td>\n",
       "      <td>1.019900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22800</td>\n",
       "      <td>1.038300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22900</td>\n",
       "      <td>1.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>1.030900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23100</td>\n",
       "      <td>1.031100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23200</td>\n",
       "      <td>1.026500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23300</td>\n",
       "      <td>1.033600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23400</td>\n",
       "      <td>1.040600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>1.025300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23600</td>\n",
       "      <td>1.042200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23700</td>\n",
       "      <td>1.034200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23800</td>\n",
       "      <td>1.050100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23900</td>\n",
       "      <td>1.051800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>1.049000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24100</td>\n",
       "      <td>1.035400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24200</td>\n",
       "      <td>1.044200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24300</td>\n",
       "      <td>1.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24400</td>\n",
       "      <td>1.043200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>1.025500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24600</td>\n",
       "      <td>1.020200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24700</td>\n",
       "      <td>1.038400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24800</td>\n",
       "      <td>1.033400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24900</td>\n",
       "      <td>1.030700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>1.041300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25100</td>\n",
       "      <td>1.059800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25200</td>\n",
       "      <td>1.023800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25300</td>\n",
       "      <td>1.028400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25400</td>\n",
       "      <td>1.007400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>1.034300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25600</td>\n",
       "      <td>1.037900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25700</td>\n",
       "      <td>1.040900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25800</td>\n",
       "      <td>1.021900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25900</td>\n",
       "      <td>1.018400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>1.028900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26100</td>\n",
       "      <td>1.038900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26200</td>\n",
       "      <td>1.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26300</td>\n",
       "      <td>1.032300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26400</td>\n",
       "      <td>1.008500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>1.029300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26600</td>\n",
       "      <td>1.017000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26700</td>\n",
       "      <td>1.037300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26800</td>\n",
       "      <td>1.022100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26900</td>\n",
       "      <td>1.036100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>1.024400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27100</td>\n",
       "      <td>1.052600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27200</td>\n",
       "      <td>1.040900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27300</td>\n",
       "      <td>1.032500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27400</td>\n",
       "      <td>1.033100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>1.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27600</td>\n",
       "      <td>1.014900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27700</td>\n",
       "      <td>1.028700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27800</td>\n",
       "      <td>1.030500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27900</td>\n",
       "      <td>1.016200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>1.040500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28100</td>\n",
       "      <td>1.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28200</td>\n",
       "      <td>1.057800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28300</td>\n",
       "      <td>1.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28400</td>\n",
       "      <td>1.038200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>1.021800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28600</td>\n",
       "      <td>1.029300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28700</td>\n",
       "      <td>1.029200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28800</td>\n",
       "      <td>1.031600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28900</td>\n",
       "      <td>1.017200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>1.021200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29100</td>\n",
       "      <td>1.037600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29200</td>\n",
       "      <td>1.021700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29300</td>\n",
       "      <td>1.024900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29400</td>\n",
       "      <td>1.028300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>1.031200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29600</td>\n",
       "      <td>1.027500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29700</td>\n",
       "      <td>1.033200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29800</td>\n",
       "      <td>1.039900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29900</td>\n",
       "      <td>1.024400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>1.039800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30100</td>\n",
       "      <td>1.017700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30200</td>\n",
       "      <td>1.038500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30300</td>\n",
       "      <td>1.014600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30400</td>\n",
       "      <td>1.025200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>1.034100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30600</td>\n",
       "      <td>1.043100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30700</td>\n",
       "      <td>1.018200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30800</td>\n",
       "      <td>1.024500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30900</td>\n",
       "      <td>1.024600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>1.035200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31100</td>\n",
       "      <td>1.034900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31200</td>\n",
       "      <td>1.044600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31300</td>\n",
       "      <td>1.022600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31400</td>\n",
       "      <td>1.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>1.041600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31600</td>\n",
       "      <td>1.030500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31700</td>\n",
       "      <td>1.033500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31800</td>\n",
       "      <td>1.032400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31900</td>\n",
       "      <td>1.035700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>1.019100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32100</td>\n",
       "      <td>1.017400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32200</td>\n",
       "      <td>1.027900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32300</td>\n",
       "      <td>1.023400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32400</td>\n",
       "      <td>1.037800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>1.023900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32600</td>\n",
       "      <td>1.022900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32700</td>\n",
       "      <td>1.045300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32800</td>\n",
       "      <td>1.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32900</td>\n",
       "      <td>1.033200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>1.047800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33100</td>\n",
       "      <td>1.011600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33200</td>\n",
       "      <td>1.017600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33300</td>\n",
       "      <td>1.039800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33400</td>\n",
       "      <td>1.017200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>1.029400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33600</td>\n",
       "      <td>1.011200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33700</td>\n",
       "      <td>1.013900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33800</td>\n",
       "      <td>1.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33900</td>\n",
       "      <td>1.004400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>1.022800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34100</td>\n",
       "      <td>1.026300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34200</td>\n",
       "      <td>1.031200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34300</td>\n",
       "      <td>1.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34400</td>\n",
       "      <td>1.013800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>1.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34600</td>\n",
       "      <td>1.020500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34700</td>\n",
       "      <td>1.012900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34800</td>\n",
       "      <td>1.032100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34900</td>\n",
       "      <td>1.027800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>1.018500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35100</td>\n",
       "      <td>1.020200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35200</td>\n",
       "      <td>1.047600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35300</td>\n",
       "      <td>1.023300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35400</td>\n",
       "      <td>1.013400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>1.008500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35600</td>\n",
       "      <td>1.008400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35700</td>\n",
       "      <td>1.027700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35800</td>\n",
       "      <td>1.024800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35900</td>\n",
       "      <td>1.038200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>1.021200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36100</td>\n",
       "      <td>1.021300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36200</td>\n",
       "      <td>0.998900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36300</td>\n",
       "      <td>1.026800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36400</td>\n",
       "      <td>1.029400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>1.024800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36600</td>\n",
       "      <td>1.016100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36700</td>\n",
       "      <td>1.022600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36800</td>\n",
       "      <td>1.013400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36900</td>\n",
       "      <td>1.043800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>1.004300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37100</td>\n",
       "      <td>1.018700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37200</td>\n",
       "      <td>1.024800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=37204, training_loss=1.044597110419155, metrics={'train_runtime': 12584.6073, 'train_samples_per_second': 47.3, 'train_steps_per_second': 2.956, 'total_flos': 9544697118842880.0, 'train_loss': 1.044597110419155, 'epoch': 1.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to C:\\Users\\jb\\Desktop\\NLP\\perso - Transformers for NLP\\saves\\MLM\\albert-small-ictrp\\model\n",
      "Configuration saved in C:\\Users\\jb\\Desktop\\NLP\\perso - Transformers for NLP\\saves\\MLM\\albert-small-ictrp\\model\\config.json\n",
      "Model weights saved in C:\\Users\\jb\\Desktop\\NLP\\perso - Transformers for NLP\\saves\\MLM\\albert-small-ictrp\\model\\pytorch_model.bin\n",
      "tokenizer config file saved in C:\\Users\\jb\\Desktop\\NLP\\perso - Transformers for NLP\\saves\\MLM\\albert-small-ictrp\\model\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\jb\\Desktop\\NLP\\perso - Transformers for NLP\\saves\\MLM\\albert-small-ictrp\\model\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(os.path.join(path_to_save, final_model_name, 'model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"inference\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Inference\n",
    "\n",
    "[Table of content](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(os.path.join(path_to_save, final_model_name, 'tokenizer'))\n",
    "model = AutoModelForMaskedLM.from_pretrained(os.path.join(path_to_save, final_model_name, 'model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm = pipeline(\n",
    "    task = 'fill-mask', \n",
    "    model = model, \n",
    "    tokenizer = tokenizer,\n",
    "    framework = 'pt',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'score': 0.7487446665763855,\n",
       "   'token': 6,\n",
       "   'token_str': ',',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to, demyelinating neuropathies,[MASK] secondary to infection or systemic[MASK], diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor[MASK], monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'},\n",
       "  {'score': 0.05170542001724243,\n",
       "   'token': 38,\n",
       "   'token_str': ':',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to: demyelinating neuropathies,[MASK] secondary to infection or systemic[MASK], diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor[MASK], monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'},\n",
       "  {'score': 0.032817292958498,\n",
       "   'token': 63,\n",
       "   'token_str': 'other',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to other demyelinating neuropathies,[MASK] secondary to infection or systemic[MASK], diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor[MASK], monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'},\n",
       "  {'score': 0.011818677186965942,\n",
       "   'token': 7,\n",
       "   'token_str': '.',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to. demyelinating neuropathies,[MASK] secondary to infection or systemic[MASK], diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor[MASK], monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'},\n",
       "  {'score': 0.01019338984042406,\n",
       "   'token': 4539,\n",
       "   'token_str': 'juvenile',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to juvenile demyelinating neuropathies,[MASK] secondary to infection or systemic[MASK], diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor[MASK], monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'}],\n",
       " [{'score': 0.17531755566596985,\n",
       "   'token': 736,\n",
       "   'token_str': 'neuropathy',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to[MASK] demyelinating neuropathies, neuropathy secondary to infection or systemic[MASK], diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor[MASK], monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'},\n",
       "  {'score': 0.05949920415878296,\n",
       "   'token': 287,\n",
       "   'token_str': 'either',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to[MASK] demyelinating neuropathies, either secondary to infection or systemic[MASK], diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor[MASK], monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'},\n",
       "  {'score': 0.04453514516353607,\n",
       "   'token': 60,\n",
       "   'token_str': 'not',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to[MASK] demyelinating neuropathies, not secondary to infection or systemic[MASK], diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor[MASK], monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'},\n",
       "  {'score': 0.04055783152580261,\n",
       "   'token': 244,\n",
       "   'token_str': 'symptoms',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to[MASK] demyelinating neuropathies, symptoms secondary to infection or systemic[MASK], diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor[MASK], monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'},\n",
       "  {'score': 0.03391161561012268,\n",
       "   'token': 219,\n",
       "   'token_str': 'syndrome',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to[MASK] demyelinating neuropathies, syndrome secondary to infection or systemic[MASK], diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor[MASK], monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'}],\n",
       " [{'score': 0.316965252161026,\n",
       "   'token': 1259,\n",
       "   'token_str': 'sclerosis',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to[MASK] demyelinating neuropathies,[MASK] secondary to infection or systemic sclerosis, diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor[MASK], monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'},\n",
       "  {'score': 0.11200817674398422,\n",
       "   'token': 42,\n",
       "   'token_str': 'disease',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to[MASK] demyelinating neuropathies,[MASK] secondary to infection or systemic disease, diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor[MASK], monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'},\n",
       "  {'score': 0.08061954379081726,\n",
       "   'token': 125,\n",
       "   'token_str': 'infection',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to[MASK] demyelinating neuropathies,[MASK] secondary to infection or systemic infection, diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor[MASK], monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'},\n",
       "  {'score': 0.07801665365695953,\n",
       "   'token': 1267,\n",
       "   'token_str': 'lupus',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to[MASK] demyelinating neuropathies,[MASK] secondary to infection or systemic lupus, diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor[MASK], monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'},\n",
       "  {'score': 0.04757551848888397,\n",
       "   'token': 2452,\n",
       "   'token_str': 'vasculitis',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to[MASK] demyelinating neuropathies,[MASK] secondary to infection or systemic vasculitis, diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor[MASK], monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'}],\n",
       " [{'score': 0.6265413761138916,\n",
       "   'token': 736,\n",
       "   'token_str': 'neuropathy',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to[MASK] demyelinating neuropathies,[MASK] secondary to infection or systemic[MASK], diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor neuropathy, monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'},\n",
       "  {'score': 0.05695892870426178,\n",
       "   'token': 6742,\n",
       "   'token_str': 'neuropathies',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to[MASK] demyelinating neuropathies,[MASK] secondary to infection or systemic[MASK], diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor neuropathies, monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'},\n",
       "  {'score': 0.027800418436527252,\n",
       "   'token': 2657,\n",
       "   'token_str': 'atrophy',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to[MASK] demyelinating neuropathies,[MASK] secondary to infection or systemic[MASK], diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor atrophy, monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'},\n",
       "  {'score': 0.021542217582464218,\n",
       "   'token': 456,\n",
       "   'token_str': 'abnormalities',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to[MASK] demyelinating neuropathies,[MASK] secondary to infection or systemic[MASK], diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor abnormalities, monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'},\n",
       "  {'score': 0.020566511899232864,\n",
       "   'token': 411,\n",
       "   'token_str': 'impairment',\n",
       "   'sequence': '[CLS] polyneuropathy of other causes, including but not limited to[MASK] demyelinating neuropathies,[MASK] secondary to infection or systemic[MASK], diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor impairment, monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory cidp and acquired demyelinating symmetric (dads) neuropathy (also known as distal cidp).[SEP]'}]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = 'Polyneuropathy of other causes, including but not limited to hereditary demyelinating neuropathies, neuropathies secondary to infection or systemic disease, diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor neuropathy, monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory CIDP and acquired demyelinating symmetric (DADS) neuropathy (also known as distal CIDP).'\n",
    "sent = f'Polyneuropathy of other causes, including but not limited to  {mlm.tokenizer.mask_token} demyelinating neuropathies, {mlm.tokenizer.mask_token} secondary to infection or systemic {mlm.tokenizer.mask_token}, diabetic neuropathy, drug- or toxin-induced neuropathies, multifocal motor {mlm.tokenizer.mask_token}, monoclonal gammopathy of uncertain significance, lumbosacral radiculoplexus neuropathy, pure sensory CIDP and acquired demyelinating symmetric (DADS) neuropathy (also known as distal CIDP).'\n",
    "mlm(sent, top_k = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Table of content](#TOC)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Token Classification",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
