# params of datasets.load_dataset
dataset_args:
    path: 'text'
    data_files:
        train: .\data\ctti\ctti-corpus.txt

is_tokenized: false
concat_inputs: false

# params of transformers.AutoTokenizer.from_pretrained
tokenizer_args:
    pretrained_model_name_or_path: albert-base-v2

# params of transformers.AutoModelForMaskedLM.from_pretrained
model_args:
    pretrained_model_name_or_path: albert-base-v2

# params of transformers.TrainingArguments
training_args:
    evaluation_strategy: 'no'
    learning_rate: 1e-4
    num_train_epochs: 1
    warmup_steps: 1500
    gradient_accumulation_steps: 1
    per_device_train_batch_size: 6
    per_device_eval_batch_size: 6
    save_strategy: 'no'
    logging_steps: 100
    seed: 42
    data_seed: 23
    torch_compile: false

# params of CustomDataCollatorForLanguageModeling
collator_args:
    task_proportions:
        - 14  # mask
        - 3   # random noise
        - 3   # keep to learn
        - 80  # ignore

