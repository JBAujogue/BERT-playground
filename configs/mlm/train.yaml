global_seed: 42
is_tokenized: false
concat_inputs: false

# params of datasets.load_dataset
dataset_args:
    path: 'text'
    data_files:
        train: data/ctti/ctti-corpus-dummy.txt
        eval: data/ctti/ctti-corpus-dummy.txt
        test: data/ctti/ctti-corpus-dummy.txt

# params of transformers.AutoTokenizer.from_pretrained
tokenizer_args:
    pretrained_model_name_or_path: albert-base-v2

# params of transformers.AutoModelForMaskedLM.from_pretrained
model_args:
    pretrained_model_name_or_path: albert-base-v2

# params of transformers.TrainingArguments
training_args:
    evaluation_strategy: 'no'
    learning_rate: 1.e-4
    lr_scheduler_type: 'constant'
    num_train_epochs: 3
    max_steps: 500
    warmup_steps: 0
    gradient_accumulation_steps: 1
    per_device_train_batch_size: 6
    per_device_eval_batch_size: 6
    dataloader_pin_memory: true
    dataloader_num_workers: 0
    bf16: false
    tf32: true
    torch_compile: false
    save_strategy: 'no'
    logging_steps: 100
    seed: 42
    data_seed: 23
    remove_unused_columns: true

# params of DataCollatorForMLM
collator_args:
    task_proportions:
        - 14  # mask
        - 3   # random noise
        - 3   # keep to learn
        - 80  # ignore

