global_seed: 42
is_tokenized: true
concat_inputs: false

# params of datasets.load_dataset
dataset_args:
    path: 'parquet'
    data_files:
        train: data/chia/chia-ner/splits/train.parquet
        eval: data/chia/chia-ner/splits/eval.parquet
        test: data/chia/chia-ner/splits/test.parquet

# params of transformers.AutoTokenizer.from_pretrained
tokenizer_args:
    pretrained_model_name_or_path: albert-base-v2

# params of transformers.AutoModelForMaskedLM.from_pretrained
model_args:
    pretrained_model_name_or_path: albert-base-v2

# params of transformers.TrainingArguments
training_args:
    evaluation_strategy: 'no'
    learning_rate: 1e-4
    lr_scheduler_type: 'constant'
    num_train_epochs: 3
    max_steps: 100
    warmup_steps: 0
    gradient_accumulation_steps: 1
    per_device_train_batch_size: 6
    per_device_eval_batch_size: 6
    dataloader_pin_memory: true
    dataloader_num_workers: 0
    bf16: false
    tf32: true
    torch_compile: false   # needs triton, which is linux-only
    save_strategy: 'no'
    logging_steps: 100
    seed: 42
    data_seed: 23
    remove_unused_columns: true

# params of DataCollatorForMLM
collator_args:
    pad_to_multiple_of: None   # should be set to 8 if fp16 is enabled

