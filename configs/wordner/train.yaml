global_seed: 42
target_fpr: 2.0

# params of bertools.tasks.wordner.load.load_annotations
dataset_args:
    data_files:
        train: 
          - data/chia/spans-flat-nooverlap/train.parquet
        eval:
          - data/chia/spans-flat-nooverlap/eval.parquet
        test:
          - data/chia/spans-flat-nooverlap/test.parquet
    preprocess_args:
        max_context_lines: 3
    # upsampling_coefs:

# params of transformers.AutoTokenizer.from_pretrained
tokenizer_args:
    pretrained_model_name_or_path: bert-base-uncased

# params of transformers.AutoModelForTokenClassification.from_pretrained
model_args:
    pretrained_model_name_or_path: bert-base-uncased

# params of bertools.tasks.wordner.transforms.Collator
collator_args: {}

# params of transformers.TrainingArguments
training_args:
    learning_rate: 5.e-5
    lr_scheduler_type: 'linear'
    num_train_epochs: 3
    max_steps: 0
    max_grad_norm: 1.
    warmup_ratio: 0.1
    weight_decay: 1.e-4
    gradient_accumulation_steps: 1
    per_device_train_batch_size: 6
    per_device_eval_batch_size: 6
    ddp_find_unused_parameters: False
    bf16: False
    tf32: True
    torch_compile: False
    eval_steps: 0.2
    evaluation_strategy: 'steps'
    save_strategy: 'no'
    logging_steps: 0.05
    report_to: none
    seed: 42
    data_seed: 23
