global_seed: 42
target_fpr: 2.0

dataset_args:
    data_files:
        train: 
          - data/chia/ner-baseline/train.parquet
        eval:
          - data/chia/ner-baseline/eval.parquet
        test:
          - data/chia/ner-baseline/test.parquet
    preprocess_args:
        max_context_lines: 3
    # upsampling_coefs:

tokenizer_args:
    pretrained_model_name_or_path: bert-base-uncased

model_args:
    pretrained_model_name_or_path: bert-base-uncased

collator_args: {}

training_args:
    learning_rate: 5.e-5
    lr_scheduler_type: 'linear'
    num_train_epochs: 3
    max_steps: 0
    max_grad_norm: 1.
    warmup_ratio: 0.1
    weight_decay: 1.e-4
    gradient_accumulation_steps: 1
    per_device_train_batch_size: 6
    per_device_eval_batch_size: 6
    ddp_find_unused_parameters: False
    bf16: False
    tf32: True
    torch_compile: False
    eval_steps: 0.2
    evaluation_strategy: 'steps'
    save_strategy: 'no'
    logging_steps: 0.05
    report_to: none
    seed: 42
    data_seed: 23
